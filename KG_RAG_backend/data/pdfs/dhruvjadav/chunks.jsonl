{"chunk_id": "d00p0001c01", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 1, "text": "Text-Guided Attention is All You Need for\nZero-Shot Robustness in Vision-Language Models\nLu Yu 1 Haiyang Zhang 1 Changsheng Xu 2\n1School of Computer Science and Engineering, Tianjin University of Technology\n2State Key Laboratory of Multimodal Artificial Intelligence Systems,\nInstitute of Automation, University of Chinese Academy of Sciences\n{luyu@email, zshy@stud}.tjut.edu.cn, csxu@nlpr.ia.ac.cn\nAbstract\nDue to the impressive zero-shot capabilities, pre-trained vision-language mod-\nels (e.g. CLIP), have attracted widespread attention and adoption across various\ndomains. Nonetheless, CLIP has been observed to be susceptible to adversar-\nial examples. Through experimental analysis, we have observed a phenomenon"}
{"chunk_id": "d00p0001c02", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 1, "text": "els (e.g. CLIP), have attracted widespread attention and adoption across various\ndomains. Nonetheless, CLIP has been observed to be susceptible to adversar-\nial examples. Through experimental analysis, we have observed a phenomenon\nwherein adversarial perturbations induce shifts in text-guided attention. Building\nupon this observation, we propose a simple yet effective strategy: Text-Guided\nAttention for Zero-Shot Robustness (TGA-ZSR). This framework incorporates two\ncomponents: the Attention Refinement module and the Attention-based Model\nConstraint module. Our goal is to maintain the generalization of the CLIP model\nand enhance its adversarial robustness: The Attention Refinement module aligns\nthe text-guided attention obtained from the target model via adversarial examples"}
{"chunk_id": "d00p0001c03", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 1, "text": "Constraint module. Our goal is to maintain the generalization of the CLIP model\nand enhance its adversarial robustness: The Attention Refinement module aligns\nthe text-guided attention obtained from the target model via adversarial examples\nwith the text-guided attention acquired from the original model via clean examples.\nThis alignment enhances the model’s robustness. Additionally, the Attention-based\nModel Constraint module acquires text-guided attention from both the target and\noriginal models using clean examples. Its objective is to maintain model perfor-\nmance on clean samples while enhancing overall robustness. The experiments\nvalidate that our method yields a 9.58% enhancement in zero-shot robust accu-"}
{"chunk_id": "d00p0001c04", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 1, "text": "original models using clean examples. Its objective is to maintain model perfor-\nmance on clean samples while enhancing overall robustness. The experiments\nvalidate that our method yields a 9.58% enhancement in zero-shot robust accu-\nracy over the current state-of-the-art techniques across 16 datasets. Our code is\navailable at https://github.com/zhyblue424/TGA-ZSR.\n1 Introduction\nLarge-scale pre-trained vision-language models (VLMs) have showcased remarkable success in\nartificial intelligence by seamlessly integrating visual and textual data to understand complex mul-\ntimodal information, such as CLIP [48]. Leveraging vast datasets and powerful architectures such\nas BERT [10] and its variants [8, 33], these models adeptly capture semantic relationships between"}
{"chunk_id": "d00p0001c05", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 1, "text": "artificial intelligence by seamlessly integrating visual and textual data to understand complex mul-\ntimodal information, such as CLIP [48]. Leveraging vast datasets and powerful architectures such\nas BERT [10] and its variants [8, 33], these models adeptly capture semantic relationships between\nimages and texts, offering significant advantages across numerous applications. From image classi-\nfication [14, 67, 55] and semantic segmentation [50] to image captioning [39] and vision question\nanswering [44], pre-trained VLMs revolutionize how machines perceive and interact with multimodal\ninformation. Their importance lies in their ability to learn rich representations from varied data\nstreams, enabling zero-shot learning and transfer learning across domains and tasks. Thus ensuring"}
{"chunk_id": "d00p0001c06", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 1, "text": "answering [44], pre-trained VLMs revolutionize how machines perceive and interact with multimodal\ninformation. Their importance lies in their ability to learn rich representations from varied data\nstreams, enabling zero-shot learning and transfer learning across domains and tasks. Thus ensuring\nthe reliability of large-scale models is crucial. However, these models are vulnerable to adversarial\nattacks as many other networks as demonstrated by recent studies [38, 59], even slight perturbations\nto input data can result in misclassification or altered outputs. Such attacks pose a significant chal-\nlenge, particularly in critical applications like autonomous vehicles [ 60], medical diagnosis [ 32],\nand maritime navigation [29], where the consequences of erroneous decisions can be severe. As"}
{"chunk_id": "d00p0001c07", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 1, "text": "to input data can result in misclassification or altered outputs. Such attacks pose a significant chal-\nlenge, particularly in critical applications like autonomous vehicles [ 60], medical diagnosis [ 32],\nand maritime navigation [29], where the consequences of erroneous decisions can be severe. As\nthese large-scale models become increasingly prevalent in real-world applications, understanding and\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2410.21802v2  [cs.CV]  30 Oct 2024"}
{"chunk_id": "d00p0002c01", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 2, "text": "mitigating the risks posed by adversarial attacks is essential to maintain trust and reliability in AI\nsystems.\nAdversarial training [53, 61, 69] has emerged as a crucial technique in enhancing the robustness\nof deep learning models against adversarial attacks. By augmenting training data with adversarial\nexamples generated through perturbations of input data, models are forced to learn more robust\ndecision boundaries, thereby improving their resilience to adversarial manipulation. Given the\nrising significance of large-scale VLMs in various applications, understanding their vulnerability to\nadversarial attacks is essential. While adversarial training presents practical challenges when applied"}
{"chunk_id": "d00p0002c02", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 2, "text": "rising significance of large-scale VLMs in various applications, understanding their vulnerability to\nadversarial attacks is essential. While adversarial training presents practical challenges when applied\nto downstream tasks, especially with large-scale models. Firstly, adversarial training typically involves\ngenerating adversarial examples during each training iteration, which increases the computational\noverhead and may lead to overfitting on the training data. This phenomenon is exacerbated in\nlarge-scale models with vast parameter spaces, where fine-tuning becomes more susceptible to\noverfitting. Moreover, adversarial training may not adequately prepare models for all possible\nadversarial scenarios, potentially leaving them vulnerable to unknown data distributions encountered"}
{"chunk_id": "d00p0002c03", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 2, "text": "large-scale models with vast parameter spaces, where fine-tuning becomes more susceptible to\noverfitting. Moreover, adversarial training may not adequately prepare models for all possible\nadversarial scenarios, potentially leaving them vulnerable to unknown data distributions encountered\nin real-world settings. Exploring zero-shot adversarial robustness in these models is particularly\npertinent as it sheds light on their ability to generalize and perform reliably in unseen scenarios.\nAdditionally, considering the multimodal nature of VLMs, the exploration of zero-shot adversarial\nrobustness offers insights into the complex interactions between visual and textual modalities, paving\nthe way for more robust and trustworthy multimodal AI systems."}
{"chunk_id": "d00p0002c04", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 2, "text": "Additionally, considering the multimodal nature of VLMs, the exploration of zero-shot adversarial\nrobustness offers insights into the complex interactions between visual and textual modalities, paving\nthe way for more robust and trustworthy multimodal AI systems.\nText-guided Contrastive Adversarial Training (TeCoA) method [38] represents the pioneering effort\nin investigating the zero-shot adversarial robustness of large-scale VLMs. They aim to bolster\nCLIP’s zero-shot generalization capacity against adversarial inputs. While their primary focus lies\non enhancing accuracy in the face of adversarial samples, this improvement comes at the expense\nof decreased performance on clean data. Subsequent work by PMG-AFT [59] builds upon this by"}
{"chunk_id": "d00p0002c05", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 2, "text": "CLIP’s zero-shot generalization capacity against adversarial inputs. While their primary focus lies\non enhancing accuracy in the face of adversarial samples, this improvement comes at the expense\nof decreased performance on clean data. Subsequent work by PMG-AFT [59] builds upon this by\nintroducing a pre-trained model guided adversarial fine-tuning technique, further enhancing both\ngeneralizability and adversarial robustness. However, despite the advancements made by both studies\nin enhancing CLIP’s zero-shot robustness, significant questions regarding the interpretability of\nadversarial attacks and the efficacy of adversarial training remain unanswered. Specifically, the\nmechanisms through which adversarial attacks influence network outputs and the reasons behind"}
{"chunk_id": "d00p0002c06", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 2, "text": "in enhancing CLIP’s zero-shot robustness, significant questions regarding the interpretability of\nadversarial attacks and the efficacy of adversarial training remain unanswered. Specifically, the\nmechanisms through which adversarial attacks influence network outputs and the reasons behind\nthe effectiveness of adversarial training strategies remain elusive. In our paper, we delve into the\ntext-guided attention shift phenomenon to shed light on how adversarial attacks alter model outputs.\nLeveraging these insights, we propose a simple yet effective strategy, TGA-ZSR, aimed at enhancing\nthe robustness of the CLIP model and preserving its performance on clean examples.\nOur main contributions are summarized follows:"}
{"chunk_id": "d00p0002c07", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 2, "text": "Leveraging these insights, we propose a simple yet effective strategy, TGA-ZSR, aimed at enhancing\nthe robustness of the CLIP model and preserving its performance on clean examples.\nOur main contributions are summarized follows:\n• To our knowledge, we are the first to introduce text-guided attention to enhance zero-shot robustness\non vision-language models while maintaining performance on clean sample.\n• We improve the interpretability of adversarial attacks for zero-shot robustness on vision-language\nmodels through a text-guided attention mechanism.\n• The experimental results show that TGA-ZSR surpasses previous state-of-the-art methods, estab-\nlishing a new benchmark in model zero-shot robust accuracy.\n2 Related Work"}
{"chunk_id": "d00p0002c08", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 2, "text": "models through a text-guided attention mechanism.\n• The experimental results show that TGA-ZSR surpasses previous state-of-the-art methods, estab-\nlishing a new benchmark in model zero-shot robust accuracy.\n2 Related Work\nPre-trained Vision-language Models. In recent years, advancements in computer vision[12, 17, 34]\nhave primarily relied on training models with image-label pairs to recognize predefined object\ncategories. However, these approaches often overlook the inherent semantic connections between\ntextual descriptions and visual content. Motivated by the remarkable progress witnessed in natural\nlanguage processing (NLP), exemplified by breakthroughs like Transformer [56], BERT [10], and\nGPT-3 [3], researchers are increasingly drawn to the prospect of using textual data to enhance"}
{"chunk_id": "d00p0002c09", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 2, "text": "textual descriptions and visual content. Motivated by the remarkable progress witnessed in natural\nlanguage processing (NLP), exemplified by breakthroughs like Transformer [56], BERT [10], and\nGPT-3 [3], researchers are increasingly drawn to the prospect of using textual data to enhance\nthe capabilities of DNNs. These methodologies are referred to as VLMs [ 21, 48, 49, 64] and one\nprominent approach is to directly learn the semantic similarity between images and corresponding\ntextual descriptions through image-text pairs. By aligning the embeddings of these two modalities,\nmodels like CLIP [48], ALIGN [21], BLIP [25], Visual-BERT [47], and ALBEF [26] aim to achieve\nsuperior performance across various tasks. CLIP [48] leverages a vast dataset of 400 million image-"}
{"chunk_id": "d00p0002c10", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 2, "text": "textual descriptions through image-text pairs. By aligning the embeddings of these two modalities,\nmodels like CLIP [48], ALIGN [21], BLIP [25], Visual-BERT [47], and ALBEF [26] aim to achieve\nsuperior performance across various tasks. CLIP [48] leverages a vast dataset of 400 million image-\ntext pairs sourced from the internet and employs contrastive loss to effectively align the embeddings\nof both modalities, thereby enhancing the model’s capabilities. Experimental results underscore the\n2"}
{"chunk_id": "d00p0003c01", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 3, "text": "significant performance gains achieved by incorporating textual information into the model, with\nzero-shot performance surpassing that of earlier deep neural network architectures. However, despite\nits impressive zero-shot accuracy, experiments [38, 59] reveal vulnerabilities to adversarial examples,\nresulting in a notable decline in robustness.\nAdversarial Robustness. Deep neural networks have been found to be vulnerable to adversarial\nexamples [54, 36, 40, 66], which can fool DNNs to produce false outputs, rendering trained models\nunreliable. To bolster robustness against such adversarial attacks, various advanced methods have\nbeen proposed, including data augmentation [28, 58, 27, 65], adversarial training [69, 53, 61, 68],"}
{"chunk_id": "d00p0003c02", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 3, "text": "examples [54, 36, 40, 66], which can fool DNNs to produce false outputs, rendering trained models\nunreliable. To bolster robustness against such adversarial attacks, various advanced methods have\nbeen proposed, including data augmentation [28, 58, 27, 65], adversarial training [69, 53, 61, 68],\nprogressive self-distillation [1], randomization strategy [ 11, 35], and adversarial purification [ 41,\n24, 62]. While these strategies aim to improve DNNs’ adversarial robustness, they often come with\nincreased complexity or limited generalizability. Adversarial training [69, 53, 61, 68] stands out as\none of the most widely used and effective approaches, fine-tuning DNNs by generating adversarial\nexamples during training. After the emergence of CLIP [48], many subsequent works [45, 16, 63]"}
{"chunk_id": "d00p0003c03", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 3, "text": "increased complexity or limited generalizability. Adversarial training [69, 53, 61, 68] stands out as\none of the most widely used and effective approaches, fine-tuning DNNs by generating adversarial\nexamples during training. After the emergence of CLIP [48], many subsequent works [45, 16, 63]\nhave utilized CLIP as a backbone, yet little attention has been given to studying its adversarial\nrobustness. CLIP is shown to be susceptible to adversarial examples [38] as well, posing a significant\nthreat to downstream tasks utilizing CLIP as a backbone. Hence, investigating the adversarial\nrobustness of CLIP is crucial.\nZero-shot Adversarial Robustness for VLMs. The visual-language model, trained on both image"}
{"chunk_id": "d00p0003c04", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 3, "text": "threat to downstream tasks utilizing CLIP as a backbone. Hence, investigating the adversarial\nrobustness of CLIP is crucial.\nZero-shot Adversarial Robustness for VLMs. The visual-language model, trained on both image\nand text data, serves as a foundational model for various tasks. However, it has shown vulnerability\nto adversarial examples [38, 59], and training from scratch is time-intensive. TeCoA [38] was the\nfirst to explore zero-shot adversarial robustness for VLMs, aiming to enhance CLIP’s adversarial\nrobustness by minimizing the cross-entropy loss between image logits and targets. While TeCoA\nsolely utilizes cross-entropy loss, yielding only marginal performance improvements, PMG-AFT [59]"}
{"chunk_id": "d00p0003c05", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 3, "text": "first to explore zero-shot adversarial robustness for VLMs, aiming to enhance CLIP’s adversarial\nrobustness by minimizing the cross-entropy loss between image logits and targets. While TeCoA\nsolely utilizes cross-entropy loss, yielding only marginal performance improvements, PMG-AFT [59]\nextends this approach by minimizing the distance between features of adversarial examples and those\nof the pre-trained model. FARE [51] primarily focuses on maintaining high clean accuracy while\nimproving model robustness, achieving this by constraining the distance between the original and\ntarget model embeddings. Our experiments reveal significant differences in attention maps between\noriginal examples and adversarial examples. Leveraging this insight, we enhance model robustness"}
{"chunk_id": "d00p0003c06", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 3, "text": "improving model robustness, achieving this by constraining the distance between the original and\ntarget model embeddings. Our experiments reveal significant differences in attention maps between\noriginal examples and adversarial examples. Leveraging this insight, we enhance model robustness\nby constraining it with text-guided attention.\n3 Methodology\n3.1 Preliminaries and Problem Setup\nFollowing the previous works [38, 59], we choose CLIP model as the pre-trained VLMs for image\nclassification task. Given an image-text pair (x, t), where x represents an image and t represents a\ntextual prompt, CLIP learns to encode both the image and the text into fixed-dimensional embeddings.\nLet f(x) denote the embedding of the image x and g(t) denote the embedding of the text prompt"}
{"chunk_id": "d00p0003c07", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 3, "text": "classification task. Given an image-text pair (x, t), where x represents an image and t represents a\ntextual prompt, CLIP learns to encode both the image and the text into fixed-dimensional embeddings.\nLet f(x) denote the embedding of the image x and g(t) denote the embedding of the text prompt\nt, y is the one-hot vector label. For training or fine-tuning on the downstream tasks, we use the\ncross-entropy loss, denoted as L(x, t, y).\nL(x, t, y) = −Ei,j\n\u0014\nyijlog exp(cos(f(x)i, g(t)j))/τ)P\nk exp(cos(f(x)i, g(t)k))/τ)\n\u0015\n(1)\nwhere we set yij = 1 if the image-text pair is positive, otherwise, yij = 0. τ is the temperature\nparameter and cos indicates calculating the cosine similarity of the two embeddings."}
{"chunk_id": "d00p0003c08", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 3, "text": "L(x, t, y) = −Ei,j\n\u0014\nyijlog exp(cos(f(x)i, g(t)j))/τ)P\nk exp(cos(f(x)i, g(t)k))/τ)\n\u0015\n(1)\nwhere we set yij = 1 if the image-text pair is positive, otherwise, yij = 0. τ is the temperature\nparameter and cos indicates calculating the cosine similarity of the two embeddings.\nAdversarial Attacks. Adversarial attacks are a concerning phenomenon where small, often imper-\nceptible perturbations are intentionally applied to input data with the aim of deceiving a model into\nproducing incorrect outputs. These perturbations are crafted with the goal of causing the model to\nmisclassify or generate erroneous predictions while appearing indistinguishable to human observers.\nThe Projected Gradient Descent (PGD) [36] method is an iterative approach for crafting adversarial"}
{"chunk_id": "d00p0003c09", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 3, "text": "producing incorrect outputs. These perturbations are crafted with the goal of causing the model to\nmisclassify or generate erroneous predictions while appearing indistinguishable to human observers.\nThe Projected Gradient Descent (PGD) [36] method is an iterative approach for crafting adversarial\nexamples. It starts with the original input data and then iteratively adjusts the data in the direction\nthat maximizes the model’s loss function while ensuring the perturbed data remains within a specified\nperturbation budget. Mathematically, the PGD attack can be expressed as follows:\nxa+1 = Πx+S(xa + ε · sign(▽xa L(xa, t, y))) (2)\nHere, L represents the loss function, x denotes the original input data, ε controls the magnitude of"}
{"chunk_id": "d00p0003c10", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 3, "text": "perturbation budget. Mathematically, the PGD attack can be expressed as follows:\nxa+1 = Πx+S(xa + ε · sign(▽xa L(xa, t, y))) (2)\nHere, L represents the loss function, x denotes the original input data, ε controls the magnitude of\nperturbation, and ▽xL represents the gradient of the loss function with respect to the input data.\n3"}
{"chunk_id": "d00p0004c01", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 4, "text": "Adv. image attention     Adv. image              Ori. image attention     Ori. image\ngerman shepherd standard poodle   standard poodle golden retriever     chihuahua             goldfish                 goose                    goose\njinrikisha chimpanzee swimming trunks chihuahua           egyptian cat       cauliflower           albatross            birdhouse\nFigure 1: The four rows depict the original image, its associated attention map, the generated\nadversarial example, and the attention map of the adversarial example. Labels in black indicate the\nground truth, while those in red represent mis-classified labels for the adversarial examples.\nBy adding or subtracting ε times the sign of this gradient to the original input data, the PGD attack"}
{"chunk_id": "d00p0004c02", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 4, "text": "adversarial example, and the attention map of the adversarial example. Labels in black indicate the\nground truth, while those in red represent mis-classified labels for the adversarial examples.\nBy adding or subtracting ε times the sign of this gradient to the original input data, the PGD attack\ngenerates adversarial examples that lead to misclassification or incorrect predictions by the model.\nΠx+S makes the perturbed data remains within an ε-neighborhood of the original input, preventing\nthe generated adversarial examples from straying too far. S is a set of allowed perturbations that\nformalizes the manipulative power of the adversary.\nAdversarial Examples Generation and Adversarial Training. The optimization objective for"}
{"chunk_id": "d00p0004c03", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 4, "text": "the generated adversarial examples from straying too far. S is a set of allowed perturbations that\nformalizes the manipulative power of the adversary.\nAdversarial Examples Generation and Adversarial Training. The optimization objective for\ncrafting adversarial examples aims to maximize the loss of model fθ with respect to a perturbed input\nxa which can be formulated as:\nxa = argmax\nxa\nL(fθ(xa, t, y)) (3)\nAdversarial training is a technique to generate adversarial examples from the original training data\nand then use these examples to train the model, forcing it to learn to resist adversarial perturbations.\nTo adapt the model to the downstream tasks, we apply adversarial fine-tuning on one target model\ntowards robustness with the following loss:\nθ = argmin\nθ\nJ (fθ(xa, t, y)) (4)"}
{"chunk_id": "d00p0004c04", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 4, "text": "and then use these examples to train the model, forcing it to learn to resist adversarial perturbations.\nTo adapt the model to the downstream tasks, we apply adversarial fine-tuning on one target model\ntowards robustness with the following loss:\nθ = argmin\nθ\nJ (fθ(xa, t, y)) (4)\nWhere J represents the total loss function used for training the model.\nZero-Shot Adversarial Robustness. In this paper, we investigate the zero-shot adversarial robustness\nof CLIP model, which refers to the ability of these models to maintain performance and reliability even\nwhen encountering unseen adversarial samples during inference, with only adversarial fine-tuning the\noriginal CLIP model on one target dataset, such as Tiny-ImageNet.\n3.2 Text-Guided Attention based Interpretation of Adversarial Attacks"}
{"chunk_id": "d00p0004c05", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 4, "text": "when encountering unseen adversarial samples during inference, with only adversarial fine-tuning the\noriginal CLIP model on one target dataset, such as Tiny-ImageNet.\n3.2 Text-Guided Attention based Interpretation of Adversarial Attacks\nText-Guided Attention. Attention mechanisms [30, 16, 31] play a crucial role in enhancing vision\nmodel performance across various tasks. At its core, attention enables models to focus on relevant\nparts of the input data while suppressing irrelevant information. Similarly, in VLMs, by incorporating\ntextual guidance, the models can effectively focus on relevant visual features while processing\n4"}
{"chunk_id": "d00p0005c01", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 5, "text": "Figure 2: An overview of our TGA-ZSR framework: We generate adversarial examples and feed\nthem into the target image encoder. To enhance the adversarial robustness of the CLIP model and\nmaintain its generalization, we introduce text-guided attention. This involves refining the framework\nfor adversarial examples through the Attention Refinement module and constraining the model to\nprevent significant drift via the Attention-based Model Constraint module.\nlanguage, thus facilitating more accurate and coherent multimodal understanding. Additionally,\ntext-guided attention enhances interpretability by providing insights into the model’s decision-making\nprocess, fostering trust and understanding in complex multimodal systems. Thus, we investigate the"}
{"chunk_id": "d00p0005c02", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 5, "text": "language, thus facilitating more accurate and coherent multimodal understanding. Additionally,\ntext-guided attention enhances interpretability by providing insights into the model’s decision-making\nprocess, fostering trust and understanding in complex multimodal systems. Thus, we investigate the\nimpact of text-guided attention on enhancing and interpreting zero-shot adversarial robustness in\nVLMs in this paper. We define the text-guided attention as following:\nA(x) = fg(x) · g(t)T, A ∈ RP×1 (5)\nWhere fg(x) represents the global image feature before the pooling operation of f(x), and P denotes\nthe dimension of the attention embeddings. We reshape A to R\n√\nP×\n√\nP to obtain the attention map,\nwhich is then resized to A ∈ RH×W . Finally, we apply a normalization operation (norm) on A to"}
{"chunk_id": "d00p0005c03", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 5, "text": "Where fg(x) represents the global image feature before the pooling operation of f(x), and P denotes\nthe dimension of the attention embeddings. We reshape A to R\n√\nP×\n√\nP to obtain the attention map,\nwhich is then resized to A ∈ RH×W . Finally, we apply a normalization operation (norm) on A to\nobtain the final text-guided attention map.\nInterpretation of Adversarial Attacks. The previous research has predominantly focused on\nbolstering the zero-shot robustness of Vision-Language Models (VLMs), yet the reasons leading\nto mis-classifications induced by adversarial attacks remain unclear. This paper aims to shed light\non interpreting the impact of adversarial attacks on VLMs. By employing Eq. 5, we compute the"}
{"chunk_id": "d00p0005c04", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 5, "text": "bolstering the zero-shot robustness of Vision-Language Models (VLMs), yet the reasons leading\nto mis-classifications induced by adversarial attacks remain unclear. This paper aims to shed light\non interpreting the impact of adversarial attacks on VLMs. By employing Eq. 5, we compute the\ntext-guided attention for both the original image ( Ori. image ) and its corresponding adversarial\ncounterpart (Adv. image ), as depicted in Fig. 1. Remarkably, despite the subtle discrepancies\nimperceptible to the human eye between the adversarial example and the original image, the former is\nmis-classified (labels in red). However, a significant difference emerges in the respective text-guided\nattention maps. Specifically, we observe a notable shift in the text-guided attention of the adversarial"}
{"chunk_id": "d00p0005c05", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 5, "text": "mis-classified (labels in red). However, a significant difference emerges in the respective text-guided\nattention maps. Specifically, we observe a notable shift in the text-guided attention of the adversarial\nexample, characterized by instances of displacement towards other objects, backgrounds, or even\ndisappearance. For instance, while the original images in the first, second, and fourth columns pay\nattention to their subjects’ heads, in their adversarial counterparts, attention diverges elsewhere. In\nthe third column, the attention shift leads from the correct object to an incorrect one, resulting in\nmis-classification. In the fifth and seventh columns, the attention in their adversarial counterparts is\nredirected towards the background.\n5"}
{"chunk_id": "d00p0006c01", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 6, "text": "3.3 Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR)\nThe semantic information embedded within text representations are preserved through a frozen\ntext encoder, offering invaluable guidance when adversarial perturbations disrupt relevant visual\nfeatures, which has not been explored for zero-shot robustness of vision-language models. We\nintroduce the Attention Refinement Module, designed to effectively filter out irrelevant information,\nthereby mitigating the impact of adversarial attacks seeking to exploit vulnerabilities in the model’s\ndecision-making process. Moreover, to maintain model’s ability to generalize effectively on clean\nimages, we introduce the Attention-based Model Constraint Module. This module ensures consistent"}
{"chunk_id": "d00p0006c02", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 6, "text": "thereby mitigating the impact of adversarial attacks seeking to exploit vulnerabilities in the model’s\ndecision-making process. Moreover, to maintain model’s ability to generalize effectively on clean\nimages, we introduce the Attention-based Model Constraint Module. This module ensures consistent\nperformance on clean data while enhancing the model against adversarial disruptions. Additionally,\nemploying text-guided attention enhances interpretability, offering crucial insights into how the model\nintegrates and processes information across modalities. This interpretability not only instills trust in\nthe model’s predictions but also facilitates the detection and mitigation of adversarial attacks. Our"}
{"chunk_id": "d00p0006c03", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 6, "text": "integrates and processes information across modalities. This interpretability not only instills trust in\nthe model’s predictions but also facilitates the detection and mitigation of adversarial attacks. Our\napproach (i.e. TGA-ZSR) presents a comprehensive framework (as shown in Fig. 2) for enhancing\nmodel robustness to adversarial perturbations while concurrently improving interpretability. We will\nintroduce the details as follows.\nAttention Refinement Module. Based on the insights gained in Section 3.2, we propose an attention\nrefinement module aimed at enhancing the robustness of the model. This module is designed to\nrectify the text-guided attention of adversarial samples, which often leads to altered predictions."}
{"chunk_id": "d00p0006c04", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 6, "text": "Attention Refinement Module. Based on the insights gained in Section 3.2, we propose an attention\nrefinement module aimed at enhancing the robustness of the model. This module is designed to\nrectify the text-guided attention of adversarial samples, which often leads to altered predictions.\nOur approach aligns the adversarial attention map with that of the clean samples, known for their\nhigh-accuracy attention distribution. This simple yet effective strategy serves to mitigate the impact\nof adversarial perturbations on the model’s predictions.\nWe take the generated adversarial samplexa to the target model ftar\ng (·) and the clean sample x to the\noriginal model fori\ng (·) and obtain the adversarial attention map A(xi\na)tar and the clean attention map"}
{"chunk_id": "d00p0006c05", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 6, "text": "of adversarial perturbations on the model’s predictions.\nWe take the generated adversarial samplexa to the target model ftar\ng (·) and the clean sample x to the\noriginal model fori\ng (·) and obtain the adversarial attention map A(xi\na)tar and the clean attention map\nA(xi)ori respectively. The attention refinement loss LAR is thus defined as:\nLAR = 1\nN ·\nNX\ni=0\n∥A(xi\na)tar − A(xi)ori∥2 (6)\nwhere A(xa)tar = ftar\ng (xa) · g(t)T and A(x)ori = fori\ng (x) · g(t)T 1, ∥∥2 denotes the L2 distance\ncomputation between two attention maps.\nAttention-based Model Constraint Module. The Attention Refinement module serves to enhance\nthe robustness of the models, consequently improving the accuracy of adversarial samples. However,"}
{"chunk_id": "d00p0006c06", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 6, "text": "g (x) · g(t)T 1, ∥∥2 denotes the L2 distance\ncomputation between two attention maps.\nAttention-based Model Constraint Module. The Attention Refinement module serves to enhance\nthe robustness of the models, consequently improving the accuracy of adversarial samples. However,\nthis enhancement comes with a trade-off: it may marginally sacrifice the accuracy on clean samples\ndue to shifts in model parameters. To preserve the generalization capability of pre-trained VLMs, we\nintroduce an Attention-based Model Constraint module. This module aims to mitigate performance\ndrops on clean images, thereby ensuring the overall effectiveness and reliability of the model.\nSpecifically, we input the clean samplex into the target model ftar\ng (·), adversarially fine-tuned on the"}
{"chunk_id": "d00p0006c07", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 6, "text": "introduce an Attention-based Model Constraint module. This module aims to mitigate performance\ndrops on clean images, thereby ensuring the overall effectiveness and reliability of the model.\nSpecifically, we input the clean samplex into the target model ftar\ng (·), adversarially fine-tuned on the\nTiny-ImageNet dataset, to acquire the text-guided attention map A(x)tar. Concurrently, the original\ntext-guided attention map outputted from the original CLIP model fori\ng (·) is denoted as A(x)ori.\nTo ensure the preservation of importance parameters for clean images, we enforce an L2 distance\nconstraint between these two attention maps. The attention-based model constraint loss LAMC is\nformulated as:\nLAMC = 1\nN ·\nNX\ni=0\n\r\rA(xi)tar − A(xi)ori\n\r\r\n2 (7)"}
{"chunk_id": "d00p0006c08", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 6, "text": "g (·) is denoted as A(x)ori.\nTo ensure the preservation of importance parameters for clean images, we enforce an L2 distance\nconstraint between these two attention maps. The attention-based model constraint loss LAMC is\nformulated as:\nLAMC = 1\nN ·\nNX\ni=0\n\r\rA(xi)tar − A(xi)ori\n\r\r\n2 (7)\nThus the final loss function can be represented as:\nLtotal = LCE + α · LAR + β · LAMC (8)\n4 Experiments\n4.1 Experimental Setup\nDatasets. Our experiments begin with training the pre-trained CLIP model on the Tiny-ImageNet [9].\nThen we evaluate the model’s zero-shot adversarial robustness across 15 subsequent datasets, fol-\n1We only compute the attention map for the image corresponding to the text prompt of the ground-truth label.\n6"}
{"chunk_id": "d00p0007c01", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 7, "text": "Table 1: Zero-shot robust accuracy on images attacked with 100 steps of PGD [36]. We performed\nseveral different methods on Tiny-ImageNet and evaluated across 16 datasets. The optimal accuracy\nis highlighted in bold, while the second-best accuracy is underlined. The values in parentheses\nrepresent the standard deviation.\nMethods Tiny-ImageNetCIFAR-10CIFAR-100STL-10SUN397Food101OxfordpetsFlowers102DTDEuroSATFGVC-AircraftImageNetCaltech-101Caltech-256StanfordCarsPCAMAverage"}
{"chunk_id": "d00p0007c02", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 7, "text": "represent the standard deviation.\nMethods Tiny-ImageNetCIFAR-10CIFAR-100STL-10SUN397Food101OxfordpetsFlowers102DTDEuroSATFGVC-AircraftImageNetCaltech-101Caltech-256StanfordCarsPCAMAverage\nCLIP [48]0.88 2.42 0.26 26.11 1.00 6.60 3.84 1.19 2.02 0.05 0.00 1.24 19.88 12.60 0.20 0.11 4.90FT-Clean13.55 19.92 4.94 40.00 0.82 0.64 2.40 0.68 2.66 0.05 0.03 1.08 14.95 9.69 0.09 1.32 7.05FT-Adv.51.5938.58 21.28 69.55 17.60 12.55 34.97 19.92 15.90 11.95 1.83 17.26 50.73 40.18 8.4248.8828.83TeCoA [38]37.57 30.30 17.53 67.19 19.70 14.76 36.44 22.46 17.4512.14 1.62 18.18 55.86 41.88 8.49 47.39 28.06FARE[51]23.88 21.25 10.72 59.59 8.30 10.97 24.56 15.48 10.96 0.14 0.84 10.54 45.96 34.35 4.38 10.17 18.25PMG-AFT[59]47.11 46.0125.8374.5122.2119.5841.6223.4515.05 12.541.98 21.4362.4245.9911.7248.6432.51"}
{"chunk_id": "d00p0007c03", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 7, "text": "TGA-ZSR (ours)63.95\n(±0.11)\n61.45\n(±0.67)\n35.27\n(±0.07)\n84.22\n(±0.21)\n33.22\n(±0.39)\n33.97\n(±0.20)\n57.75\n(±0.76)\n34.55\n(±0.35)\n22.08\n(±0.16)\n14.27\n(±0.26)\n4.75\n(±0.27)\n28.74\n(±0.11)\n70.97\n(±0.42)\n60.06\n(±0.46)\n20.40\n(±0.68)\n47.76\n(±0.35)\n42.09\n(±0.12)\nTable 2: Zero-shot clean accuracy. We performed several different methods on Tiny-ImageNet and\nevaluated across 16 datasets. The values in parentheses represent the standard deviation.\nMethods Tiny-ImageNetCIFAR-10CIFAR-100STL-10SUN397Food101OxfordpetsFlowers102DTDEuroSATFGVC-AircraftImageNetCaltech-101Caltech-256StanfordCarsPCAMAverage"}
{"chunk_id": "d00p0007c04", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 7, "text": "Methods Tiny-ImageNetCIFAR-10CIFAR-100STL-10SUN397Food101OxfordpetsFlowers102DTDEuroSATFGVC-AircraftImageNetCaltech-101Caltech-256StanfordCarsPCAMAverage\nCLIP [48]57.2688.0660.4597.04 57.26 83.89 87.41 65.47 40.69 42.59 20.25 59.15 85.34 81.73 52.02 52.09 64.42FT-Clean79.0484.55 54.25 93.78 46.80 47.10 80.98 46.43 30.32 24.399.30 44.40 78.69 70.81 31.15 47.89 54.37FT-Adv.73.83 68.96 39.69 86.89 33.37 27.74 60.10 33.45 23.14 16.49 4.86 32.06 67.41 57.72 18.11 49.91 43.36TeCoA [38]63.97 66.14 36.74 87.24 40.54 35.11 66.15 38.75 25.53 17.13 6.75 37.09 74.63 62.50 24.65 50.0145.81FARE[51]77.5487.5862.8094.3349.91 70.0281.4757.1036.3322.69 14.1951.7884.0477.5044.3546.07 59.85PMG-AFT[59]67.11 74.62 44.68 88.85 37.42 37.47 66.34 35.66 21.17 17.76 4.71 35.93 76.70 61.96 25.21 49.99 46.60"}
{"chunk_id": "d00p0007c05", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 7, "text": "TGA-ZSR(ours)75.72\n(±0.12)\n86.46\n(±0.26)\n56.52\n(±0.35)\n93.48\n(±0.19)\n51.99\n(±0.25)\n57.59\n(±0.34)\n77.32\n(±0.30)\n48.08\n(±0.37)\n29.06\n(±0.35)\n24.24\n(±0.49)\n11.93\n(±0.27)\n48.04\n(±0.06)\n80.70\n(±0.09)\n74.74\n(±0.18)\n36.62\n(±1.03)\n49.58\n(±0.17)\n56.44\n(±0.08)\nlowed by previous studies, such as TeCoA [38] and PMG-AFT [59]. These datasets include several\ncommonly used classfication datasets, including CIFAR-10 [23], CIFAR-100 [23], STL-10 [6], Ima-\ngeNet [9], Caltech-101 [13], and Caltech-256 [15]. Additionally, fine-grained image classification\ndatasets such as StanfordCars [22], Flowers102 [42], Food101 [2], FGVCAircraft [37], and Oxford-\nPets [46] are included. Furthermore, the scene recognition dataset SUN397 [43], the medical image"}
{"chunk_id": "d00p0007c06", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 7, "text": "geNet [9], Caltech-101 [13], and Caltech-256 [15]. Additionally, fine-grained image classification\ndatasets such as StanfordCars [22], Flowers102 [42], Food101 [2], FGVCAircraft [37], and Oxford-\nPets [46] are included. Furthermore, the scene recognition dataset SUN397 [43], the medical image\ndataset PCAM [57], and the satellite image classifacation dataset EuroSAT [18] and the texture recog-\nnition dataset DTD [5] are incorporated for comprehensive evaluation. We also conduct experiments\non four additional datastes (i.e. ImageNet_subset, ImageNet-A, ImageNet-O and ImageNet-R) as\nshown in Supp. Mat. A.1.\nImplementation Details. Following the protocol of previous works [59], we fine-tuned the CLIP"}
{"chunk_id": "d00p0007c07", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 7, "text": "on four additional datastes (i.e. ImageNet_subset, ImageNet-A, ImageNet-O and ImageNet-R) as\nshown in Supp. Mat. A.1.\nImplementation Details. Following the protocol of previous works [59], we fine-tuned the CLIP\nmodel on the adversarial samples of Tiny-ImageNet [9] as ‘adversarial fine-tuning’ and subsequently\nevaluated its performance across 15 datasets and Tiny-ImageNet itself. We employ ViT-B/32 as the\nbackbone in CLIP and utilize the SGD optimizer to minimize loss. During adversarial fine-tuning,\nwe update all parameters of the image encoder with a learning rate of 1e-4, weight decay of 0,\nmomentum of 0.9, and a batch size of 128. We utilize l∞ norm PGD-2 [36] with 2 iterations to\ngenerate adversarial examples, with an attack strength ε of 1/255 and the attack step size is 1/255."}
{"chunk_id": "d00p0007c08", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 7, "text": "we update all parameters of the image encoder with a learning rate of 1e-4, weight decay of 0,\nmomentum of 0.9, and a batch size of 128. We utilize l∞ norm PGD-2 [36] with 2 iterations to\ngenerate adversarial examples, with an attack strength ε of 1/255 and the attack step size is 1/255.\nTo evaluate zero-shot adversarial inference, we employl∞ norm PGD-100 [36] with 100 iterations,\nattack step of 1/255 and a batch size of 256 to generate adversarial examples for verifying CLIP’s\nadversarial robustness. Additionally, to assess the model’s robustness under different attack strengths,\nwe perform inference using adversarial strengths ε of 1/255, 2/255, and 4/255. The hyper-parameters\nα and β are set to 0.08 and 0.05 respectively in Eq. 8 in the main experiments. Maintain the same"}
{"chunk_id": "d00p0007c09", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 7, "text": "we perform inference using adversarial strengths ε of 1/255, 2/255, and 4/255. The hyper-parameters\nα and β are set to 0.08 and 0.05 respectively in Eq. 8 in the main experiments. Maintain the same\nparameters for the CW attack. For the AutoAttack [7] experiments, α and β are set to 0.08 and 0.009.\nWe conducted the experiment utilizing the RTX 3090, which required a training period ranging from\n3 to 4 hours.\n4.2 Main Results\nTo validate the effectiveness of our approach, we conduct comparisons with several state-of-the-\nart methods such as TeCoA [ 38], PMG-AFT [ 59], and FARE [ 51]. Additionally, we extend the\ncomparison to include CLIP (the original pre-trained CLIP model), FT-Adv. (adversarial fine-tuning"}
{"chunk_id": "d00p0007c10", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 7, "text": "To validate the effectiveness of our approach, we conduct comparisons with several state-of-the-\nart methods such as TeCoA [ 38], PMG-AFT [ 59], and FARE [ 51]. Additionally, we extend the\ncomparison to include CLIP (the original pre-trained CLIP model), FT-Adv. (adversarial fine-tuning\nusing the contrastive loss of the original CLIP) and FT-Clean (fine-tuning on clean examples with the\ncontrastive loss of the original CLIP) for a comprehensive evaluation.\n7"}
{"chunk_id": "d00p0008c01", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 8, "text": "Table 3: Zero-shot robust accuracy on images attacked with ε of 1/255 of AutoAttack [ 7]. We\nperformed several different methods on Tiny-ImageNet and evaluated on 16 datasets.\nMethods Tiny-ImageNetCIFAR-10CIFAR-100STL-10SUN397Food101OxfordpetsFlowers102DTDEuroSATFGVC-AircraftImageNetCaltech-101Caltech-256StanfordCarsPCAMAverage"}
{"chunk_id": "d00p0008c02", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 8, "text": "CLIP [48]0.02 0.01 0.08 0.03 0.04 0.01 0.00 0.03 0.16 0.12 0.06 0.04 0.43 0.10 0.11 0.22 0.09FT-Clean 0.08 0.03 0.01 0.91 0.09 0.04 0.06 0.03 0.48 0.02 0.03 0.12 1.38 0.66 0.03 0.03 0.25FT-Adv. 50.4837.55 20.39 69.14 16.25 11.23 33.91 18.5419.9511.591.65 16.21 49.90 39.24 7.5748.8428.28TeCoA [38]35.03 28.18 16.09 66.08 17.41 13.05 34.81 20.80 15.37 11.40 1.32 16.32 54.54 40.15 7.15 47.12 26.55FARE [51]28.59 23.37 13.58 60.70 9.72 13.88 27.72 15.48 9.15 0.25 0.87 12.07 47.45 36.68 6.77 10.23 19.78PMG-AFT [59]44.2644.12 23.66 73.9019.6317.2539.2520.8713.7211.991.6819.1760.5744.259.5948.5330.78TGA-ZSR (ours)49.4540.5322.3872.0620.3615.5840.3121.4317.1311.192.6419.2857.1645.6810.4748.0330.86\nTable 4: Zero-shot robust accuracy across 16 datasets with CW attack [4]. The optimal accuracy is"}
{"chunk_id": "d00p0008c03", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 8, "text": "Table 4: Zero-shot robust accuracy across 16 datasets with CW attack [4]. The optimal accuracy is\nhighlighted in bold.\nMethods Tiny-ImageNetCIFAR-10CIFAR-100STL-10SUN397Food101OxfordpetsFlowers102DTDEuroSATFGVC-AircraftImageNetCaltech-101Caltech-256StanfordCarsPCAMAverage\nCLIP [48]0.21 0.36 0.10 10.59 1.16 0.82 1.23 1.09 2.18 0.01 0.00 1.14 13.50 7.36 2.36 0.07 3.64PMG-AFT[59]44.59 44.86 24.15 74.11 19.99 17.33 39.88 20.95 13.51 12.09 1.47 19.51 60.99 44.46 10.5748.59 31.07TGA-ZSR(ours)63.85 60.50 34.62 84.11 22.03 33.28 58.33 32.95 21.22 13.89 4.56 20.42 70.34 59.73 20.2048.02 40.50\nAdversarial Zero-shot Robust Accuracy. Table 1 shows that the average accuracy of our TGA-ZSR\noutperforms the original CLIP model by 37.19%. Compared to current stat-of-the-art method, PMG-"}
{"chunk_id": "d00p0008c04", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 8, "text": "Adversarial Zero-shot Robust Accuracy. Table 1 shows that the average accuracy of our TGA-ZSR\noutperforms the original CLIP model by 37.19%. Compared to current stat-of-the-art method, PMG-\nAFT, the proposed method achieve an average improvement of 9.58%. In general, our method is\nsuperior than all the other methods on most datasets except a comparable result on PCAM dataset. In\naddition, we obtain the best result on Tiny-ImageNet, which is not a strict zero-shot test. It indicates\nthat our method is robust on the adversarial attack on both seen and unseen datasets.\nZero-shot Clean Accuracy. Table 2 illustrates the model’s accuracy for clean examples using\ndifferent methods. Our method outperforms PMG-AFT by 9.84% and FT-clean by 2.07% in terms of"}
{"chunk_id": "d00p0008c05", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 8, "text": "that our method is robust on the adversarial attack on both seen and unseen datasets.\nZero-shot Clean Accuracy. Table 2 illustrates the model’s accuracy for clean examples using\ndifferent methods. Our method outperforms PMG-AFT by 9.84% and FT-clean by 2.07% in terms of\naverage accuracy. Similar to Table 1, zero-shot clean accuracy exhibits improvement not only on an\nindividual dataset but across all datasets. However, we observed that our zero-shot clean accuracy is\n3.41% lower than that achieved by FARE. It is important to note that FARE prioritizes preserving\nzero-shot clean accuracy. However, we have significantly enhanced the zero-shot robust accuracy in\nadversarial scenarios with 23.84% gain compared to FARE in Table 1.\n4.3 Experiments on More Attack Types"}
{"chunk_id": "d00p0008c06", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 8, "text": "zero-shot clean accuracy. However, we have significantly enhanced the zero-shot robust accuracy in\nadversarial scenarios with 23.84% gain compared to FARE in Table 1.\n4.3 Experiments on More Attack Types\nResults against AutoAttack. AutoAttack [7] stands out as a strong attack method for assessing\nmodel robustness. We follow TeCoA and PMG-AFT to verify the perturbation bound ε of 1/255\nin the standard version of AutoAttack. The results are summarized in Table 3. We can see that the\noriginal CLIP model experienced a significant performance decline, decreasing to 0.09% on the\nadversarial example. Our TGA-ZSR also demonstrates a decline but still achieves superior results\ncompared to other methods, validating its effectiveness against stronger attacks."}
{"chunk_id": "d00p0008c07", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 8, "text": "original CLIP model experienced a significant performance decline, decreasing to 0.09% on the\nadversarial example. Our TGA-ZSR also demonstrates a decline but still achieves superior results\ncompared to other methods, validating its effectiveness against stronger attacks.\nResults against CW Attack. CW attack [4] is an optimization-based approach designed to generate\nsmall perturbations to input data, causing the model to make incorrect predictions while keeping the\nperturbed input visually similar to the original. We further evaluate the robustness of our approach\nagainst this challenging attack, using a perturbation bound of ε = 1/255. The results, shown in\nTable 4, demonstrate that our method significantly outperforms the state-of-the-art method PMG-AFT"}
{"chunk_id": "d00p0008c08", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 8, "text": "perturbed input visually similar to the original. We further evaluate the robustness of our approach\nagainst this challenging attack, using a perturbation bound of ε = 1/255. The results, shown in\nTable 4, demonstrate that our method significantly outperforms the state-of-the-art method PMG-AFT\non both adversarial and clean samples. This substantial margin indicates the adversarial robustness of\nour proposed method.\n4.4 Ablation Study\nDifferent Types of Attentions. To validate the important role of text-guided attention in our method,\nwe conducted experiments by replacing it with vision-based attention. We employ Grad-CAM [52], a\nwidely adopted method, for generating attention maps based on vision. Table 5 demonstrates that"}
{"chunk_id": "d00p0008c09", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 8, "text": "Different Types of Attentions. To validate the important role of text-guided attention in our method,\nwe conducted experiments by replacing it with vision-based attention. We employ Grad-CAM [52], a\nwidely adopted method, for generating attention maps based on vision. Table 5 demonstrates that\nreplacing the text-guided attention with vision-based attention yields results that are still comparable to\n8"}
{"chunk_id": "d00p0009c01", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 9, "text": "Table 5: Comparison of vision-based attention and our text-guided attention. We evaluate the state-\nof-the-art method PMG-AFT alongside our pipeline, incorporating two different types of attention\nmechanisms on Tiny-ImageNet and evaluating performance across 16 datasets.\nTest Methods Tiny-ImageNetCIFAR-10CIFAR-100STL-10SUN397Food101OxfordpetsFlowers102DTDEuroSATFGVC-AircraftImageNetCaltech-101Caltech-256StanfordCarsPCAMAverage\nRobustPMG-AFT[59]47.11 46.01 25.83 74.51 22.21 19.58 41.62 23.45 15.05 12.54 1.98 21.43 62.42 45.99 11.7248.6432.51Vision-based52.81 40.46 22.66 70.26 19.50 13.74 37.67 19.78 16.97 11.79 2.64 18.08 55.64 42.45 8.88 38.11 29.47TGA-ZSR (ours)63.97 61.82 35.25 83.99 32.78 34.13 56.91 34.20 21.92 14.20 4.44 28.62 70.53 59.70 21.1547.7541.96"}
{"chunk_id": "d00p0009c02", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 9, "text": "CleanPMG-AFT[59]67.11 74.62 44.68 88.85 37.42 37.47 66.34 35.66 21.17 17.76 4.71 35.93 76.70 61.96 25.21 49.99 46.60Vision-based74.31 70.77 41.03 87.24 36.91 30.07 62.52 33.89 24.10 16.26 5.70 33.59 72.35 59.75 20.5051.2945.02TGA-ZSR (ours)76.85 86.23 56.55 93.28 51.71 57.72 77.08 48.32 29.15 23.99 12.03 48.10 80.82 74.58 37.7249.6056.48\nTable 6: Zero-shot robust accuracy on images attacked with ε of 1/255, 2/255 and 4/255 of PGD [36].\nWe performed several different methods on Tiny-ImageNet and evaluated across 16 datasets. We\nrepresent the average accuracy across various attack strength.\nMethods Tiny-ImageNetCIFAR-10CIFAR-100STL-10SUN397Food101OxfordpetsFlowers102DTDEuroSATFGVC-AircraftImageNetCaltech-101Caltech-256StanfordCarsPCAMAverage"}
{"chunk_id": "d00p0009c03", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 9, "text": "CLIP [48]0.64 2.15 0.12 20.35 0.52 5.94 2.97 0.72 0.71 0.03 0.00 0.71 14.28 9.18 0.11 0.04 3.65FT-Clean12.44 18.80 4.65 37.16 0.43 0.52 2.03 0.41 0.92 0.02 0.01 0.54 13.02 7.96 0.03 0.44 6.21FT-Adv. 29.3318.10 11.06 45.13 8.58 5.65 16.45 10.15 9.729.82 0.83 8.81 33.43 24.14 3.8038.06 17.07TeCoA [38]18.17 12.78 8.12 39.87 8.90 6.53 16.61 11.0410.079.88 0.63 8.43 34.94 23.92 3.45 33.20 15.41PMG-AFT [59]25.30 21.7113.2947.6911.429.49 20.6812.869.45 10.650.9011.2841.8628.385.4037.88 19.27FARE [51]12.41 9.09 4.23 33.72 2.98 4.75 9.67 5.52 4.26 0.05 0.28 3.90 23.97 16.95 1.48 3.43 8.54TGA-ZSR (ours)33.4027.7214.6859.5912.4012.9924.6913.429.70 7.47 1.5311.6941.4432.517.2919.64 20.45\nthe state-of-the-art method PMG-AFT in terms of both zero-shot robust accuracy and clean accuracy."}
{"chunk_id": "d00p0009c04", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 9, "text": "the state-of-the-art method PMG-AFT in terms of both zero-shot robust accuracy and clean accuracy.\nThis finding validates the effectiveness of our method’s pipeline. Furthermore, our text-guided\nattention significantly improves the average accuracy, demonstrating the advantage of incorporating\ntextual guidance.\nEffect of Attack Strength. We further assess the robustness of the pre-trained model by different\nlevels of PGD-2 attacks. Specifically, we set ε to values of 1/255, 2/255 and 4/255, progressively\namplifying the magnitude of the adversarial perturbation. This allows us to investigate whether\na model trained on weak adversarial examples exhibits robustness against stronger adversarial"}
{"chunk_id": "d00p0009c05", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 9, "text": "levels of PGD-2 attacks. Specifically, we set ε to values of 1/255, 2/255 and 4/255, progressively\namplifying the magnitude of the adversarial perturbation. This allows us to investigate whether\na model trained on weak adversarial examples exhibits robustness against stronger adversarial\nperturbations. In Table 6, we present the average results for three distinct levels of attack strength.\nDespite a general decline in the robustness of all methods, our approach still achieves superior results,\noutperforming PMG-AFT by 1.18%, and FARE by 11.91%.\nEffect of Each Component. We conducted several experiments to thoroughly evaluate the effective-\nness of each component of our method, as summarized in Table 7. Using LCE alone significantly"}
{"chunk_id": "d00p0009c06", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 9, "text": "outperforming PMG-AFT by 1.18%, and FARE by 11.91%.\nEffect of Each Component. We conducted several experiments to thoroughly evaluate the effective-\nness of each component of our method, as summarized in Table 7. Using LCE alone significantly\nenhances the model’s robustness through standard adversarial training, but it also results in a notable\ndecrease in clean accuracy compared to the original CLIP model. Applying our Attention Refinement\nmodule further improves the average zero-shot accuracy on both adversarial and clean samples.\nFinally, the Attention-based Model Constraint module dramatically boosts performance, increasing\nrobustness by 10.25% and clean accuracy by 6.52%.\nTrade-off between Robust and Clean Accuracy.Achieving the balance between robustness and"}
{"chunk_id": "d00p0009c07", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 9, "text": "Finally, the Attention-based Model Constraint module dramatically boosts performance, increasing\nrobustness by 10.25% and clean accuracy by 6.52%.\nTrade-off between Robust and Clean Accuracy.Achieving the balance between robustness and\nclean accuracy is crucial in adversarial training. Overfitting in models tends to yield high robustness\nbut low clean accuracy, whereas underfitting typically results in the opposite scenario. As shown\nin Fig. 3, methods positioned close to the dotted line excel in either adversarial accuracy or clean\naccuracy, yet they often struggle to strike a balance between robustness and clean accuracy. In\ncontrast, our method demonstrates not only an enhancement in the model’s robustness but also the"}
{"chunk_id": "d00p0009c08", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 9, "text": "in Fig. 3, methods positioned close to the dotted line excel in either adversarial accuracy or clean\naccuracy, yet they often struggle to strike a balance between robustness and clean accuracy. In\ncontrast, our method demonstrates not only an enhancement in the model’s robustness but also the\nmaintenance of clean accuracy, resulting in an overall superior performance.\nMore comprehensive and detailed ablation studies, including hyperparameter selection, the effect of\ndistance metrics on the loss function, and the effect of learning rate, can be found in Supp. Mat. A.2.\n4.5 Computational Overhead and Time Efficiency\nWe have evaluated our method against others in terms of memory usage, training time, and test time,"}
{"chunk_id": "d00p0009c09", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 9, "text": "distance metrics on the loss function, and the effect of learning rate, can be found in Supp. Mat. A.2.\n4.5 Computational Overhead and Time Efficiency\nWe have evaluated our method against others in terms of memory usage, training time, and test time,\nand the results are summarized in Table 8. Our method increases memory consumption by approxi-\n9"}
{"chunk_id": "d00p0010c01", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 10, "text": "Table 7: Ablation study on each component. Af-\nter adversarial fine-tuning the model using adver-\nsarial examples generated by PGD-2, we verify\nthe robustness of the model using adversarial ex-\namples generated by PGD-100.\nRobust Clean Average\nCLIP 4.90 64.42 34.66\nLCE 29.45 44.97 37.21\n+LAR 31.71 49.96 40.84\n+LAMC 41.96 56.48 49.22\nFigure 3: The trade-off between robustness and\nclean accuracy. Each point on the graph repre-\nsents a method, with the size of the point indi-\ncating the extent to which it achieves a favorable\ntrade-off between robustness and clean accuracy.\nTable 8: Comparison of memory usage, training time, and test time.\nMethods Train memory usage Train time (per epoch / batch) Test time (per batch)\nCLIP [48] 0Mb 0s / 0s 21s\nTeCoA [38] 12873Mb 512s / 0.65s 21s"}
{"chunk_id": "d00p0010c02", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 10, "text": "trade-off between robustness and clean accuracy.\nTable 8: Comparison of memory usage, training time, and test time.\nMethods Train memory usage Train time (per epoch / batch) Test time (per batch)\nCLIP [48] 0Mb 0s / 0s 21s\nTeCoA [38] 12873Mb 512s / 0.65s 21s\nPMG-AFT[59] 18449Mb 828s / 1.06s 21s\nTGA-ZSR (ours) 21227Mb 885s / 1.13s 21s\nmately 15% compared to state-of-the-art method PMG-AFT. This is due to the additional computation\nrequired for the text-guided attention map. The training time for our method is comparable to that of\nPMG-AFT. The test time remains consistent across all methods.\n5 Conclusion and Limitations\nIn this paper, we discovered that adversarial attacks lead shift of text-guided attention. Building"}
{"chunk_id": "d00p0010c03", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 10, "text": "required for the text-guided attention map. The training time for our method is comparable to that of\nPMG-AFT. The test time remains consistent across all methods.\n5 Conclusion and Limitations\nIn this paper, we discovered that adversarial attacks lead shift of text-guided attention. Building\non this observation, we introduce a text-guided approach, TGA-ZSR, which incorporates two key\ncomponents to preform adversarial fine-tuning and constrain the model. This strategy prevents\nmodel drift while enhancing model robustness. Extensive experiments validate the performance\nof TGA-ZSR, which not only improves CLIP’s zero-shot adversarial robustness but also maintains\nzero-shot clean accuracy on clean examples, gaining a favorable balance."}
{"chunk_id": "d00p0010c04", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 10, "text": "model drift while enhancing model robustness. Extensive experiments validate the performance\nof TGA-ZSR, which not only improves CLIP’s zero-shot adversarial robustness but also maintains\nzero-shot clean accuracy on clean examples, gaining a favorable balance.\nLimitations. We use a simple text-guided attention mechanism by multiplying the text embedding\nand vision embedding which is effective against most attack types. However, for more challenging\nattacks such as AutoAttack, the improvement remains limited. This indicates that while our approach\nshows promise, it may require further refinement to enhance robustness under stronger adversarial\nscenarios.\nBorder Impact. Large-scale pre-trained vision-language models (VLMs) like CLIP [48] integrate vi-"}
{"chunk_id": "d00p0010c05", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 10, "text": "shows promise, it may require further refinement to enhance robustness under stronger adversarial\nscenarios.\nBorder Impact. Large-scale pre-trained vision-language models (VLMs) like CLIP [48] integrate vi-\nsual and textual data, revolutionizing applications such as image classification, semantic segmentation,\nand vision question answering. While these models excel in zero-shot learning and transfer learning,\nthey are vulnerable to adversarial attacks, posing risks in critical applications like autonomous vehi-\ncles and medical diagnosis. Adversarial training improves robustness but has practical challenges,\nincluding increased computational overhead and potential overfitting. Exploring zero-shot adversarial\nrobustness is essential to ensure reliability."}
{"chunk_id": "d00p0010c06", "doc_id": 1, "doc": "2410.21802v2.pdf", "page": 10, "text": "cles and medical diagnosis. Adversarial training improves robustness but has practical challenges,\nincluding increased computational overhead and potential overfitting. Exploring zero-shot adversarial\nrobustness is essential to ensure reliability.\nAcknowledgement. This work was supported by National Science and Technology Major Project\nunder Grant 2021ZD0112200, in part by the National Natural Science Foundation of China under\nGrants 62202331, U23A20387, 62036012, 62276118.\n10"}
{"chunk_id": "d01p0001c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 1, "text": "LangChain and Knowledge Graph RAG in \nDocument QA Systems\n1. Conceptual Overview and Theoretical Foundation\nLangChain is a high-level framework for building applications around Large Language Models \n(LLMs). Rather than using an LLM in isolation, LangChain treats the LLM as one building block \namong many. It provides abstractions for prompt engineering, conversation chaining, and \nintegration with external data sources. In LangChain, components like prompt templates, chains, \nagents, tools, retrievers, and memory work together to orchestrate LLM calls in a modular, \ncustomizable pipeline. For example, one can chain prompts and logic so that the model’s output can \nfeed into subsequent steps, enabling complex applications like multi-turn chatbots or data analysis"}
{"chunk_id": "d01p0001c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 1, "text": "agents, tools, retrievers, and memory work together to orchestrate LLM calls in a modular, \ncustomizable pipeline. For example, one can chain prompts and logic so that the model’s output can \nfeed into subsequent steps, enabling complex applications like multi-turn chatbots or data analysis \ntools. In effect, LangChain’s theoretical foundation is that an LLM’s knowledge and generation \ncapability are augmented by surrounding logic (retrieval, memory, tool calls) to improve accuracy \nand control. As one commentator notes, “LangChain is an LLM application framework, in which \nthe actual usage of LLMs is a mere building block, but effective user input parsing, prompt \nformulation, conversational chat history, and alignment of answers become part of LLM usage”."}
{"chunk_id": "d01p0001c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 1, "text": "and control. As one commentator notes, “LangChain is an LLM application framework, in which \nthe actual usage of LLMs is a mere building block, but effective user input parsing, prompt \nformulation, conversational chat history, and alignment of answers become part of LLM usage”.\nKnowledge-Graph RAG (GraphRAG) is a form of retrieval-augmented generation that \nincorporates a structured knowledge graph into the retrieval process. In traditional RAG, a user \nquery triggers retrieval of relevant text chunks (via embeddings or text search) which are passed to \nthe LLM. By contrast, Knowledge Graph RAG uses a graph database of typed entities and \nrelationships as its retrieval surface. Formally, a knowledge graph RAG system first constructs a"}
{"chunk_id": "d01p0001c04", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 1, "text": "query triggers retrieval of relevant text chunks (via embeddings or text search) which are passed to \nthe LLM. By contrast, Knowledge Graph RAG uses a graph database of typed entities and \nrelationships as its retrieval surface. Formally, a knowledge graph RAG system first constructs a \ngraph of entities and relations from source documents, then retrieves context by traversing relevant \nsubgraphs, and finally feeds the structured context into the LLM. This approach embeds explicit \nsemantics into retrieval: instead of just retrieving nearest-neighbor text passages, the system finds \nrelated entities via graph links, enabling multi-hop reasoning and explainability. Hypermode (a \nGenAI blog) emphasizes that KG-RAG “uses a formal knowledge graph as its foundation for"}
{"chunk_id": "d01p0001c05", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 1, "text": "semantics into retrieval: instead of just retrieving nearest-neighbor text passages, the system finds \nrelated entities via graph links, enabling multi-hop reasoning and explainability. Hypermode (a \nGenAI blog) emphasizes that KG-RAG “uses a formal knowledge graph as its foundation for \nretrieval” and introduces “explicit semantics through typed entities and relationships”. In other \nwords, GraphRAG augments RAG by letting the model navigate a network of meaning rather than \nan unstructured text corpus. This theoretically grounds answers in traceable facts (nodes/edges) and \nsupports complex queries that link concepts (e.g. finding how a regulation connects to a product via \nsupply-chain relations)."}
{"chunk_id": "d01p0001c06", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 1, "text": "an unstructured text corpus. This theoretically grounds answers in traceable facts (nodes/edges) and \nsupports complex queries that link concepts (e.g. finding how a regulation connects to a product via \nsupply-chain relations).\nTogether, these frameworks sit at opposite ends of the GenAI design spectrum: LangChain provides \na toolkit for orchestrating LLM workflows, while Knowledge Graph RAG provides a \nsemantically rich retrieval foundation. Both address the common RAG goal of grounding LLM \noutputs in real data, but LangChain emphasizes flexibility and modular pipelines, whereas KG-\nRAG emphasizes structured, explainable knowledge. In practice one can combine them: for \nexample, using LangChain to orchestrate an agent that retrieves from a knowledge graph index (as \nwe discuss below)."}
{"chunk_id": "d01p0002c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 2, "text": "2. LangChain: Key Components and Working Mechanisms\nLangChain breaks LLM applications into reusable components that manage prompts, external \ntools, memory, and data. The main building blocks include:\n• Chains. A Chain in LangChain is a sequence of one or more calls to LLMs or other \ncomponents. In effect, it encapsulates a multi-step workflow. For example, an LLMChain \nwraps an LLM with a prompt template; it formats inputs, calls the model, and handles \noutputs. As one user explains, “LLMChain is a chain that wraps an LLM to add additional \nfunctionality. It handles prompt formatting, input/output parsing, conversations, etc.”. In \npractice, simple question-answer chains or more complex map-reduce or sequential chains"}
{"chunk_id": "d01p0002c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 2, "text": "outputs. As one user explains, “LLMChain is a chain that wraps an LLM to add additional \nfunctionality. It handles prompt formatting, input/output parsing, conversations, etc.”. In \npractice, simple question-answer chains or more complex map-reduce or sequential chains \ncan be built. LangChain provides specialized chain types (e.g. “sequential” chain that \nthreads outputs into inputs, or “router” chains that use an LLM to choose a processing path).\n• Agents and Tools. An Agent is an automated decision-maker that can invoke tools. Agents \nuse LLMs to decide what actions to take next, given the current state. They typically operate \nin a loop: at each step the LLM looks at the conversation (including tool outputs), and"}
{"chunk_id": "d01p0002c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 2, "text": "• Agents and Tools. An Agent is an automated decision-maker that can invoke tools. Agents \nuse LLMs to decide what actions to take next, given the current state. They typically operate \nin a loop: at each step the LLM looks at the conversation (including tool outputs), and \noutputs either text or a command (tool call). For example, an agent might ask the LLM \n“should we call the calculator tool or the search tool?” and then execute the appropriate \naction. LangChain conceptual documentation defines agents as systems where “an LLM…\nchooses a sequence of actions to take”, enabling dynamic workflows. Tools are external \nfunctions or APIs that an agent can call. Each tool has a defined schema (name, description,"}
{"chunk_id": "d01p0002c04", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 2, "text": "action. LangChain conceptual documentation defines agents as systems where “an LLM…\nchooses a sequence of actions to take”, enabling dynamic workflows. Tools are external \nfunctions or APIs that an agent can call. Each tool has a defined schema (name, description, \ninput arguments). For instance, a web-search tool or a SQL-query tool can be registered, and \nthe agent’s LLM can invoke it by outputting the tool name and arguments. This tool-calling \nmechanism is supported by Chat model APIs: the LLM can be fed the tool schemas and in \nits output it can “return” a tool invocation. In short, LangChain agents combine LLM \nreasoning with external capabilities.\n• Retrievers and Vector Stores. A Retriever in LangChain abstracts the process of fetching"}
{"chunk_id": "d01p0002c05", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 2, "text": "its output it can “return” a tool invocation. In short, LangChain agents combine LLM \nreasoning with external capabilities.\n• Retrievers and Vector Stores. A Retriever in LangChain abstracts the process of fetching \nrelevant documents for a query. Commonly, this is implemented on top of a vector database: \nembeddings of documents are stored, and a similarity search returns top-K matching chunks. \nLangChain provides connectors to vector stores (Chroma, FAISS, Pinecone, etc.) and wraps \nthem as retrievers. For example, one can do:\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.chains import RetrievalQA\ntexts = [\"Doc1 text\", \"Doc2 text\", ...]\ndb = Chroma.from_texts(texts, OpenAIEmbeddings())"}
{"chunk_id": "d01p0002c06", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 2, "text": "them as retrievers. For example, one can do:\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.chains import RetrievalQA\ntexts = [\"Doc1 text\", \"Doc2 text\", ...]\ndb = Chroma.from_texts(texts, OpenAIEmbeddings())\nretriever = db.as_retriever()      # a LangChain retriever\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), retriever=retriever)\nanswer = qa.run(\"Query?\")\nHere, RetrievalQA is a chain that uses a retriever under the hood. This pattern is central \nto LangChain’s document QA: split text into chunks, embed and store them, then retrieve by \nsimilarity when answering. (We will illustrate such a pipeline with an image below.)\n• Memory. LangChain includes memory modules to store conversation or application state."}
{"chunk_id": "d01p0002c07", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 2, "text": "to LangChain’s document QA: split text into chunks, embed and store them, then retrieve by \nsimilarity when answering. (We will illustrate such a pipeline with an image below.)\n• Memory. LangChain includes memory modules to store conversation or application state. \nThe simplest is buffer memory, which just appends chat history. More advanced types \ninclude window memory (keeping only the last k interactions) and entity memory, which \nextracts and summarizes named entities across turns. For example, entity memory uses an"}
{"chunk_id": "d01p0003c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 3, "text": "LLM to pick out key entities from each user message and accumulate them. This way, an \nagent can “remember” facts (like “the user’s favorite sports team is the Lakers”) across \nturns. LangChain currently offers many built-in memory strategies (with various providers \nlike in-memory, Redis, etc.). In general, memory helps LangChain maintain context \n(especially in chat/agent flows) without re-prompting all history each time.\n• PromptTemplates and Formatting. A core piece is flexible prompt templates. LangChain \nlets you define templates with input variables (e.g. \"Translate to French: \n{text}\") and then supply input values at runtime. The template system handles formatting \nand ensures consistent prompt construction. For more complex tasks, LangChain provides"}
{"chunk_id": "d01p0003c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 3, "text": "lets you define templates with input variables (e.g. \"Translate to French: \n{text}\") and then supply input values at runtime. The template system handles formatting \nand ensures consistent prompt construction. For more complex tasks, LangChain provides \nprompt libraries (chain-of-thought, few-shot examples, etc.) but the basics rely on these \nsimple template calls.\nLangChain’s architecture can be summarized as a modular pipeline (Figure 1). In the indexing \nphase, documents are “Load”ed (from PDFs, DBs, etc.), then “Split” into smaller chunks. Each \nchunk is run through an embedding model (“Embed”) and the resulting vectors are “Store”d in a \nvector store (or other retriever)【83†】. At query time, the retriever finds relevant embeddings"}
{"chunk_id": "d01p0003c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 3, "text": "phase, documents are “Load”ed (from PDFs, DBs, etc.), then “Split” into smaller chunks. Each \nchunk is run through an embedding model (“Embed”) and the resulting vectors are “Store”d in a \nvector store (or other retriever)【83†】. At query time, the retriever finds relevant embeddings \n(“Retrieve”), the LLM is prompted with the query plus retrieved context (“Prompt”), and the \nmodel outputs an answer【84†】. This parallels standard RAG architectures, but LangChain adds \nlayers: chains to process steps, agents to add logic, and memory to maintain context.\nFigure 1: LangChain RAG indexing pipeline. Documents are loaded, split into passages, embedded \nvia an embedding model, and stored in a vector database for later retrieval. In practice, tools like"}
{"chunk_id": "d01p0003c04", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 3, "text": "layers: chains to process steps, agents to add logic, and memory to maintain context.\nFigure 1: LangChain RAG indexing pipeline. Documents are loaded, split into passages, embedded \nvia an embedding model, and stored in a vector database for later retrieval. In practice, tools like \nChroma, FAISS, or Neo4j’s Vector Index can serve as the vector store. For example, LangChain’s \nNeo4jVector.from_existing_graph(...) can index graph-stored texts with OpenAI \nembeddings (see next section).\nFigure 2: LangChain RAG retrieval and generation flow. A user “Question” triggers a similarity \nsearch (Retrieve) over stored embeddings; the top context passages plus the query form the LLM \nprompt; and the LLM (e.g. GPT) produces the “Answer”. This flow is the core of LangChain’s"}
{"chunk_id": "d01p0003c05", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 3, "text": "Figure 2: LangChain RAG retrieval and generation flow. A user “Question” triggers a similarity \nsearch (Retrieve) over stored embeddings; the top context passages plus the query form the LLM \nprompt; and the LLM (e.g. GPT) produces the “Answer”. This flow is the core of LangChain’s \nretrieval QA (using classes like RetrievalQA or ConversationalRetrievalChain). \nNotably, LangChain also supports variants like threaded or map-reduce chains that chunk and \nreassemble answers for very large documents.\n3. Knowledge Graph RAG: Key Components and Mechanisms\nKnowledge-Graph RAG extends RAG by leveraging structured graph data. Its main components \nare:\n• Knowledge Graph Database. At the heart is a graph database (Neo4j, Amazon Neptune,"}
{"chunk_id": "d01p0003c06", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 3, "text": "reassemble answers for very large documents.\n3. Knowledge Graph RAG: Key Components and Mechanisms\nKnowledge-Graph RAG extends RAG by leveraging structured graph data. Its main components \nare:\n• Knowledge Graph Database. At the heart is a graph database (Neo4j, Amazon Neptune, \nTigerGraph, etc.) that stores nodes (entities) and edges (relationships). The graph schema is \ndomain-specific: nodes might represent persons, products, locations, etc., and edges might \nrepresent works_at, located_in, part_of relationships. Graph databases allow complex \nqueries (via languages like Cypher or Gremlin) and graph algorithms (path finding, \ncommunity detection). In RAG, the graph is the index for retrieval. For instance, a"}
{"chunk_id": "d01p0003c07", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 3, "text": "represent works_at, located_in, part_of relationships. Graph databases allow complex \nqueries (via languages like Cypher or Gremlin) and graph algorithms (path finding, \ncommunity detection). In RAG, the graph is the index for retrieval. For instance, a \ncompany’s PDF manuals and support logs can be processed into a Neo4j graph of products, \nissues, and solutions, enabling relation-based lookup."}
{"chunk_id": "d01p0004c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 4, "text": "• Entity Extraction and Linking. To build the graph, entity linking is used: unstructured \ntext is parsed into graph triples. This often uses NLP tools or LLMs to identify entities and \nrelation phrases. For example, one might use an LLM to extract triplets like <“Sarah”, \nworks_for, “AcmeCorp”> from a sentence. MongoDB’s GraphRAG blog explains \nthis: instead of vector embeddings, an entity extraction model (often another LLM) converts \ndocuments into KG triples. Tools like OpenAI Functions or LangChain’s new graph-\nconstruction modules can automate KG creation by feeding text to the LLM with prompts to \noutput entities and relations. The result is a knowledge graph (possibly stored in Neo4j or \neven a simple document-store graph) that encodes the domain knowledge."}
{"chunk_id": "d01p0004c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 4, "text": "construction modules can automate KG creation by feeding text to the LLM with prompts to \noutput entities and relations. The result is a knowledge graph (possibly stored in Neo4j or \neven a simple document-store graph) that encodes the domain knowledge.\n• Graph Traversal and Retrieval. Once the graph exists, retrieval is performed by graph \nqueries rather than text similarity. Given a user question, the system translates it into a graph \nquery (e.g. a Cypher query) or directly traverses the graph. For example, a query “Which \nemployees moved to New York in 2024?” may be answered by traversing edges like \n(:Person)-[:moved_to {year:2024}]->(:Location{name:\"New \nYork\"}). LangChain provides the GraphCypherQAChain, where an LLM first"}
{"chunk_id": "d01p0004c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 4, "text": "query (e.g. a Cypher query) or directly traverses the graph. For example, a query “Which \nemployees moved to New York in 2024?” may be answered by traversing edges like \n(:Person)-[:moved_to {year:2024}]->(:Location{name:\"New \nYork\"}). LangChain provides the GraphCypherQAChain, where an LLM first \ngenerates a Cypher query from the natural language question, and then this query is \nexecuted on Neo4j. This leverages the graph’s structure: instead of loosely matching vectors, \nthe RAG system finds exact or semantically defined connections. For instance, even if two \ndocuments don’t share vocabulary, if their concepts are linked in the graph, they can be \nretrieved together. Hybrid strategies are common: e.g. first retrieve relevant entities via"}
{"chunk_id": "d01p0004c04", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 4, "text": "the RAG system finds exact or semantically defined connections. For instance, even if two \ndocuments don’t share vocabulary, if their concepts are linked in the graph, they can be \nretrieved together. Hybrid strategies are common: e.g. first retrieve relevant entities via \nembedding search, then expand via graph edges (as in KG2RAG).\n• Graph Reasoning and Inference. A key advantage of KG-RAG is multi-hop reasoning. \nBecause the graph explicitly encodes relations, the system can perform multi-step inference. \nFor example, to answer “How is product X affected by regulation Y?”, the system might \nfind a path Product X –[made_by]→ AcmeCorp –[subject_to]→ \nRegulation Y. The graph’s schema and the query language support this reasoning"}
{"chunk_id": "d01p0004c05", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 4, "text": "For example, to answer “How is product X affected by regulation Y?”, the system might \nfind a path Product X –[made_by]→ AcmeCorp –[subject_to]→ \nRegulation Y. The graph’s schema and the query language support this reasoning \nnatively. Advanced pipelines may incorporate graph algorithms (shortest path, PageRank, \netc.) to find influential nodes or chain relationships. Moreover, some approaches use the \nLLM to augment reasoning by querying the graph, then summarizing the results. The \noutcome is more explainable answers: the path taken in the graph can be traced, unlike a \nblack-box embedding retrieval.\nIn summary, a Knowledge-Graph RAG system works as follows (see Hypermode’s outline): (1) \nGraph Construction: ingest documents and extract triples into a KG; (2) Contextual Retrieval:"}
{"chunk_id": "d01p0004c06", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 4, "text": "black-box embedding retrieval.\nIn summary, a Knowledge-Graph RAG system works as follows (see Hypermode’s outline): (1) \nGraph Construction: ingest documents and extract triples into a KG; (2) Contextual Retrieval: \nfor each user query, navigate the graph to select a subgraph of relevant entities (using semantic \nmatching or direct queries); (3) LLM Integration: serialize the retrieved subgraph (or its contents) \ninto a prompt. Because the graph relations are explicit, the LLM is given a clean, contextually \nlinked set of facts. This generally yields more precise, multi-hop-complete answers than flat text \nretrieval."}
{"chunk_id": "d01p0005c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 5, "text": "4. Architectural Workflows\n4.1 LangChain RAG Workflow\nLangChain’s RAG architecture involves an offline indexing pipeline and an online query pipeline. \nWe illustrated this with LangChain’s own diagrams above【83†】 and 【84†】. Concretely, a \ndeveloper might:\n1. Load and Index Data: Use LangChain document loaders to read PDFs, HTML, JSON, etc. \nThen use text splitters to break long docs into smaller passages. Pass each passage through \nan embedding model (e.g. OpenAIEmbeddings, SentenceTransformers, etc.) and \nstore the resulting vectors in a vector store (Chroma, Pinecone, FAISS, or even a Neo4j \nVector index). This creates a searchable index of all knowledge.\n2. Run RetrievalQA Chain: At runtime, the user’s question is fed to a chain like"}
{"chunk_id": "d01p0005c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 5, "text": "store the resulting vectors in a vector store (Chroma, Pinecone, FAISS, or even a Neo4j \nVector index). This creates a searchable index of all knowledge.\n2. Run RetrievalQA Chain: At runtime, the user’s question is fed to a chain like \nRetrievalQA or ConversationalRetrievalChain. The chain’s retriever \nperforms a similarity search over the index, returning the most relevant chunks. These are \nconcatenated with the question according to a prompt template (or passed into a chain-of-\nthought chain) and the LLM (e.g. GPT-4o, Llama 3, Claude) generates an answer.\n3. (Optional) Use Agents or Memory: For interactive systems, LangChain agents can \ndynamically choose tools during an answer. For example, a QA agent might decide to call a"}
{"chunk_id": "d01p0005c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 5, "text": "thought chain) and the LLM (e.g. GPT-4o, Llama 3, Claude) generates an answer.\n3. (Optional) Use Agents or Memory: For interactive systems, LangChain agents can \ndynamically choose tools during an answer. For example, a QA agent might decide to call a \ncalculator tool or fetch a web page mid-answer. Memory modules can maintain conversation \nstate across turns (e.g. tracking that “Alice is a customer of ACME” from an earlier turn).\nThe key architectural benefit is modularity. Each component (loader, splitter, retriever, LLM, \nprompt template) is replaceable. One can, for instance, swap out the vector store for a local SQLite, \nor the LLM for a local LLaMA model. LangChain simply wires these together as shown in Fig.1–2."}
{"chunk_id": "d01p0005c04", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 5, "text": "The key architectural benefit is modularity. Each component (loader, splitter, retriever, LLM, \nprompt template) is replaceable. One can, for instance, swap out the vector store for a local SQLite, \nor the LLM for a local LLaMA model. LangChain simply wires these together as shown in Fig.1–2. \nNote that LangChain also abstracts alternate retrieval strategies: multi-query, hierarchical, or \ncompressor chains can be inserted to improve retrieval.\n4.2 Knowledge Graph RAG Workflow\nA Knowledge Graph RAG workflow (Fig.3) differs in the retrieval stage. Typically the steps are:\n1. Graph Construction: Use NLP or LLM tools to parse source texts into a graph. This might \nbe done once or updated incrementally. For example, one can prompt an LLM with: “Extract"}
{"chunk_id": "d01p0005c05", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 5, "text": "A Knowledge Graph RAG workflow (Fig.3) differs in the retrieval stage. Typically the steps are:\n1. Graph Construction: Use NLP or LLM tools to parse source texts into a graph. This might \nbe done once or updated incrementally. For example, one can prompt an LLM with: “Extract \nentities and relations from this paragraph” to get triples. These triples are written into a \ngraph database (e.g. using the Neo4j Bolt connector). In some setups, text is both embedded \nand stored in the graph nodes (Neo4j’s vector index can attach embeddings to nodes).\n2. Query Routing: When a question comes in, the system may decide to use pure KG retrieval \nor combine it with vector search. For instance, the example in Fig.3 (from Neo4j) shows a"}
{"chunk_id": "d01p0005c06", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 5, "text": "and stored in the graph nodes (Neo4j’s vector index can attach embeddings to nodes).\n2. Query Routing: When a question comes in, the system may decide to use pure KG retrieval \nor combine it with vector search. For instance, the example in Fig.3 (from Neo4j) shows a \nuser query branching to “Graph QA” and “Graph QA with Vector Search” subchains\n【90†】. The Graph QA path would involve only the structured graph: the LLM generates a \nCypher query (via GraphCypherQAChain) and executes it to get an answer. The \nGraph+Vector path might first retrieve relevant text via embeddings to identify seed nodes, \nthen use graph traversal."}
{"chunk_id": "d01p0006c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 6, "text": "3. Graph Query and Reasoning: Using the chosen path, the system either purely queries the \ngraph (via Cypher generated by an LLM or pre-written) or does a hybrid search. The result \nis usually a set of nodes or a small subgraph that contains the answer.\n4. LLM Answer Generation: The retrieved graph context is turned into a prompt. Often this \nmeans converting triples back into text or JSON and including them with the query to the \nLLM. For example, the Neo4j documentation notes: “The LangChain \nGraphCypherQAChain will then submit the generated Cypher query to a graph database… \nto retrieve query output. Finally, the LLM will return a response based on the initial query \nand graph response.”.\n5. Result: The LLM’s answer is thus grounded on the graph query result. Importantly, each"}
{"chunk_id": "d01p0006c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 6, "text": "GraphCypherQAChain will then submit the generated Cypher query to a graph database… \nto retrieve query output. Finally, the LLM will return a response based on the initial query \nand graph response.”.\n5. Result: The LLM’s answer is thus grounded on the graph query result. Importantly, each \nanswer can often be explained by the paths or nodes used, enhancing traceability.\nFigure 3: Example GraphRAG flow (source: Neo4j). A user query may be handled by a pure \nGraph-QA chain or a hybrid chain (Graph QA + vector search) before producing an answer. In this \ndiagram, the query is routed to different subchains, each returning a response that is ultimately \nmerged. In practice, most real systems implement one of these paths: e.g. a Language Model"}
{"chunk_id": "d01p0006c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 6, "text": "Graph-QA chain or a hybrid chain (Graph QA + vector search) before producing an answer. In this \ndiagram, the query is routed to different subchains, each returning a response that is ultimately \nmerged. In practice, most real systems implement one of these paths: e.g. a Language Model \ngenerating a Cypher query (Graph QA), or a vector retriever followed by graph traversal.\n5. Combining LangChain and Graph RAG in Document QA\nLangChain and Knowledge-Graph RAG are complementary. In a document QA system, one can use \nthem individually or together:\n• LangChain-Only QA: For many applications, LangChain with a vector retriever suffices. \nThe entire document corpus is indexed as vectors, and a simple RetrievalQA or \nConversationalRetrievalChain handles queries【84†】. LangChain’s agents can"}
{"chunk_id": "d01p0006c04", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 6, "text": "them individually or together:\n• LangChain-Only QA: For many applications, LangChain with a vector retriever suffices. \nThe entire document corpus is indexed as vectors, and a simple RetrievalQA or \nConversationalRetrievalChain handles queries【84†】. LangChain’s agents can \nbe added for extra capabilities (e.g. a math calculator tool for numeric answers). This \napproach works well when factual precision is moderate and the corpus is mostly text. Real-\nworld examples include chatbots over PDF manuals or customer service logs. For instance, \none might load all helpdesk tickets into Chroma, and let the user ask questions about past \nsupport issues.\n• KG-RAG-Only QA: In domains where relationships matter (e.g. enterprise search,"}
{"chunk_id": "d01p0006c05", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 6, "text": "world examples include chatbots over PDF manuals or customer service logs. For instance, \none might load all helpdesk tickets into Chroma, and let the user ask questions about past \nsupport issues.\n• KG-RAG-Only QA: In domains where relationships matter (e.g. enterprise search, \ncompliance, biotech), a pure Graph RAG may be used. Here, all relevant entities from the \ncorpus are stored in a graph. The QA system translates queries into graph queries (via \nLangChain’s GraphCypherQAChain or custom code) and answers from the graph. For \nexample, a legal KB might link statutes and cases; a query like “Which regulations affect \nProduct X?” can be answered by graph traversal. The advantage is precision: as Hypermode"}
{"chunk_id": "d01p0006c06", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 6, "text": "LangChain’s GraphCypherQAChain or custom code) and answers from the graph. For \nexample, a legal KB might link statutes and cases; a query like “Which regulations affect \nProduct X?” can be answered by graph traversal. The advantage is precision: as Hypermode \nnotes, KG-RAG “delivers more deliberate context construction, less token waste, and \nhigher explainability” compared to plain RAG. However, building and maintaining the \ngraph can be labor-intensive.\n• Hybrid QA (LangChain + Graph): Often the best solution is hybrid. LangChain can \norchestrate both vector and graph retrieval. For example, one could use a RetrievalQA \nchain that first does a vector search to find relevant passages, then passes those passages"}
{"chunk_id": "d01p0006c07", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 6, "text": "graph can be labor-intensive.\n• Hybrid QA (LangChain + Graph): Often the best solution is hybrid. LangChain can \norchestrate both vector and graph retrieval. For example, one could use a RetrievalQA \nchain that first does a vector search to find relevant passages, then passes those passages \nthrough a GraphCypherQAChain to refine via the graph. Or vice versa: use the graph to \nfind a subgraph of interest, then use LangChain to retrieve and summarize text from those"}
{"chunk_id": "d01p0007c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 7, "text": "nodes. Databricks and Neo4j have shown such workflows: an initial LLM retrieves seeds, \nthe graph refines multi-hop answers, and the LLM then crafts the final answer. LangChain’s \nmodular chains make this easy: you can nest a GraphQA chain inside a larger QA chain, or \nroute queries between them.\n• Memory-Augmented QA: LangChain’s memory features can be applied in either case to \nremember prior conversation context or graph states. For instance, a user might first provide \nan entity (“We have data on CometX Prodigy”), and the system can store that in memory so \nsubsequent queries automatically focus on that entity, even if not repeated in later questions.\nIn code terms, integrating them might look like:\nfrom langchain.chains import RetrievalQA, GraphCypherQAChain"}
{"chunk_id": "d01p0007c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 7, "text": "an entity (“We have data on CometX Prodigy”), and the system can store that in memory so \nsubsequent queries automatically focus on that entity, even if not repeated in later questions.\nIn code terms, integrating them might look like:\nfrom langchain.chains import RetrievalQA, GraphCypherQAChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.graphs.neo4j_graph import Neo4jGraph\n# Vector-based retriever over documents\ndocs = [\"Doc text about Product A ...\", \"Doc about Regulation 123 ...\"]\ndb = Chroma.from_texts(docs, OpenAIEmbeddings())\nretriever = db.as_retriever()\n# Graph QA chain (requires a Neo4j instance with a loaded graph)"}
{"chunk_id": "d01p0007c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 7, "text": "# Vector-based retriever over documents\ndocs = [\"Doc text about Product A ...\", \"Doc about Regulation 123 ...\"]\ndb = Chroma.from_texts(docs, OpenAIEmbeddings())\nretriever = db.as_retriever()\n# Graph QA chain (requires a Neo4j instance with a loaded graph)\nneo4j_graph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", \npassword=\"secret\")\n# Combine: first retrieve docs, then query graph\nllm = ChatOpenAI(temperature=0)\nvector_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, \nchain_type=\"stuff\")\ngraph_chain = GraphCypherQAChain.from_llm(llm, neo4j_graph)\nquestion = \"How does policy X impact Product A?\"\n# Example: run vector QA then graph QA (alternatively, could route between them)\npartial = vector_chain.run(question)\nanswer = graph_chain.run(partial)\nprint(answer)"}
{"chunk_id": "d01p0007c04", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 7, "text": "chain_type=\"stuff\")\ngraph_chain = GraphCypherQAChain.from_llm(llm, neo4j_graph)\nquestion = \"How does policy X impact Product A?\"\n# Example: run vector QA then graph QA (alternatively, could route between them)\npartial = vector_chain.run(question)\nanswer = graph_chain.run(partial)\nprint(answer)\nThis sketch shows how LangChain chains could be composed. (In practice, one would prompt the \ngraph chain with a Cypher template and handle JSON output properly.) The key is that LangChain’s \ncomponents cooperate: document retrieval can narrow the search space for the graph, and vice \nversa.\n6. Integration with Vector Stores, Embedding Models, and \nLLMs\nBoth LangChain and Graph RAG rely on underlying models and stores:"}
{"chunk_id": "d01p0007c05", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 7, "text": "components cooperate: document retrieval can narrow the search space for the graph, and vice \nversa.\n6. Integration with Vector Stores, Embedding Models, and \nLLMs\nBoth LangChain and Graph RAG rely on underlying models and stores:\n• Local vs Cloud. LangChain can run with fully local models (e.g. a local LLaMA or Mistral \n7B) and local vector stores (FAISS, Chroma, local SQLite). Alternatively, it can use cloud \nLLM APIs (OpenAI, Anthropic) and managed vector DBs (Pinecone, Weaviate). Similarly, \nknowledge graphs can be local (Neo4j Desktop, JanusGraph) or cloud (Neo4j Aura, AWS \nNeptune). The deployment choice affects scalability and latency. For example, a small-scale"}
{"chunk_id": "d01p0008c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 8, "text": "on-premise QA might use a CPU model and Chroma; a production service might use \nOpenAI’s GPT-4o with Pinecone and Neo4j Aura.\n• Embedding Models. The retrieval step depends on good embeddings. Common choices \ninclude OpenAI’s text-embedding-3 models or open-source SentenceTransformer models. \nEmbeddings can be generated on demand or precomputed. LangChain makes it easy: one \ncan simply pass an embedding class to the vector store (OpenAIEmbeddings(), \nHuggingFaceEmbeddings(), etc.). For graph RAG, embeddings may also be used to \nindex entities (Neo4j Vector Index attaches an “embedding” property to graph nodes).\n• LLMs. LangChain is model-agnostic. It supports any chat or completion model with an API. \nPopular picks include OpenAI’s GPT-4 (for high accuracy), Mistral 7B (small, fast), or"}
{"chunk_id": "d01p0008c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 8, "text": "index entities (Neo4j Vector Index attaches an “embedding” property to graph nodes).\n• LLMs. LangChain is model-agnostic. It supports any chat or completion model with an API. \nPopular picks include OpenAI’s GPT-4 (for high accuracy), Mistral 7B (small, fast), or \nLlama 3. The ChatOpenAI or OpenAI classes in LangChain wrap these. Graph RAG can \nuse the same LLM for both generating queries and final answers. For example, a system \nmight use GPT-4 for high-end accuracy, or use an open model like Llama locally to avoid \nAPI cost.\n• GraphDB + Vector Integration. Newer developments blur the line between vector and \ngraph stores. Neo4j’s Vector Index allows one to index graph nodes by vector and do \ncombined graph+vector queries. LangChain now supports it via Neo4jVector. For"}
{"chunk_id": "d01p0008c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 8, "text": "API cost.\n• GraphDB + Vector Integration. Newer developments blur the line between vector and \ngraph stores. Neo4j’s Vector Index allows one to index graph nodes by vector and do \ncombined graph+vector queries. LangChain now supports it via Neo4jVector. For \ninstance, one can embed all text fields into a Neo4j property and then use LangChain’s \nretriever to query it. This means the graph itself can serve as a vector store, unifying the \npipelines.\n• Memory and State. On the integration side, LangChain’s memory modules may use Redis \nor other stores. If running in a distributed environment, one might connect memory to a \nshared database.\nIn practice, the main deployment considerations are: latency and scale for LLM calls and vector"}
{"chunk_id": "d01p0008c04", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 8, "text": "• Memory and State. On the integration side, LangChain’s memory modules may use Redis \nor other stores. If running in a distributed environment, one might connect memory to a \nshared database.\nIn practice, the main deployment considerations are: latency and scale for LLM calls and vector \nsearches, and graph size/performance. Knowledge graphs with millions of nodes may require a \ncluster. Embedding large document sets (GBs of text) requires storage and search infrastructure. \nOne must also handle token limits: RAG helps mitigate this by retrieving only relevant context. \nFinally, for on-premise use, open-source embeddings and LLMs are crucial; for cloud, managed \nservices simplify maintenance.\n7. Real-World Implementations and Use Cases"}
{"chunk_id": "d01p0008c05", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 8, "text": "One must also handle token limits: RAG helps mitigate this by retrieving only relevant context. \nFinally, for on-premise use, open-source embeddings and LLMs are crucial; for cloud, managed \nservices simplify maintenance.\n7. Real-World Implementations and Use Cases\nAcademic and R&D: Recent research (e.g. KG2RAG) has demonstrated that integrating KGs into \nRAG improves answer coherence. In experiments on the HotpotQA dataset, Zhu et al. showed that \na KG-guided RAG outperforms standard RAG in multi-hop question quality. DataCamp and Neo4j \nhave published tutorials on building GraphRAG systems (often using LangChain) for use cases like \ncyber-security (BloodHound AD data).\nEnterprise: Many companies use LangChain and graph RAG in domains requiring precise QA. For"}
{"chunk_id": "d01p0008c06", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 8, "text": "have published tutorials on building GraphRAG systems (often using LangChain) for use cases like \ncyber-security (BloodHound AD data).\nEnterprise: Many companies use LangChain and graph RAG in domains requiring precise QA. For \nexample, Neo4j’s own developer blog describes a system combining Neo4j’s graph and vector \nindex with LangChain to answer complex queries in corporate knowledge bases. Similarly, \nDatabricks published GraphRAG pipelines for security analytics. Large enterprises in finance, \nhealthcare, and law are exploring KG-RAG because regulatory answers must be traceable."}
{"chunk_id": "d01p0009c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 9, "text": "Products and Tools: Many open-source projects leverage these ideas. LangChain itself is used in \ndozens of commercial projects. Vector stores like FAISS, Chroma, Pinecone, and Weaviate have \ntutorials combining them with LangChain for document QA. Neo4j’s community edition and Atlas \nDB support GraphRAG demos. Tools like Haystack (by deepset) and LlamaIndex also implement \nvariants of RAG/graph retrieval (see next section).\nLimitations: These systems have caveats. LangChain pipelines can suffer from hallucinations if the \nprompt/context isn’t carefully controlled. Knowledge graphs require accurate entity linking – errors \nhere can lead to wrong answers. Graph queries may be slow for very large graphs, and schema"}
{"chunk_id": "d01p0009c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 9, "text": "Limitations: These systems have caveats. LangChain pipelines can suffer from hallucinations if the \nprompt/context isn’t carefully controlled. Knowledge graphs require accurate entity linking – errors \nhere can lead to wrong answers. Graph queries may be slow for very large graphs, and schema \ndesign (choosing what entities/edges to include) is critical. Often a hybrid approach (dense retrieval \nplus graph reasoning) is used to balance recall and precision.\n8. Comparison with Alternative Frameworks\n• Haystack (deepset): Haystack is an end-to-end open-source QA framework with built-in \npipelines for indexing (ElasticSearch, Weaviate, FAISS) and QA (“Reader” models). Unlike \nLangChain, which is a general toolkit, Haystack is focused on production-ready NLP"}
{"chunk_id": "d01p0009c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 9, "text": "• Haystack (deepset): Haystack is an end-to-end open-source QA framework with built-in \npipelines for indexing (ElasticSearch, Weaviate, FAISS) and QA (“Reader” models). Unlike \nLangChain, which is a general toolkit, Haystack is focused on production-ready NLP \npipelines (question answering, semantic search). It has ready-made components (e.g. \nDocumentStore, Retriever, Reader) and emphasizes scalability and orchestration. \nHaystack’s architecture is modular (you can mix and match retrievers and readers) but it is \nless flexible than LangChain for custom LLM workflows. In practice, LangChain offers \nmore fine-grained control (agents, memory), whereas Haystack offers out-of-the-box end-to-\nend pipelines. For example, one could say: Haystack is ideal for building a scalable QA"}
{"chunk_id": "d01p0009c04", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 9, "text": "less flexible than LangChain for custom LLM workflows. In practice, LangChain offers \nmore fine-grained control (agents, memory), whereas Haystack offers out-of-the-box end-to-\nend pipelines. For example, one could say: Haystack is ideal for building a scalable QA \nsystem with Elasticsearch, whereas LangChain is ideal for a bespoke agent that calls \nmultiple tools.\n• LlamaIndex (GPT Index): LlamaIndex (now GPT Index) provides data indexing layers for \nLLMs. It specializes in creating structured indices (trees, lists, graph-based) over large \ncorpora to optimize LLM querying. Compared to LangChain, LlamaIndex is more focused \non data management (e.g. creating vector or graph indexes and managing LLM context). It"}
{"chunk_id": "d01p0009c05", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 9, "text": "LLMs. It specializes in creating structured indices (trees, lists, graph-based) over large \ncorpora to optimize LLM querying. Compared to LangChain, LlamaIndex is more focused \non data management (e.g. creating vector or graph indexes and managing LLM context). It \noffers various index types (vector indices, keyword tables, SQL databases) and makes it easy \nto connect text data sources to an LLM. In contrast, LangChain focuses on chaining \noperations. LlamaIndex is often used under the hood in QA apps to structure data; \nLangChain can call into a LlamaIndex as a retriever. Essentially, LlamaIndex provides smart \nindexing of data for LLMs, while LangChain provides the orchestration/logic around the \nLLM calls."}
{"chunk_id": "d01p0009c06", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 9, "text": "operations. LlamaIndex is often used under the hood in QA apps to structure data; \nLangChain can call into a LlamaIndex as a retriever. Essentially, LlamaIndex provides smart \nindexing of data for LLMs, while LangChain provides the orchestration/logic around the \nLLM calls.\n• Traditional Graph Pipelines: Before these frameworks, one would build a KG RAG \nsystem by hand: write NLP scripts to extract triples, load them into Neo4j, then write custom \nquery-generation code. LangChain and related tools automate much of this: the \nGraphCypherQAChain can generate queries, and connectors simplify DB access. \nCompared to a “hand-rolled” Neo4j pipeline, LangChain offers reusability and integration \nwith LLMs. However, a traditional pipeline might be more optimized for a specific domain"}
{"chunk_id": "d01p0009c07", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 9, "text": "GraphCypherQAChain can generate queries, and connectors simplify DB access. \nCompared to a “hand-rolled” Neo4j pipeline, LangChain offers reusability and integration \nwith LLMs. However, a traditional pipeline might be more optimized for a specific domain \nif carefully engineered.\n• LangChain vs Haystack (summary): A recent comparison notes that LangChain offers \n“the highest degree of flexibility and customization, ideal for bespoke NLP solutions,” \nwhereas Haystack provides a “modular and comprehensive framework, suitable for building"}
{"chunk_id": "d01p0010c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 10, "text": "and deploying scalable NLP systems.”. LlamaIndex excels at fast querying of large text \ndatasets through optimized indexing. In short, the choice depends on needs: LangChain for \ncustom LLM workflows; Haystack for production QA with minimal fuss; LlamaIndex for \nadvanced data indexing.\n9. Annotated Code Snippets\nBelow are illustrative code snippets (not full apps) showing core ideas in LangChain and \nGraphRAG.\n# Example: A simple LangChain LLM chain with a prompt template\nfrom langchain import LLMChain, PromptTemplate\nfrom langchain.llms import OpenAI\n# Define a prompt template\ntemplate = \"Translate to French:\\n\\n{text}\"\nprompt = PromptTemplate(input_variables=[\"text\"], template=template)\n# Create an LLM (with temperature=0 for consistency)\nllm = OpenAI(temperature=0)"}
{"chunk_id": "d01p0010c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 10, "text": "from langchain.llms import OpenAI\n# Define a prompt template\ntemplate = \"Translate to French:\\n\\n{text}\"\nprompt = PromptTemplate(input_variables=[\"text\"], template=template)\n# Create an LLM (with temperature=0 for consistency)\nllm = OpenAI(temperature=0)  \n# Build a chain: LLMChain wraps the LLM with the prompt template\nchain = LLMChain(llm=llm, prompt=prompt)\n# Run the chain on input text\nresult = chain.run(text=\"Hello, world!\")\nprint(result)  # e.g. \"Bonjour tout le monde!\"\n# Example: LangChain RetrievalQA with a Chroma vector store\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQA\n# Some example documents\ndocs = ["}
{"chunk_id": "d01p0010c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 10, "text": "# Example: LangChain RetrievalQA with a Chroma vector store\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQA\n# Some example documents\ndocs = [\n    \"PyTorch is a deep learning library based on Torch.\",\n    \"NumPy provides support for large, multi-dimensional arrays and matrices.\"\n]\n# Create a Chroma vector store with OpenAI embeddings\nvectorstore = Chroma.from_texts(docs, OpenAIEmbeddings())\n# Build a retriever interface\nretriever = vectorstore.as_retriever()\n# Build a RetrievalQA chain using a chat LLM\nqa_chain = RetrievalQA.from_chain_type(\n    llm=ChatOpenAI(temperature=0),\n    retriever=retriever,"}
{"chunk_id": "d01p0010c04", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 10, "text": "vectorstore = Chroma.from_texts(docs, OpenAIEmbeddings())\n# Build a retriever interface\nretriever = vectorstore.as_retriever()\n# Build a RetrievalQA chain using a chat LLM\nqa_chain = RetrievalQA.from_chain_type(\n    llm=ChatOpenAI(temperature=0),\n    retriever=retriever,\n    chain_type=\"stuff\"  # simple chain that stuffs docs into prompt\n)\nprint(qa_chain.run(\"What is NumPy used for?\"))\n# Expected: \"NumPy is used for working with large, multi-dimensional arrays and \nmatrices.\"\n# Example: LangChain Graph Cypher QA chain with Neo4j\nfrom langchain.chains.graph_qa.cypher import GraphCypherQAChain\nfrom langchain.graphs.neo4j_graph import Neo4jGraph"}
{"chunk_id": "d01p0011c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 11, "text": "from langchain.llms import OpenAI\n# Assume a Neo4j instance is running with a 'Person' and 'works_for' data\ngraph = Neo4jGraph(\n    url=\"bolt://localhost:7687\", \n    username=\"neo4j\", password=\"secret\"\n)\n# Build a GraphCypherQAChain using an LLM\ncypher_chain = GraphCypherQAChain.from_llm(OpenAI(temperature=0), graph)\n# Ask a natural language question; the chain will generate a Cypher query under \nthe hood\nquestion = \"Who works for Prismatic AI?\"\nanswer = cypher_chain.run(question)\nprint(answer)\n# The chain generated a Cypher query, executed it, and printed the result.\nEach snippet highlights a core concept: the first shows chaining an LLM with prompts, the second \nshows retrieval-augmented QA, and the third shows a graph-based QA using Cypher. In a real"}
{"chunk_id": "d01p0011c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 11, "text": "print(answer)\n# The chain generated a Cypher query, executed it, and printed the result.\nEach snippet highlights a core concept: the first shows chaining an LLM with prompts, the second \nshows retrieval-augmented QA, and the third shows a graph-based QA using Cypher. In a real \nsystem, these pieces would be integrated (and have error handling, etc.), but this illustrates the \nbuilding blocks.\n10. Dependencies, Limitations, and Deployment \nConsiderations\n• Dependencies: LangChain relies on Python packages for models and stores. Key ones \ninclude langchain, openai (if using OpenAI), torch or tensorflow (if using local \nmodels), and connectors like neo4j. For vector stores, dependencies might include \nchromadb or faiss-cpu. Knowledge graph RAG requires a graph DB driver (e.g. the"}
{"chunk_id": "d01p0011c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 11, "text": "include langchain, openai (if using OpenAI), torch or tensorflow (if using local \nmodels), and connectors like neo4j. For vector stores, dependencies might include \nchromadb or faiss-cpu. Knowledge graph RAG requires a graph DB driver (e.g. the \nneo4j Python driver) and any NLP libraries for entity extraction (SpaCy, HuggingFace, or \njust OpenAI).\n• Computational Considerations: LLM calls can be expensive or slow, so batching prompts \nand caching results helps. Vector search is typically fast, but indexing large corpora may \ntake time. Graph operations (like multi-hop traversal) can be compute-heavy for large \ngraphs. One must monitor memory and CPU/GPU usage. If on the cloud, managed GPUs or \ninference endpoints (e.g. Mistral 7B on Hugging Face) can be used. Locally, one might run"}
{"chunk_id": "d01p0011c04", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 11, "text": "take time. Graph operations (like multi-hop traversal) can be compute-heavy for large \ngraphs. One must monitor memory and CPU/GPU usage. If on the cloud, managed GPUs or \ninference endpoints (e.g. Mistral 7B on Hugging Face) can be used. Locally, one might run \n7B-13B models on a GPU.\n• Limitations: Both approaches have trade-offs. LangChain’s RAG is limited by the retrieval \nrecall of the vector store – if the correct passage isn’t retrieved, the answer may miss facts. It \nalso inherits LLM hallucinations if the model guesses outside its context. Knowledge Graph \nRAG requires a well-constructed KG; errors in entity extraction or outdated data lead to \nwrong answers. Graph databases may struggle with real-time updates or extremely large"}
{"chunk_id": "d01p0011c05", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 11, "text": "also inherits LLM hallucinations if the model guesses outside its context. Knowledge Graph \nRAG requires a well-constructed KG; errors in entity extraction or outdated data lead to \nwrong answers. Graph databases may struggle with real-time updates or extremely large \ngraphs. There is also a limit to reasoning complexity: while multi-hop is possible, very long \nreasoning chains may still be challenging for LLMs.\n• Cloud vs Local: A cloud deployment (e.g. using OpenAI’s GPT-4o and Pinecone/AWS \nNeptune) offers scalability and ease-of-use, but at cost and with potential data privacy \nconcerns. A local deployment (local embeddings, local LLM, on-prem Neo4j) maximizes \ncontrol and data privacy but requires managing infrastructure. For academic projects or"}
{"chunk_id": "d01p0012c01", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 12, "text": "small-scale services, a local setup with open models (Llama3, Mistral) and lightweight \nstores (Chroma) is common. Enterprises often use cloud-managed databases (e.g. Neo4j \nAura, Amazon Neptune) for reliability.\n• Versioning and Governance: When deploying in production, one must track the versions of \nmodels and data. LangChain’s openness means different teams might use different LLMs, so \nconsistency and auditing (via tools like LangSmith) become important. Graph data often \nrequires schema governance, especially if multiple sources feed into the KG.\nIn conclusion, LangChain and Knowledge Graph RAG offer powerful architectures for document \nQA. LangChain excels at orchestrating LLM interactions (prompts, retrieval, tools), while KG-"}
{"chunk_id": "d01p0012c02", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 12, "text": "requires schema governance, especially if multiple sources feed into the KG.\nIn conclusion, LangChain and Knowledge Graph RAG offer powerful architectures for document \nQA. LangChain excels at orchestrating LLM interactions (prompts, retrieval, tools), while KG-\nRAG grounds answers in explicit facts and relations. Both can be integrated: for example, \nLangChain’s support for Neo4j Vector Index and GraphCypherQAChain enables hybrid systems \nthat leverage the best of retrieval and graph reasoning. By carefully designing the pipeline, selecting \nappropriate models/stores, and incorporating memory and tools, developers can build sophisticated \nQA systems that answer user queries accurately and transparently."}
{"chunk_id": "d01p0012c03", "doc_id": 2, "doc": "LangChain_KGRAG.pdf", "page": 12, "text": "that leverage the best of retrieval and graph reasoning. By carefully designing the pipeline, selecting \nappropriate models/stores, and incorporating memory and tools, developers can build sophisticated \nQA systems that answer user queries accurately and transparently.\nSources: The above draws on LangChain official documentation and community blogs, as well as \nrecent discussions of Knowledge-Graph RAG and comparative analyses. Each component and \nclaim is supported by citations to ensure accuracy and currency."}
{"chunk_id": "d02p0001c01", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 1, "text": "Candidate Name\nRoll No Seating Number\nRegistered E-Mail ID\nDate of Birth\nPwD Status Compensatory \nTime Required Scribe Required\nExam Date\nReporting Time Gate Closure\nExam Timing Shift\nTest Centre Name\nTest Centre Address\nCandidate Name\nDate of Birth\nExam Date\nReporting Time\nRoll No\nTest Center Name\n(Signature of \nCandidate)\nExam Timing\nGate Closure\nShift\nNPTEL\nHall Ticket For\nNational Programme on Technology \nEnhanced Learning\nTest Center Address\nNPTEL Coordinator \n01:00 P.M. 02:30 P.M.\n02:00 P.M. to 05:00 P.M. Afternoon\n1. The Admit Card must be presented for veriﬁcation along with one original photo identiﬁcation (not photocopy or scanned copy). Example of acceptable"}
{"chunk_id": "d02p0001c02", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 1, "text": "National Programme on Technology \nEnhanced Learning\nTest Center Address\nNPTEL Coordinator \n01:00 P.M. 02:30 P.M.\n02:00 P.M. to 05:00 P.M. Afternoon\n1. The Admit Card must be presented for veriﬁcation along with one original photo identiﬁcation (not photocopy or scanned copy). Example of acceptable\nphoto identiﬁcation documents are School ID, College ID, Employee ID, Driving License, Passport, PAN card, Voter ID, Aadhaar-ID. Printed copy of hall\nticket and original photo ID card should be brought to the exam centre. Hall ticket and ID card copies on the phone will not be permitted.\n2. This Admit Card is valid only if the candidate's photograph and signature images are legible. To ensure this, print the admit card on A4 sized paper using\na laser printer, preferably a colour photo printer."}
{"chunk_id": "d02p0001c03", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 1, "text": "2. This Admit Card is valid only if the candidate's photograph and signature images are legible. To ensure this, print the admit card on A4 sized paper using\na laser printer, preferably a colour photo printer.\n3. Please report to the examination venue by 01:00 pm. CANDIDATES WILL NOT BE ALLOWED TO ENTER THE EXAM PREMISES AFTER 02.30pm.\n4. Candidates will be permitted to appear for the examination ONLY after their credentials are veriﬁed by center officials.\n5.  At 01:40 pm – Candidates will be permitted to occupy their allotted seats. At 01:50 pm – Candidates can login and start reading instructions prior to the\nexamination.\n6. Questions will be provided on the computer. Answer sheets will be given to each candidate. You have to mark/write on these answer sheets, tie the"}
{"chunk_id": "d02p0001c04", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 1, "text": "examination.\n6. Questions will be provided on the computer. Answer sheets will be given to each candidate. You have to mark/write on these answer sheets, tie the\nsheets together and return the same to the invigilator. Answers written on any other paper will not be evaluated.\n7. Candidates are advised to locate the examination center at least a day prior to the examination, so that they can reach the center on time for the\nexamination.\n8. The total duration of the examination is 180 minutes. Candidates will be permitted to leave the examination hall only after 3.40 pm, on a need basis.\n9. NPTEL & exam authorities are not responsible for the safe keeping of your personal belongings."}
{"chunk_id": "d02p0001c05", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 1, "text": "examination.\n8. The total duration of the examination is 180 minutes. Candidates will be permitted to leave the examination hall only after 3.40 pm, on a need basis.\n9. NPTEL & exam authorities are not responsible for the safe keeping of your personal belongings.\n10. Any kinds of watches, mobile phones or any other electronic/communication devices are STRICTLY PROHIBITED inside the examination hall. There\nmay not be any facility for safekeeping of these devices outside the examination hall; it will be prudent to not bring valuables to the examination center.\n11. Eatables and drinks (including water bottles) are not allowed inside the exam hall.\n12. Please DO NOT bring any type of Calculator for use in the exam. On-screen calculator will be available during the exam."}
{"chunk_id": "d02p0001c06", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 1, "text": "11. Eatables and drinks (including water bottles) are not allowed inside the exam hall.\n12. Please DO NOT bring any type of Calculator for use in the exam. On-screen calculator will be available during the exam.\n13. Candidates are advised to familiarize themselves with this virtual Scientiﬁc calculator well ahead of the exam.\nLink: https://www.tcsion.com/OnlineAssessment/ScientiﬁcCalculator/Calculator.html\n14. Scribble pads will be provided to candidate for rough work. Candidates have to write their name and registration number on the scribble pad before they\nstart using it. The scribble pad must be returned to the invigilator after the end of the examination.\n15. MANDATORY - ALL COPIES OF HALL TICKET PRINT-OUTS HAVE TO BE RETURNED TO THE INVIGILATOR BEFORE LEAVING THE EXAM"}
{"chunk_id": "d02p0001c07", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 1, "text": "start using it. The scribble pad must be returned to the invigilator after the end of the examination.\n15. MANDATORY - ALL COPIES OF HALL TICKET PRINT-OUTS HAVE TO BE RETURNED TO THE INVIGILATOR BEFORE LEAVING THE EXAM\nHALL. NO PAPER CAN BE TAKEN OUT OF THE EXAM HALL.\n16. IMPORTANT: Basic code of conduct during the exam should be followed, failing which, NPTEL reserves the right to take appropriate action.\n17. It is mandatory that you press the SUBMIT button on the computer after you have completed the exam.\nINSTRUCTIONS FOR THE EXAM\nPhotograph\nThursday, 17th December 2020\nP.T.O.\n1. Entry into exam hall - ID card: You will need to bring an ID card to the exam hall in order for us to verify your details shown"}
{"chunk_id": "d02p0001c08", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 1, "text": "INSTRUCTIONS FOR THE EXAM\nPhotograph\nThursday, 17th December 2020\nP.T.O.\n1. Entry into exam hall - ID card: You will need to bring an ID card to the exam hall in order for us to verify your details shown \nin your hall ticket. The ID card should be a government issued ID card that shows your name, DOB, photo and signature. \nWithout this, you will not be allowed to enter the exam hall.\n2. Hall ticket and signing the attendance sheet: Candidates have to bring one hard copy of the hall ticket to be verified at \nthe entry point, along with your ID card. Once you are seated, the invigilator will again verify your ID card and give you an \nattendance sheet to sign. Do not write anything on the hall ticket. You also need to return the hall ticket to the invigilator."}
{"chunk_id": "d02p0001c09", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 1, "text": "the entry point, along with your ID card. Once you are seated, the invigilator will again verify your ID card and give you an \nattendance sheet to sign. Do not write anything on the hall ticket. You also need to return the hall ticket to the invigilator. \n3. TIMELINE: 6:00 am - Report to the examination venue | 6:40 am – Candidates will be permitted to occupy their allotted seats| \n6:50 am – Candidates can login and start reading the instructions prior to the examination | 7:00 am - Exam starts | 7:30 am \n- Gate closes, candidates will not be allowed after this time | 8:30 am Submit button will be enabled; candidates who have \ncompleted the exam will be allowed to leave the exam hall | 10:00 am exam ends.\n06:00 am 07:30 am\n07:00 am Forenoon\n(Signature of \nCandidate)\nFORENOON \nSESSION"}
{"chunk_id": "d02p0001c10", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 1, "text": "- Gate closes, candidates will not be allowed after this time | 8:30 am Submit button will be enabled; candidates who have \ncompleted the exam will be allowed to leave the exam hall | 10:00 am exam ends.\n06:00 am 07:30 am\n07:00 am Forenoon\n(Signature of \nCandidate)\nFORENOON \nSESSION\n(7.00 AM)\nHALL TICKET AND ENTRY:\nThe total duration of the examination is 180 minutes.\nIt is mandatory for students to remain seated till 90 minutes from the start of the exam.\nStudents will not be permitted to leave the exam hall prior to this.\nNPTEL EXAM- 19 APRIL, 2025\nGeneral instructions for candidates  \n(All timings mentioned here are in IST)\nSaturday, 19 April, 2025\nConservation Economics\nDhruv Kalpesh Jadav\nNOC25BT15S1959900978 26\ndhruvkalpesh.jadav2022@vitstudent.ac.in\n17-02-2004\nPRP-706"}
{"chunk_id": "d02p0001c11", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 1, "text": "NPTEL EXAM- 19 APRIL, 2025\nGeneral instructions for candidates  \n(All timings mentioned here are in IST)\nSaturday, 19 April, 2025\nConservation Economics\nDhruv Kalpesh Jadav\nNOC25BT15S1959900978 26\ndhruvkalpesh.jadav2022@vitstudent.ac.in\n17-02-2004\nPRP-706\nVIT, Vellore Campus, Tiruvalam Road, Katpadi,\nVellore, Tamil Nadu.\nN/AN/AN/A\nNPTEL generated Hall ticket"}
{"chunk_id": "d02p0002c01", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 2, "text": "4. Candidates will be permitted to appear for the examination ONLY after their credentials are verified by the exam centre officials.\nCODE OF CONDUCT (COC):\nThese instructions and guidelines are designed to ensure the integrity and fairness of the examination process. It is imperative that \nall exam takers adhere to these guidelines to maintain the credibility of the examination.\n1. Mode of Examination: The exam is a computer based exam which you have to attend in person at the exam centre assigned \nto you.  The exam will be invigilated by your college faculty and by the NPTEL, IIT Madras team. Every click you make during \nthe exam will be recorded and stored on NPTEL’s server, which will be used for internal purposes."}
{"chunk_id": "d02p0002c02", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 2, "text": "to you.  The exam will be invigilated by your college faculty and by the NPTEL, IIT Madras team. Every click you make during \nthe exam will be recorded and stored on NPTEL’s server, which will be used for internal purposes.\n2. Confidentiality: Candidates must maintain the confidentiality of the exam. This includes refraining from sharing personal \ndetails with invigilators, discussing exam content with others, or disclosing any details of the exam to individuals outside the \nexamination environment.\n3. Assistance: Candidates are prohibited from providing or seeking assistance from other candidates during the exam. This \nincludes discussing answers with fellow candidates, passing written information via paper, email, text, or any other means. This"}
{"chunk_id": "d02p0002c03", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 2, "text": "examination environment.\n3. Assistance: Candidates are prohibited from providing or seeking assistance from other candidates during the exam. This \nincludes discussing answers with fellow candidates, passing written information via paper, email, text, or any other means. This \nwill be considered malpractice, and results will be discarded for students found indulging in this.\n4. Use of Technology:  Use only the latest version of Chrome browser to access the exam.  After logging into the exam link, exam \ntakers are permitted to use only a single active window. Attempts to close the tab, open multiple tabs, access different Chrome \nprofiles, or switch devices will result in disqualification and might potentially disable your exam link."}
{"chunk_id": "d02p0002c04", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 2, "text": "takers are permitted to use only a single active window. Attempts to close the tab, open multiple tabs, access different Chrome \nprofiles, or switch devices will result in disqualification and might potentially disable your exam link.\n5. Tab Switching: Candidates are strictly prohibited from switching tabs during the exam. Any attempt to switch tabs will be \nrecorded and may lead to score disqualification.\n6. Monitoring: Actions such as resizing the tab or switching to mobile view will be monitored during the exam and will be recorded \nat the backend and may lead to score disqualification.\n7. Seeking Clarifications: If a candidate requires clarification on any operational aspect of the exam, they may direct their query"}
{"chunk_id": "d02p0002c05", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 2, "text": "at the backend and may lead to score disqualification.\n7. Seeking Clarifications: If a candidate requires clarification on any operational aspect of the exam, they may direct their query \nto the invigilator. Invigilators will provide necessary assistance to resolve queries. Subject related queries - answer to the best \nof your knowledge and raise the query on the forum after the exam.\n8. Code of Conduct Violations:  Candidate found to have violated any aspect of the Code of Conduct of the exam will be subject \nto disciplinary procedures. Penalties may include but not be restricted to withholding of exam results and Certificate.\n9. Acknowledgement: By participating in the exam, candidates acknowledge that they have read, understood, and agree to"}
{"chunk_id": "d02p0002c06", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 2, "text": "to disciplinary procedures. Penalties may include but not be restricted to withholding of exam results and Certificate.\n9. Acknowledgement: By participating in the exam, candidates acknowledge that they have read, understood, and agree to \nabide by the Code of Conduct governing the conduct of the exams.\nExaminees are required to explicitly agree to follow the Code of Conduct before proceeding with the exam.\nLOGGING INTO THE PORTAL:\n1. Use only the Latest Chrome browser (Incognito window) to login to the exam link. DO NOT USE ANY OTHER BROWSER.\n2. Username: Your registered email ID (as given in the hall ticket).\n3. Password: “nptel” followed by your date of birth in the format “ddmmyyyy” as given in the hall ticket"}
{"chunk_id": "d02p0002c07", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 2, "text": "1. Use only the Latest Chrome browser (Incognito window) to login to the exam link. DO NOT USE ANY OTHER BROWSER.\n2. Username: Your registered email ID (as given in the hall ticket).\n3. Password: “nptel” followed by your date of birth in the format “ddmmyyyy” as given in the hall ticket    \n(e.g., if your date of birth is 01 June, 1990, the password would be “nptel01061990”).\n4. Once you login with the credentials given, you will have to enter the exam code.  We will announce this at the start of the exam \nin the respective exam halls.\n5. After this step, your system will be restricted to a single active window. \nSTATIONERY REQUIREMENTS:\n• A4 sheets will be provided to candidates for rough work. Candidates have to write their name and registration number on the"}
{"chunk_id": "d02p0002c08", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 2, "text": "in the respective exam halls.\n5. After this step, your system will be restricted to a single active window. \nSTATIONERY REQUIREMENTS:\n• A4 sheets will be provided to candidates for rough work. Candidates have to write their name and registration number on the \nA4 Sheets before they start using it. The A4 sheets must be returned to the invigilator at the end of the examination.\n• You should bring your own pen/pencil in a transparent pouch; it will NOT be given at the examination centre.\nP.T.O."}
{"chunk_id": "d02p0003c01", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 3, "text": "Signature of the Candidate\nDRESS CODE:\n• Candidates are expected to come in professional attire to write the exams.\n• Candidates wearing SHORTS will NOT be permitted inside the exam hall.\nPERMITTED:\n• You may bring non electronic vehicle keys inside the exam hall.\n• You are advised to carry your own drinking water in a transparent bottle.\n• Candidates are allowed to bring sanitizer in a small transparent bottle.\nNOT PERMITTED:\n• Watches, wallets, mobile phones, Bluetooth devices, microphones, pagers, health bands or any other electronic gadgets, any \nprinted/blank/handwritten paper, log tables, writing pads, scales, geometry/pencil-boxes, pouches, calculators, pen drives,"}
{"chunk_id": "d02p0003c02", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 3, "text": "NOT PERMITTED:\n• Watches, wallets, mobile phones, Bluetooth devices, microphones, pagers, health bands or any other electronic gadgets, any \nprinted/blank/handwritten paper, log tables, writing pads, scales, geometry/pencil-boxes, pouches, calculators, pen drives, \nelectronic pens, handbags, goggles, electronic vehicle keys or similar such items are NOT allowed inside the examination \ncentre. There may not be any facility for the safekeeping of these devices outside the examination hall; it will be prudent not \nto bring valuables to the examination center. Candidates will not be permitted to carry any food items in the exam centre. We \nsuggest that you bring a bag to keep routine belongings outside the exam hall. Neither NPTEL nor the exam provider takes"}
{"chunk_id": "d02p0003c03", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 3, "text": "to bring valuables to the examination center. Candidates will not be permitted to carry any food items in the exam centre. We \nsuggest that you bring a bag to keep routine belongings outside the exam hall. Neither NPTEL nor the exam provider takes \nresponsibility for the bag and the belongings. You may keep it outside at your own risk.\nMANDATORY:\n• Hall tickets have to be returned to the invigilator before leaving the exam hall. No paper can be taken out of the exam hall.\n• Press the SUBMIT button on the computer after you have completed the exam.\nIMPORTANT:\n• The basic code of conduct during the exam should be followed, failing which, NPTEL reserves the right to take appropriate \naction."}
{"chunk_id": "d02p0003c04", "doc_id": 3, "doc": "NOC25BT15S1959900978.pdf", "page": 3, "text": "• Press the SUBMIT button on the computer after you have completed the exam.\nIMPORTANT:\n• The basic code of conduct during the exam should be followed, failing which, NPTEL reserves the right to take appropriate \naction.\n• In case the exam is delayed due to any unforeseen circumstances, NPTEL will decide on the appropriate course of action as it \ndeems fit.\n• Scores will be discarded for the students who do not adhere to the above instructions.\nAT THE EXAM CENTRE, IF YOU ENCOUNTER ANY ISSUES WITH RESPECT TO THE COMPUTER OR EXAM OFFICIALS, \nKINDLY CONTACT THE NPTEL EXAM REPRESENTATIVE, WHO WILL BE AVAILABLE AT THE CENTRE.\nI HEREBY ACKNOWLEDGE THAT I HAVE READ, UNDERSTOOD, AND AGREE TO FOLLOW THE ABOVE-MENTIONED \nINSTRUCTIONS."}
{"chunk_id": "d04p0001c01", "doc_id": 5, "doc": "pdf_knowledge_compiler_documentation.pdf", "page": 1, "text": "PDF Knowledge Compiler - Full Code Walkthrough and Documentation\nOverview\nThis document explains the internal working of a Python module designed to extract structured data from\nacademic PDFs. It captures figures, tables, citations, and chunks paragraphs for RAG-based QA systems.\nThe source code is modular and uses libraries like PyMuPDF, pdfplumber, and regex-based parsing.\nCore Libraries Used\n- `fitz (PyMuPDF)`: Extracts images and text from PDF pages.\n- `pdfplumber`: Extracts tabular data.\n- `difflib`: Normalizes section titles.\n- `re`: Regular expressions for parsing references and citations.\n- `os`, `json`, `argparse`, `collections`, `pathlib`: Standard Python utilities.\nFunctions Explained"}
{"chunk_id": "d04p0001c02", "doc_id": 5, "doc": "pdf_knowledge_compiler_documentation.pdf", "page": 1, "text": "- `pdfplumber`: Extracts tabular data.\n- `difflib`: Normalizes section titles.\n- `re`: Regular expressions for parsing references and citations.\n- `os`, `json`, `argparse`, `collections`, `pathlib`: Standard Python utilities.\nFunctions Explained\n- `normalize_title(text)`: Cleans and matches raw section headings using approximate string matching.\n- `extract_figures(pdf_path)`: Scans all PDF pages and indexes images using unique figure IDs.\n- `extract_tables(pdf_path)`: Uses pdfplumber to extract tables and assigns table IDs per page.\n- `extract_references_from_section(sections)`: Finds the References section and parses citations like [1], [2].\n- `extract_citations(text)`: Identifies all `[n]` citation patterns in a paragraph."}
{"chunk_id": "d04p0001c03", "doc_id": 5, "doc": "pdf_knowledge_compiler_documentation.pdf", "page": 1, "text": "- `extract_tables(pdf_path)`: Uses pdfplumber to extract tables and assigns table IDs per page.\n- `extract_references_from_section(sections)`: Finds the References section and parses citations like [1], [2].\n- `extract_citations(text)`: Identifies all `[n]` citation patterns in a paragraph.\n- `split_into_chunks(paragraphs, max_words)`: Merges paragraphs into 500-word chunks for QA embedding.\n- `track_assets_per_chunk(...)`: Maps chunks to nearby figures/tables by page.\n- `parse_sections_with_assets(...)`: Parses all sections, aligns paragraph text with metadata (images, tables,\ncitations).\n- `extract_all(...)`: Main driver. Coordinates all other modules, saves JSON to cache.\nAdvanced Python Constructs Used\n- `enumerate()`: Used to index pages and images during iteration."}
{"chunk_id": "d04p0001c04", "doc_id": 5, "doc": "pdf_knowledge_compiler_documentation.pdf", "page": 1, "text": "citations).\n- `extract_all(...)`: Main driver. Coordinates all other modules, saves JSON to cache.\nAdvanced Python Constructs Used\n- `enumerate()`: Used to index pages and images during iteration.\n- `defaultdict(list)`: Avoids manual checks when aggregating per-page assets.\n- `get_close_matches()`: From `difflib`, performs fuzzy matching of headings.\n- `os.makedirs(..., exist_ok=True)`: Ensures cache directories are created safely.\n- `re.findall()`, `re.search()`, `re.match()`: Core of reference/citation parsing logic."}
{"chunk_id": "d04p0002c01", "doc_id": 5, "doc": "pdf_knowledge_compiler_documentation.pdf", "page": 2, "text": "Execution\nThis script can be run from the command line:\n    python pdf_knowledge_extractor.py sample.pdf --cache-dir ./cache\nIt generates:\n- Structured JSON with sections, images, tables\n- Maps citations, figures, and tables per paragraph chunk\n- Output written to `./cache/output.json`\nConclusion\nThis system is optimized for research pipelines needing structured PDF parsing before semantic indexing or\nLLM-driven Q&A. The code is modular, extensible, and tailored for digital PDFs like arXiv or IEEE formats."}
{"chunk_id": "d05p0001c01", "doc_id": 6, "doc": "Project details.pdf", "page": 1, "text": "Project Title\nIntelligent Local PDF Knowledge Compiler and QA System Using LangChain, LlamaIndex, \nNeo4j, and Local LLMs\nProject Overview\nThe system ingests scientific research PDFs (e.g., from arXiv), extracts structured knowledge (text \nchunks, entities, citations), builds:\n• A vector-based semantic index using LlamaIndex and Sentence-Transformers,\n• A Neo4j knowledge graph of named entities and their relationships,\n• And orchestrates question answering with LangChain agents using local LLMs (e.g. \nLLaMA 3, Mistral via Ollama).\nThis hybrid architecture supports natural language queries, graph traversal, semantic retrieval, \nand citation-grounded responses — all offline and open-source.\nSystem Architecture Summary\n1. Data Flow\nPDFs → Text Chunks → Embeddings & Entity Triples →"}
{"chunk_id": "d05p0001c02", "doc_id": 6, "doc": "Project details.pdf", "page": 1, "text": "LLaMA 3, Mistral via Ollama).\nThis hybrid architecture supports natural language queries, graph traversal, semantic retrieval, \nand citation-grounded responses — all offline and open-source.\nSystem Architecture Summary\n1. Data Flow\nPDFs → Text Chunks → Embeddings & Entity Triples → \n[Vector Index + Knowledge Graph] → QA System\n2. Core Technologies\nComponent Tech Used\nText Extraction PyMuPDF / pdfplumber\nText Chunking LangChain’s TextSplitter\nEmbeddings sentence-transformers/all-MiniLM-L6-v2\nVector Index LlamaIndex.VectorStoreIndex\nEntity Extraction spaCy NER\nKnowledge Graph Neo4j with Cypher and triples\nQA Orchestration LangChain chains + tools + memory\nLLM Backend Local LLMs via Ollama / Transformers"}
{"chunk_id": "d05p0002c01", "doc_id": 6, "doc": "Project details.pdf", "page": 2, "text": "Project Flow: End-to-End\n1. PDF Ingestion & Chunking\n• PDFs loaded via PyMuPDF\n• Pages split into ~500-character overlapping chunks\n• Chunks saved with metadata: {doc_id, page, chunk_id}\n2. Named Entity Recognition\n• Run spaCy over each chunk\n• Extract (entity, type) tuples\n• Link entities to avoid duplicates across docs\n3. Vector Indexing (LlamaIndex)\n• Embed chunks using all-MiniLM-L6-v2\n• Store as VectorStoreIndex, persisted to disk\n• Allows fast similarity search for user queries\n4. Knowledge Graph Construction (Neo4j)\n• Each Document, Paragraph, and Entity is a node\n• Relationships:\n• (Paragraph)-[:MENTIONS]->(Entity)\n• (Document)-[:HAS_CHUNK]->(Paragraph)\n• Optional: (Entity)-[:WORKS_AT]->(Entity) via relation extraction\n5. LangChain Orchestration\n• Two Tools:"}
{"chunk_id": "d05p0002c02", "doc_id": 6, "doc": "Project details.pdf", "page": 2, "text": "4. Knowledge Graph Construction (Neo4j)\n• Each Document, Paragraph, and Entity is a node\n• Relationships:\n• (Paragraph)-[:MENTIONS]->(Entity)\n• (Document)-[:HAS_CHUNK]->(Paragraph)\n• Optional: (Entity)-[:WORKS_AT]->(Entity) via relation extraction\n5. LangChain Orchestration\n• Two Tools:\n• VectorSearchTool: queries LlamaIndex\n• GraphQueryTool: queries Neo4j (via Cypher)\n• Agent: ZERO_SHOT_REACT_DESCRIPTION uses both tools\n• ConversationMemory: preserves multi-turn chat context\n6. Answer Generation\n• Retrieved content (text chunks + graph facts) passed to LLM\n• LLM generates final answer, with source citations from metadata"}
{"chunk_id": "d05p0003c01", "doc_id": 6, "doc": "Project details.pdf", "page": 3, "text": "Key Methods & Concepts Used\nPurpose Method / Concept\nEntity Linking spaCy + canonical name matching\nGraph Building Cypher queries and MERGE statements\nTriple Representation (Subject, Predicate, Object) → Neo4j edge\nVector Similarity Search Cosine distance via Sentence-Transformers\nRetrieval-Augmented Generation LangChain + LlamaIndex + Tools\nGraph Query Interpretation Cypher query templates from NL input\nConversational QA LangChain Memory (e.g. ConversationBuffer)\n Modular Code Architecture\npdf_qa_system/\n│\n├── data/\n│   ├── pdfs/              # Raw input PDFs\n│   ├── text_chunks/       # JSON chunks with metadata\n│   └── graph_db/          # Optional local graph snapshots\n│\n├── modules/\n│   ├── ingest.py          # Extracts + splits PDF text"}
{"chunk_id": "d05p0003c02", "doc_id": 6, "doc": "Project details.pdf", "page": 3, "text": "Modular Code Architecture\npdf_qa_system/\n│\n├── data/\n│   ├── pdfs/              # Raw input PDFs\n│   ├── text_chunks/       # JSON chunks with metadata\n│   └── graph_db/          # Optional local graph snapshots\n│\n├── modules/\n│   ├── ingest.py          # Extracts + splits PDF text\n│   ├── entities.py        # NER and linking logic\n│   ├── graph_builder.py   # Neo4j graph construction\n│   ├── index_builder.py   # LlamaIndex creation/persistence\n│   ├── retriever.py       # Semantic search wrapper\n│   ├── tools.py           # LangChain tools (graph, vector)\n│   └── qa_pipeline.py     # Agent setup with memory\n│\n├── config.py              # Paths, model names, Neo4j creds\n├── main.py                # CLI or API interface\n├── requirements.txt       # Dependencies"}
{"chunk_id": "d05p0003c03", "doc_id": 6, "doc": "Project details.pdf", "page": 3, "text": "│   ├── tools.py           # LangChain tools (graph, vector)\n│   └── qa_pipeline.py     # Agent setup with memory\n│\n├── config.py              # Paths, model names, Neo4j creds\n├── main.py                # CLI or API interface\n├── requirements.txt       # Dependencies\n└── README.md              # Usage instructions\nAdvantages of the Design\n• Local-first: No cloud dependencies; privacy-compliant\n• Modular: Each function (ingest, extract, index, QA) is isolated\n• Hybrid Retrieval: Combines vector search + knowledge graph\n• Citation-Aware: Every answer is traceable to PDF + chunk\n• Scalable: Easy to extend to thousands of PDFs, multiple query types"}
{"chunk_id": "d05p0004c01", "doc_id": 6, "doc": "Project details.pdf", "page": 4, "text": "Example Use Cases\nQuery Type Backend\n“What is BERT and who created it?” Vector + Graph\n“Where did Einstein work in 1921?” Graph (via Cypher)\n“Summarize section 3.1 of doc1” Vector\n“What’s the relationship between X and Y?” Graph traversal\nFinal Summary\nYou are building a cost-efficient, offline, LLM-augmented QA system capable of:\n• Understanding academic PDFs,\n• Structuring knowledge into text + triples,\n• Enabling fast and explainable QA with semantic and symbolic reasoning.\nThis hybrid approach mirrors modern industrial RAG pipelines, scaled down to run entirely on a \nlocal machine.\nLet me know if you want a detailed README template, dev workflow, or test plan to finalize your \nrepository.\nx-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}
{"chunk_id": "d06p0001c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 1, "text": "Training Tips for the Transformer Model\nMartin Popel, Ondřej Bojar\nCharles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics,\nPrague, Czechia\nAbstract\nThis article describes our experiments in neural machine translation using the recent Ten-\nsor2TensorframeworkandtheTransformersequence-to-sequencemodel(Vaswanietal.,2017).\nWe examine some of the critical parameters that aﬀect the ﬁnal translation quality, memory\nusage, training stability and training time, concluding each experiment with a set of recom-\nmendations for fellow researchers. In addition to conﬁrming the general mantra “more data\nand larger models”, we address scaling to multiple GPUs and provide practical tips for im-"}
{"chunk_id": "d06p0001c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 1, "text": "usage, training stability and training time, concluding each experiment with a set of recom-\nmendations for fellow researchers. In addition to conﬁrming the general mantra “more data\nand larger models”, we address scaling to multiple GPUs and provide practical tips for im-\nproved training regarding batch size, learning rate, warmup steps, maximum sentence length\nandcheckpointaveraging. Wehopethatourobservationswillallowotherstogetbetterresults\ngiven their particular hardware and data constraints.\n1. Introduction\nIt has been already clearly established that neural machine translation (NMT) is\nthe new state of the art in machine translation, see e.g. the most recent evaluation\ncampaigns(Bojaretal.,2017a;Cettoloetal.,2017). Manyfundamentalchangesofthe"}
{"chunk_id": "d06p0001c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 1, "text": "1. Introduction\nIt has been already clearly established that neural machine translation (NMT) is\nthe new state of the art in machine translation, see e.g. the most recent evaluation\ncampaigns(Bojaretal.,2017a;Cettoloetal.,2017). Manyfundamentalchangesofthe\nunderlying neural network architecture are nevertheless still frequent and it is very\ndiﬃcult to predict which of the architectures has the best combination of properties\ntowininthelongterm,consideringallrelevantcriterialiketranslationquality,model\nsize, stability and speed of training, interpretability but also practical availability of\ngoodimplementations. Aconsiderablepartofamodel’ssuccessintranslationquality\nconsists in the training data, the model’s sensitivity to noise in the data but also on a"}
{"chunk_id": "d06p0001c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 1, "text": "size, stability and speed of training, interpretability but also practical availability of\ngoodimplementations. Aconsiderablepartofamodel’ssuccessintranslationquality\nconsists in the training data, the model’s sensitivity to noise in the data but also on a\nwide range of hyper-parameters that aﬀect the training. Having the right setting of\nthem turns out to be often a critical component for the success.\n1\narXiv:1804.00247v2  [cs.CL]  2 May 2018"}
{"chunk_id": "d06p0002c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "Inthisarticle,weexperimentwitharelativelynewNMTmodel,calledTransformer\n(Vaswanietal.,2017)asimplementedintheTensor2Tensor /one.superior(abbreviatedT2T)toolkit,\nversion1.2.9. Themodelandthetoolkithavebeenreleasedshortlyaftertheevaluation\ncampaignatWMT2017 /two.superioranditsbehavioronlarge-datanewstranslationisnotyetfully\nexplored. We want to empirically explore some of the important hyper-parameters.\nHopefully, our observations will be useful also for other researchers considering this\nmodel and framework.\nWhileinvestigationsintotheeﬀectofhyper-parameterslikelearningrateandbatch\nsize are available in the deep-learning community (e.g. Bottou et al., 2016; Smith and\nLe, 2017; Jastrzebski et al., 2017), these are either mostly theoretic or experimentally"}
{"chunk_id": "d06p0002c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "model and framework.\nWhileinvestigationsintotheeﬀectofhyper-parameterslikelearningrateandbatch\nsize are available in the deep-learning community (e.g. Bottou et al., 2016; Smith and\nLe, 2017; Jastrzebski et al., 2017), these are either mostly theoretic or experimentally\nsupported from domains like image recognition rather than machine translation. In\nthis article, we ﬁll the gap by focusing exclusively on MT and on the Transformer\nmodel only, providing hopefully the best practices for this particular setting.\nSome of our observations conﬁrm the general wisdom (e.g. larger training data\naregenerallybetter)andquantifythebehavioronEnglish-to-Czechtranslationexper-\niments. Some of our observations are somewhat surprising, e.g. that two GPUs are"}
{"chunk_id": "d06p0002c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "Some of our observations conﬁrm the general wisdom (e.g. larger training data\naregenerallybetter)andquantifythebehavioronEnglish-to-Czechtranslationexper-\niments. Some of our observations are somewhat surprising, e.g. that two GPUs are\nmore than three times faster than a single GPU, or our ﬁndings about the interaction\nbetween maximum sentence length, learning rate and batch size.\nThearticleisstructuredasfollows. InSection2,wediscussourevaluationmethod-\nologyandmaincriteria: translationqualityandspeedoftraining. Section3describes\nour dataset and its preparations. Section 4 is the main contribution of the article: a\nset of commented experiments, each with a set of recommendations. Finally, Sec-\ntion 5 compares our best Transformer run with systems participating in WMT17. We"}
{"chunk_id": "d06p0002c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "our dataset and its preparations. Section 4 is the main contribution of the article: a\nset of commented experiments, each with a set of recommendations. Finally, Sec-\ntion 5 compares our best Transformer run with systems participating in WMT17. We\nconclude in Section 6.\n2. Evaluation Methodology\nMachine translation can be evaluated in many ways and some forms of human\njudgment should be always used for the ultimate resolution in any ﬁnal application.\nThecommonpracticeinMTresearchistoevaluatethemodelperformanceonatestset\nagainst one or more human reference translations. The most widespread automatic\nmetricisundoubtedlytheBLEUscore(Papinenietal.,2002),despiteitsacknowledged\nproblems and better-performing alternatives (Bojar et al., 2017b). For simplicity, we"}
{"chunk_id": "d06p0002c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "against one or more human reference translations. The most widespread automatic\nmetricisundoubtedlytheBLEUscore(Papinenietal.,2002),despiteitsacknowledged\nproblems and better-performing alternatives (Bojar et al., 2017b). For simplicity, we\nstick to BLEU, too (we evaluated all our results also with/c.sc/h.sc/r.scF(Popović, 2015), but\nfoundnosubstantialdiﬀerencesfromBLEU).Inparticular,weusethecase-insensitive\nsacréBLEU/three.superiorwhich uses a ﬁxed tokenization (identical tomteval-v14.pl --interna-\n/one.superiorhttps://github.com/tensorflow/tensor2tensor\n/two.superiorhttp://www.statmt.org/wmt17\n/three.superiorhttps://github.com/awslabs/sockeye/tree/master/contrib/sacrebleu\nThe signature of the BLEU scores reported in this paper isBLEU+case.lc+lang.en-cs+numrefs.1+smooth."}
{"chunk_id": "d06p0002c06", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "/one.superiorhttps://github.com/tensorflow/tensor2tensor\n/two.superiorhttp://www.statmt.org/wmt17\n/three.superiorhttps://github.com/awslabs/sockeye/tree/master/contrib/sacrebleu\nThe signature of the BLEU scores reported in this paper isBLEU+case.lc+lang.en-cs+numrefs.1+smooth.\nexp+test.wmt13+tok.intl+version.1.2.3.\n2"}
{"chunk_id": "d06p0003c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 3, "text": "tional-tokenization) and automatically downloads the reference translation for a\ngiven WMT testset.\n2.1. Considerations on Stopping Criterion\nThe situation in NMT is further complicated by the fact that the training of NMT\nsystemsisusuallynon-deterministic, ⁴ and(esp. withthemostrecentmodels)hardly\neverconvergesorstartsoverﬁtting ⁵ onreasonablybigdatasets. Thisleadstolearning\ncurvesthatneverfullyﬂattenletalonestartdecreasing(seeSection4.2). Thecommon\npractice of machine learning to evaluate the model on a ﬁnal test set when it started\noverﬁtting (or a bit sooner) is thus not applicable in practice.\nMany papers in neural machine translation do not specify any stopping criteria\nwhatsoever. Sometimes,theymentiononlyanapproximatenumberofdaysthemodel"}
{"chunk_id": "d06p0003c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 3, "text": "overﬁtting (or a bit sooner) is thus not applicable in practice.\nMany papers in neural machine translation do not specify any stopping criteria\nwhatsoever. Sometimes,theymentiononlyanapproximatenumberofdaysthemodel\nwas trained for, e.g. Bahdanau et al. (2015), sometimes the exact number of training\nsteps is given but no indication on “how much converged” the model was at that\npoint, e.g. Vaswani et al. (2017). Most probably, the training was run until no further\nimprovements were clearly apparent on the development test set, and the model was\nevaluated at that point. Such an approximate stopping criterion is rather risky: it is\nconceivablethatdiﬀerentsetupswerestoppedatdiﬀerentstagesoftrainingandtheir\ncomparison is not fair."}
{"chunk_id": "d06p0003c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 3, "text": "improvements were clearly apparent on the development test set, and the model was\nevaluated at that point. Such an approximate stopping criterion is rather risky: it is\nconceivablethatdiﬀerentsetupswerestoppedatdiﬀerentstagesoftrainingandtheir\ncomparison is not fair.\nA somewhat more reliable method is to keep training for a speciﬁed number of\niterations or a certain number of epochs. This is however not a perfect solution\neither, if the models are not quite converged at that time and the diﬀerence in their\nperformance is not suﬃciently large. It is quite possible that e.g. a more complex\nmodel would need a few more epochs and eventually arrived at a higher score than\nits competitor. Also, the duration of one training step (or one epoch) diﬀers between"}
{"chunk_id": "d06p0003c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 3, "text": "performance is not suﬃciently large. It is quite possible that e.g. a more complex\nmodel would need a few more epochs and eventually arrived at a higher score than\nits competitor. Also, the duration of one training step (or one epoch) diﬀers between\nmodels(seeSection4.1)andfromthepracticalpointofview,wearemostlyinterested\nin the wall-clock time.\nWhen we tried the standard technique of early stopping, whenN subsequent\nevaluationsonthedevelopmenttestsetdonotgiveimprovementslargerthanagiven\ndelta,wesawabigvarianceinthetrainingtimeandﬁnalBLEU,evenforexperiments\nwith the same hyper-parameters and just a diﬀerent random seed. Moreover to get\nthe best results, we would have had to use a very largeN and a very small delta."}
{"chunk_id": "d06p0003c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 3, "text": "delta,wesawabigvarianceinthetrainingtimeandﬁnalBLEU,evenforexperiments\nwith the same hyper-parameters and just a diﬀerent random seed. Moreover to get\nthe best results, we would have had to use a very largeN and a very small delta.\n⁴ Even if we ﬁx the random seed (which was not done properly in T2T v1.2.9), a change of some hyper-\nparameters may aﬀect the results not because of the change itself, but because it inﬂuenced the random\ninitialization.\n⁵ By overﬁtting we mean here that the translation quality (test-set BLEU) begins to worsen, while the\ntraining loss keeps improving.\n3"}
{"chunk_id": "d06p0004c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 4, "text": "2.2. Our Final Choice: Full Learning Curves\nBasedonthediscussionabove,wedecidedtoreportalwaysthefulllearningcurves\nand not just single scores. This solution does not fully prevent the risk of premature\njudgments, butthereaderscanatleastjudgeforthemselvesiftheywouldexpectany\nsudden twist in the results or not.\nIn all cases, we plot the case-insensitive BLEU score against the wall-clock time\nin hours. This solution obviously depends on the hardware chosen, so we always\nused the same equipment: one up to eight GeForce GTX 1080 Ti GPUs with NVIDIA\ndriver 375.66. Some variation in the measurements is unfortunately unavoidable\nbecause we could not fully isolate the computation from diﬀerent processes on the\nsame machine and from general network traﬃc, but based on our experiments with"}
{"chunk_id": "d06p0004c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 4, "text": "driver 375.66. Some variation in the measurements is unfortunately unavoidable\nbecause we could not fully isolate the computation from diﬀerent processes on the\nsame machine and from general network traﬃc, but based on our experiments with\nreplicated experiments such variation is negligible.\n2.3. Terminology\nFor clarity, we deﬁne the following terms and adhere to them for the rest of the\npaper:\nTranslation qualityis an automatic estimate of how well the translation carried out\nby a particular ﬁxed model expresses the meaning of the source. We estimate\ntranslation quality solely by BLEU score against one reference translation.\nTraining Stepsdenote the number of iterations, i.e. the number of times the opti-\nmizer update was run. This number also equals the number of (mini)batches"}
{"chunk_id": "d06p0004c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 4, "text": "translation quality solely by BLEU score against one reference translation.\nTraining Stepsdenote the number of iterations, i.e. the number of times the opti-\nmizer update was run. This number also equals the number of (mini)batches\nthat were processed.\nBatch Size isthenumberoftrainingexamplesusedbyoneGPUinonetrainingstep.\nIn sequence-to-sequence models, batch size is usually speciﬁed as the number\nofsentencepairs. However,theparameter batch_size inT2Ttranslationspeciﬁes\nthe approximate number oftokens(subwords) in one batch.⁶ This allows to use\na higher number of short sentences in one batch or a smaller number of long\nsentences.\nEﬀective Batch Sizeis the number of training examples consumed in one training\nstep. WhentrainingonmultipleGPUs,theparameter batch_size isinterpreted"}
{"chunk_id": "d06p0004c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 4, "text": "a higher number of short sentences in one batch or a smaller number of long\nsentences.\nEﬀective Batch Sizeis the number of training examples consumed in one training\nstep. WhentrainingonmultipleGPUs,theparameter batch_size isinterpreted\nperGPU.Thatis,with batch_size=1500 and8GPUs,thesystemactuallydigests\n12k subwords of each language in one step.\nTraining Epochcorresponds to one complete pass over the training data. Unfortu-\nnately, it is not easy to measure the number of training epochs in T2T.⁷ T2T\n⁶ For this purpose, the number of tokens in a sentence is deﬁned as the maximum of source and target\nsubwords. T2T also does reordering and bucketing of the sentences by their length to minimize the use of"}
{"chunk_id": "d06p0004c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 4, "text": "nately, it is not easy to measure the number of training epochs in T2T.⁷ T2T\n⁶ For this purpose, the number of tokens in a sentence is deﬁned as the maximum of source and target\nsubwords. T2T also does reordering and bucketing of the sentences by their length to minimize the use of\npadding symbols. However, some padding is still needed, thusbatch_size only approximates the actual\nnumber of (non-padding) subwords in a batch.\n⁷https://github.com/tensorflow/tensor2tensor/issues/415\n4"}
{"chunk_id": "d06p0005c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 5, "text": "reports only the number of training steps. In order to convert training steps to\nepochs, we need to multiply the steps by the eﬀective batch size and divide by\nthenumberofsubwordsinthetrainingdata(seeSection3.1). Thesegmentation\nofthetrainingdataintosubwordsisusuallyhiddentotheuserandthenumber\nof subwords must be thus computed by a special script.\nComputation Speedissimplytheobservednumberoftrainingstepsperhour. Com-\nputation speed obviously depends on the hardware (GPU speed, GPU-CPU\ncommunication) and software (driver version, CUDA library version, imple-\nmentation). The main parameters aﬀecting computation speed are the model\nsize,optimizerandothersettingsthatdirectlymodifytheformulaoftheneural\nnetwork.\nTraining Throughputis the amount of training data digested by the training. We"}
{"chunk_id": "d06p0005c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 5, "text": "mentation). The main parameters aﬀecting computation speed are the model\nsize,optimizerandothersettingsthatdirectlymodifytheformulaoftheneural\nnetwork.\nTraining Throughputis the amount of training data digested by the training. We\nreport training throughput in subwords per hour. Training Throughput equals\nto the Computation Speed multiplied by the eﬀective batch size.\nConvergence SpeedorBLEUConvergence istheincreaseinBLEUdividedbytime.\nConvergence speed changes heavily during training, starting very high and\ndecreasing as the training progresses. A converged model should have conver-\ngence speed of zero.\nTime Till Scoreis the training time needed to achieve a certain level of translation\nquality, in our case BLEU. We use this as an informal measure because it is not"}
{"chunk_id": "d06p0005c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 5, "text": "decreasing as the training progresses. A converged model should have conver-\ngence speed of zero.\nTime Till Scoreis the training time needed to achieve a certain level of translation\nquality, in our case BLEU. We use this as an informal measure because it is not\nclear how to deﬁne the moment of “achieving” a given BLEU score. We deﬁne\nit as time after which the BLEU never falls below the given level.⁸\nExamples Till Scoreis the number of training examples (in subwords) needed to\nachieve a certain level of BLEU. It equals to the Time Till Score multiplied by\nTraining Throughput.\n2.4. Tools for Evaluation within Tensor2Tensor\nT2T, being implemented in TensorFlow, provides nice TensorBoard visualizations\nof the training progress. The original implementation was optimized towards speed"}
{"chunk_id": "d06p0005c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 5, "text": "Training Throughput.\n2.4. Tools for Evaluation within Tensor2Tensor\nT2T, being implemented in TensorFlow, provides nice TensorBoard visualizations\nof the training progress. The original implementation was optimized towards speed\nof evaluation rather than towards following the standards of the ﬁeld. T2T thus\nreports“approx-bleu”bydefault,whichiscomputedontheinternalsubwords(never\nexposed to the user, actually) instead of words (according to BLEU tokenization).\nAs a result, “approx-bleu” is usually about 1.2–1.8 times higher than the real BLEU.\nDue to its dependence on the training data (for the subword vocabulary), it is not\neasily reproducible in varying experiments and thus not suitable for reporting in\npublications."}
{"chunk_id": "d06p0005c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 5, "text": "As a result, “approx-bleu” is usually about 1.2–1.8 times higher than the real BLEU.\nDue to its dependence on the training data (for the subword vocabulary), it is not\neasily reproducible in varying experiments and thus not suitable for reporting in\npublications.\n⁸ Such deﬁnition of Time Till Score leads to a high variance of its values because of the relatively high\nBLEU variance between subsequent checkpoints (visible as a “ﬂickering” of the learning curves in the\nﬁgures). To decrease the variation one can use a bigger development test set.\n5"}
{"chunk_id": "d06p0006c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 6, "text": "sentences EN words CS words\nCzEng 1.7 57 M 618 M 543 M\neuroparl-v7 647 k 15 M 13 M\nnews-commentary-v11 190 k 4.1 M 3.7 M\ncommoncrawl 161 k 3.3 M 2.9 M\nTotal 58 M 640 M 563 M\nTable 1: Training data resources\nWeimplementedahelperscript t2t-bleu whichcomputesthe“real”BLEU(giving\nthe same result as sacréBLEU with--tokenization intl). Our script can be used in\ntwo ways:\n• To evaluate one translated ﬁle:\nt2t-bleu --translation=my-wmt13.de --reference=wmt13_deen.de\n• To evaluate all translations in a given directory (created e.g. byt2t-translate-\nall) and store the results in a TensorBoard events ﬁle. All the ﬁgures in this\narticle were created this way.\nWe also implementedt2t-translate-all andt2t-avg-all scripts, which translate"}
{"chunk_id": "d06p0006c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 6, "text": "• To evaluate all translations in a given directory (created e.g. byt2t-translate-\nall) and store the results in a TensorBoard events ﬁle. All the ﬁgures in this\narticle were created this way.\nWe also implementedt2t-translate-all andt2t-avg-all scripts, which translate\nall checkpoints in a given directory and average a window of N subsequent check-\npoints, respectively.⁹ For details on averaging see Section 4.10.\n3. Data Selection and Preprocessing\nWe focused on the English-to-Czech translation direction. Most of our training\ndata comes from the CzEng parallel treebank, version 1.7 (57M sentence pairs),/one.superior⁰\nand the rest (1M sentence pairs) comes from three smaller sources (Europarl, News\nCommentary, Common Crawl) as detailed in Table 1."}
{"chunk_id": "d06p0006c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 6, "text": "data comes from the CzEng parallel treebank, version 1.7 (57M sentence pairs),/one.superior⁰\nand the rest (1M sentence pairs) comes from three smaller sources (Europarl, News\nCommentary, Common Crawl) as detailed in Table 1.\nWe use this dataset of 58M sentence pairs for most our experiments. In some\nexperiments (in Sections 4.2 and 4.6), we substitute CzEng 1.7 with an older and\nconsiderably smaller CzEng 1.0 (Bojar et al., 2012) containing 15M sentence pairs\n(233M/206M of en/cs words).\nTo plot the performance throughout the training, we use WMT newstest2013 as\na development set (not overlapping with the training data). In Section 5, we apply\nour best model (judged from the performance on the development set) to the WMT\nnewstest2017, for comparison with the state-of-the-art systems."}
{"chunk_id": "d06p0006c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 6, "text": "a development set (not overlapping with the training data). In Section 5, we apply\nour best model (judged from the performance on the development set) to the WMT\nnewstest2017, for comparison with the state-of-the-art systems.\n⁹ All three scripts are now merged in the T2T master. All three scripts can be used while the training is\nstill in progress, i.e. they wait a given number of minutes for new checkpoints to appear.\n/one.superior⁰http://ufal.mff.cuni.cz/czeng/czeng17, which is a subset of CzEng 1.6 (Bojar et al., 2016).\n6"}
{"chunk_id": "d06p0007c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "3.1. Training Data Preprocessing\nData preprocessing such as tokenization and truecasing has always been a very\nimportant part of the setup of statistical machine translation systems. A huge leap in\nscaling NMT to realistic data size has been achieved by the introduction of subword\nunits(Sennrichetal.,2016),butthelong-termvisionofthedeep-learningcommunity\nis to leave all these “technicalities” up to the trained neural network and feed it with\nas original input as possible (see e.g. Lee et al., 2016).\nT2T adopts this vision and while it supports the use of external subword units, it\ncomes with its own built-in method similar to the word-piece algorithm by Wu et al.\n(2016)anddoesnotexpecttheinputtobeeventokenized. Basedonasmallsampleof"}
{"chunk_id": "d06p0007c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "as original input as possible (see e.g. Lee et al., 2016).\nT2T adopts this vision and while it supports the use of external subword units, it\ncomes with its own built-in method similar to the word-piece algorithm by Wu et al.\n(2016)anddoesnotexpecttheinputtobeeventokenized. Basedonasmallsampleof\nthe training data, T2T will train a subword vocabulary and apply it to all the training\nand later evaluation data.\nWe follow the T2T default and provide raw plain text training sentences. We\nuse the default parameters: shared source and target (English and Czech) subword\nvocabulary of size 32k./one.superior/one.superiorAfter this preprocessing, the total number of subwords in\nour main training data is 992 millions (taking the maximum of English and Czech"}
{"chunk_id": "d06p0007c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "use the default parameters: shared source and target (English and Czech) subword\nvocabulary of size 32k./one.superior/one.superiorAfter this preprocessing, the total number of subwords in\nour main training data is 992 millions (taking the maximum of English and Czech\nlengths for each sentence pair, as needed for computing the number of epochs, see\nSection 2.3). The smaller dataset CzEng 1.0 has 327 million subwords. In both cases\nthe average number of subwords per (space-delimited) word is about 1.5.\nEvenwhenfollowingthedefaults,therearesomeimportantdetailsthatshouldbe\nconsidered. We thus provide our ﬁrst set of technical tips here:\nTips on Training Data Preprocessing\n• Makesurethatthesubwordvocabularyistrainedonasuﬃcientlylargesample\nof the training data./one.superior/two.superior"}
{"chunk_id": "d06p0007c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "Evenwhenfollowingthedefaults,therearesomeimportantdetailsthatshouldbe\nconsidered. We thus provide our ﬁrst set of technical tips here:\nTips on Training Data Preprocessing\n• Makesurethatthesubwordvocabularyistrainedonasuﬃcientlylargesample\nof the training data./one.superior/two.superior\n• AsdiscussedinSection4.5,ahigherbatchsizemaybebeneﬁcialforthetraining\nand the batch size can be higher when excluding training sentences longer\nthan a given threshold. This can be controlled with parametermax_length (see\nSection4.4),butitmaybeagoodideatoexcludetoolongsentencesevenbefore\npreparingthetrainingdatausing t2t-datagen. ThiswaytheTFRecordstraining\nﬁles will be smaller and their processing a bit faster./one.superior/three.superior"}
{"chunk_id": "d06p0007c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "than a given threshold. This can be controlled with parametermax_length (see\nSection4.4),butitmaybeagoodideatoexcludetoolongsentencesevenbefore\npreparingthetrainingdatausing t2t-datagen. ThiswaytheTFRecordstraining\nﬁles will be smaller and their processing a bit faster./one.superior/three.superior\n/one.superior/one.superiorMoredetailsonT2TwithBPEsubwordunitsbySennrichetal.(2016)vs. theinternalimplementation\ncanbefoundinthetechnicalreport“MorphologicalandLanguage-AgnosticWordSegmentationforNMT”\nattached to the Deliverable 2.3 of the project QT21:http://www.qt21.eu/resources/.\n/one.superior/two.superiorThis is controlled by afile_byte_budget constant, which must be changed directly in the source code"}
{"chunk_id": "d06p0007c06", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "canbefoundinthetechnicalreport“MorphologicalandLanguage-AgnosticWordSegmentationforNMT”\nattached to the Deliverable 2.3 of the project QT21:http://www.qt21.eu/resources/.\n/one.superior/two.superiorThis is controlled by afile_byte_budget constant, which must be changed directly in the source code\ninT2Tv1.2.9. Asignoftoosmalltrainingdataforthesubwordvocabularyisthatthe min_count asreported\nin the logs is too low, so the vocabulary is estimated from words seen only once or twice.\n/one.superior/three.superiorWe did no such pre-ﬁltering in our experiments.\n7"}
{"chunk_id": "d06p0008c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "4. Experiments\nIn this section, we present several experiments, always summarizing the obser-\nvations and giving some generally applicable tips that we learned. All experiments\nwere done with T2T v1.2.9 unless stated otherwise.\nWe experiment with two sets of hyper-parameters pre-deﬁned in T2T:trans-\nformer_big_single_gpu (BIG) andtransformer_base_single_gpu (BASE), which diﬀer\nmainly in the size of the model. Note thattransformer_big_single_gpu and trans-\nformer_base_single_gpu are just names of a set of hyper-parameters, which can be\napplied even when training on multiple GPUs, as we do in our experiments, see\nSection 4.7./one.superior⁴\nOur baseline setting uses the BIG model with its default hyper-parameters except\nfor:"}
{"chunk_id": "d06p0008c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "former_base_single_gpu are just names of a set of hyper-parameters, which can be\napplied even when training on multiple GPUs, as we do in our experiments, see\nSection 4.7./one.superior⁴\nOur baseline setting uses the BIG model with its default hyper-parameters except\nfor:\n• batch_size=1500 (see the discussion of diﬀerent sizes in Section 4.5),\n• --train_steps=6000000, i.e. high enough, so we can stop each experiment man-\nually as needed,\n• --save_checkpoints_secs=3600 which forces checkpoint saving each hour (see\nSection 4.10),\n• --schedule=train which disables the internal evaluation withapprox_bleu and\nthus makes training a bit faster (see Section 2)./one.superior⁵\n4.1. Computation Speed and Training Throughput"}
{"chunk_id": "d06p0008c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "ually as needed,\n• --save_checkpoints_secs=3600 which forces checkpoint saving each hour (see\nSection 4.10),\n• --schedule=train which disables the internal evaluation withapprox_bleu and\nthus makes training a bit faster (see Section 2)./one.superior⁵\n4.1. Computation Speed and Training Throughput\nWe are primarily interested in the translation quality (BLEU learning curves and\nTimeTillScore)andwediscussitinthefollowingsections4.2–4.10. Inthissection,we\nfocushoweveronlyonthe computationspeed andtrainingthroughput. Bothareaﬀected\nby three important factors: batch size, number of used GPUs and model size. The\nspeed is usually almost constant for a given experiment./one.superior⁶\nTable 2 shows the computation speed and training throughput for a single GPU"}
{"chunk_id": "d06p0008c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "by three important factors: batch size, number of used GPUs and model size. The\nspeed is usually almost constant for a given experiment./one.superior⁶\nTable 2 shows the computation speed and training throughput for a single GPU\nand various batch sizes and model sizes (BASE and BIG). The BASE model allows\nfor using a higher batch size than the BIG model. The cells where the BIG model\nresulted in out-of-memory errors are marked with “OOM”./one.superior⁷We can see that the\n/one.superior⁴According to our experiments (not reported here),transformer_big_single_gpu is better thantrans-\nformer_big even when training on 8 GPUs, although the naming suggests that the T2T authors had an\nopposite experience."}
{"chunk_id": "d06p0008c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "/one.superior⁴According to our experiments (not reported here),transformer_big_single_gpu is better thantrans-\nformer_big even when training on 8 GPUs, although the naming suggests that the T2T authors had an\nopposite experience.\n/one.superior⁵Alsotherearesomeproblemswiththealternativeschedules train_and_evaluate (itneedsmoremem-\nory) andcontinuous_train_and_eval (see https://github.com/tensorflow/tensor2tensor/issues/556).\n/one.superior⁶TensorBoard showsglobal_step/sec statistics, i.e. the computation speed curve. These curves in our\nexperimentsarealmostconstantforthewholetrainingwithvariationwithin2%,exceptformomentswhen\na checkpoint is being saved (and the computation speed is thus much slower)."}
{"chunk_id": "d06p0008c06", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "/one.superior⁶TensorBoard showsglobal_step/sec statistics, i.e. the computation speed curve. These curves in our\nexperimentsarealmostconstantforthewholetrainingwithvariationwithin2%,exceptformomentswhen\na checkpoint is being saved (and the computation speed is thus much slower).\n/one.superior⁷For these experiments, we usedmax_length=50 in order to be able to test bigger batch sizes. However,\nin additional experiments we checked thatmax_length does not aﬀect the training throughput itself.\n8"}
{"chunk_id": "d06p0009c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 9, "text": "model\nbatch_size BASE BIG\n500 43.4k 23.6k\n1000 30.2k 13.5k\n1500 22.3k 9.8k\n2000 16.8k 7.5k\n2500 14.4k 6.5k\n3000 12.3k OOM\n4500 8.2k OOM\n6000 6.6k OOM\n(a) Computation speed (steps/hour)\nmodel\nbatch_size BASE BIG\n500 21.7M 11.9M\n1000 30.2M 13.5M\n1500 33.4M 14.7M\n2000 33.7M 15.0M\n2500 36.0M 16.2M\n3000 37.0M OOM\n4500 36.7M OOM\n6000 39.4M OOM\n(b) Training throughput (subwords/hour)\nTable 2: Computation speed and training throughput for a single GPU.\ncomputation speed decreases with increasing batch size because not all operations\nin GPU are fully batch-parallelizable. The training throughput grows sub-linearly\nwith increasing batch size, so based on these experiments only, there is just a small\nadvantage when setting the batch size to the maximum value. We will return to this"}
{"chunk_id": "d06p0009c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 9, "text": "in GPU are fully batch-parallelizable. The training throughput grows sub-linearly\nwith increasing batch size, so based on these experiments only, there is just a small\nadvantage when setting the batch size to the maximum value. We will return to this\nquestion in Section 4.5, while taking into account the translation quality.\nWe can also see the BASE model has approximately two times bigger throughput\nas well as computation speed relative to the BIG model.\nGPUs steps/hour subwords/hour\n1 9.8k 14.7M\n2 7.4k 22.2M\n6 5.4k 48.6M\n8 5.6k 67.2M\nTable 3: Computation speed and training throughput for various numbers of GPUs,\nwith the BIG model andbatch_size=1500.\nTable 3 uses the BIG model andbatch_size=1500, while varying the number of"}
{"chunk_id": "d06p0009c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 9, "text": "GPUs steps/hour subwords/hour\n1 9.8k 14.7M\n2 7.4k 22.2M\n6 5.4k 48.6M\n8 5.6k 67.2M\nTable 3: Computation speed and training throughput for various numbers of GPUs,\nwith the BIG model andbatch_size=1500.\nTable 3 uses the BIG model andbatch_size=1500, while varying the number of\nGPUs. The overhead in GPU synchronization is apparent from the decreasing com-\nputation speed. Nevertheless, the training throughput still grows with more GPUs,\nso e.g. with 6 GPUs we process 3.2 times more training data per hour relative to\na single GPU (while without any overhead we would hypothetically expect 6 times\nmore data).\n9"}
{"chunk_id": "d06p0010c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 10, "text": "24\n25\n26\n27\n0 50 100 150 200 250\n1 2 3 4 5 6 7 8 9 10 11\nBLEU\nTraining time (hours)\nTraining time (days)\n58M training sentences\n16M training sentences\nFigure 1: Training data size eﬀect. BLEU learning curves for our main training\ndataset with 58 million sentence pairs and an alternative training dataset with 16\nmillion sentence pairs. Both trained with 8 GPUs, BIG model andbatch_size=1500.\nThe overhead when scaling to multiple GPUs is smaller than the overhead when\nscaling to a higher batch size. Scaling from a single GPU to 6 GPUs increases the\nthroughput 3.2 times, but scaling from batch size 1000 to 6000 on a single GPU\nincreases the throughput 1.3 times.\n4.2. Training Data Size\nForthisexperiment,wesubstitutedCzEng1.7withCzEng1.0inthetrainingdata,"}
{"chunk_id": "d06p0010c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 10, "text": "scaling to a higher batch size. Scaling from a single GPU to 6 GPUs increases the\nthroughput 3.2 times, but scaling from batch size 1000 to 6000 on a single GPU\nincreases the throughput 1.3 times.\n4.2. Training Data Size\nForthisexperiment,wesubstitutedCzEng1.7withCzEng1.0inthetrainingdata,\nso the total training size is 16 million sentence pairs (255M / 226M of English/Czech\nwords). Figure1comparestheBLEUlearningcurvesoftwoexperimentswhichdiﬀer\nonly in the training data: the baseline CzEng 1.7 versus the smaller CzEng 1.0. Both\nare trained on the same hardware with the same hyper-parameters (8 GPUs, BIG,\nbatch_size=1500). Training on the smaller dataset (2.5 times smaller in the number\nof words) converges to BLEU of about 25.5 after two days of training and does not"}
{"chunk_id": "d06p0010c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 10, "text": "are trained on the same hardware with the same hyper-parameters (8 GPUs, BIG,\nbatch_size=1500). Training on the smaller dataset (2.5 times smaller in the number\nof words) converges to BLEU of about 25.5 after two days of training and does not\nimprove over the next week of training. Training on the bigger dataset gives slightly\nworse results in the ﬁrst eight hours of training (not shown in the graph) but clearly\nbetter results after two days of training, reaching over 26.5 BLEU after eight days./one.superior⁸\nWithbatch_size=1500 and8GPUs,trainingoneepochofthesmallerdataset(with\nCzEng 1.0) takes 27k steps (5 hours of training), compared to 83k steps (15 hours) for\nthe bigger dataset (with CzEng 1.7). This meansabout 10 epochs in the smaller dataset"}
{"chunk_id": "d06p0010c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 10, "text": "Withbatch_size=1500 and8GPUs,trainingoneepochofthesmallerdataset(with\nCzEng 1.0) takes 27k steps (5 hours of training), compared to 83k steps (15 hours) for\nthe bigger dataset (with CzEng 1.7). This meansabout 10 epochs in the smaller dataset\nwere needed for reaching the convergenceand this is also the moment when the bigger\n/one.superior⁸We compared the two datasets also in another experiment with two GPUs, where CzEng 1.7 gave\nslightly worse results than CzEng 1.0 during the ﬁrst two days of training but clearly better results after\neight days. We hypothesize CzEng 1.0 is somewhat cleaner than CzEng 1.7.\n10"}
{"chunk_id": "d06p0011c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 11, "text": "dataset startsbeing clearlybetter. However,even 18epochs inthe bigger datasetwere not\nenough to reach the convergence. enough to reach the convergence\nTips on Training Data Size\n• For comparing diﬀerent datasets (e.g. smaller and cleaner vs. bigger and\nnoisier), we need to train long enough becauseresults after ﬁrst hours (or days if\ntraining on a single GPU) may be misleading.\n• For large training data (as CzEng 1.7 which has over half a gigaword),BLEU\nimproves even after one week of training on eight GPUs(or after 20 days of training\non two GPUs in another experiment).\n• We cannot easily interpolate one dataset results to another dataset.While the smaller\ntraining data (with CzEng 1.0) converged after 2 days, the main training data"}
{"chunk_id": "d06p0011c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 11, "text": "improves even after one week of training on eight GPUs(or after 20 days of training\non two GPUs in another experiment).\n• We cannot easily interpolate one dataset results to another dataset.While the smaller\ntraining data (with CzEng 1.0) converged after 2 days, the main training data\n(with CzEng 1.7), which is 2.5 times bigger, continues improving even after\n2.5×2 days./one.superior⁹\n4.3. Model Size\nChoosing the right model size is important for practical reasons: larger models\nmaynotﬁtanymoreonyourGPUortheymayrequiretouseaverysmallbatchsize.\nWe experiment with two models,/two.superior⁰as pre-deﬁned in Tensor2Tensor –transfor-\nmer_big_single_gpu (BIG) andtransformer_base_single_gpu (BASE), which diﬀer in\nfour hyper-parameters summarized in Table 4."}
{"chunk_id": "d06p0011c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 11, "text": "maynotﬁtanymoreonyourGPUortheymayrequiretouseaverysmallbatchsize.\nWe experiment with two models,/two.superior⁰as pre-deﬁned in Tensor2Tensor –transfor-\nmer_big_single_gpu (BIG) andtransformer_base_single_gpu (BASE), which diﬀer in\nfour hyper-parameters summarized in Table 4.\nmodel hidden_size ﬁlter_size num_heads adam_beta2\nBASE 512 2048 8 0.980\nBIG 1024 4096 16 0.998\nTable4: transformer_big_single_gpu (BIG)and transformer_base_single_gpu (BASE)\nhyper-parameter diﬀerences.\nFigure2showsthatonasingleGPU,theBIGmodelbecomesclearlybetterthanthe\nBASEmodelafter4hoursoftrainingifwekeepthebatchsizethesame–2000(andwe\nhave conﬁrmed it with 1500 in other experiments). However, the BASE model takes\nlessmemory,sowecanaﬀordahigherbatchsize,inourcase4500(withno max_length"}
{"chunk_id": "d06p0011c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 11, "text": "Figure2showsthatonasingleGPU,theBIGmodelbecomesclearlybetterthanthe\nBASEmodelafter4hoursoftrainingifwekeepthebatchsizethesame–2000(andwe\nhave conﬁrmed it with 1500 in other experiments). However, the BASE model takes\nlessmemory,sowecanaﬀordahigherbatchsize,inourcase4500(withno max_length\nrestriction,seethenextsection),whichimprovestheBLEU(seeSection4.5). Buteven\n/one.superior⁹Althoughsuchanexpectationmayseemnaïve,wecanﬁnditinliterature. Forexample,Bottou(2012)\ninSection4.2writes: “ Expectthevalidationperformancetoplateauafteranumberofepochsroughlycomparableto\nthe number of epochs needed to reach this point on the small training set.”\n/two.superior⁰We tried also a model three times as large as BASE (1.5 times as large as BIG), but it did not reach"}
{"chunk_id": "d06p0011c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 11, "text": "inSection4.2writes: “ Expectthevalidationperformancetoplateauafteranumberofepochsroughlycomparableto\nthe number of epochs needed to reach this point on the small training set.”\n/two.superior⁰We tried also a model three times as large as BASE (1.5 times as large as BIG), but it did not reach\nbetter results than BIG, so we don’t report it here.\n11"}
{"chunk_id": "d06p0012c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 12, "text": "16\n18\n20\n22\n24\n0 10 20 30 40 50 60 70\nBLEU\nTraining time (hours)\nBIG model, batch size 2000, 1 GPU\nBASE model, batch size 4500, 1 GPU\nBASE model, batch size 2000, 1 GPU\nFigure 2: Eﬀect of model size and batch size on a single GPU.\n22\n23\n24\n25\n26\n0 10 20 30 40 50\nBLEU\nTraining time (hours)\nBIG model, batch size 1500, 8 GPUs\nBASE model, batch size 4500, 8 GPUs\nFigure 3: Eﬀect of model size and batch size on 8 GPUs.\nso, after less than one day of training, BIG with batch size 2000 becomes better than\nBASE with batch size 4500 (or even 6000 withmax_length=70 in another experiment)\nand the diﬀerence grows up to 1.8 BLEU after three days of training.\nFigure3conﬁrmsthiswith8GPUs–hereBIGwithbatchsize1500becomesclearly\nbetter than BASE with batch size 4500 after 18 hours of training."}
{"chunk_id": "d06p0012c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 12, "text": "BASE with batch size 4500 (or even 6000 withmax_length=70 in another experiment)\nand the diﬀerence grows up to 1.8 BLEU after three days of training.\nFigure3conﬁrmsthiswith8GPUs–hereBIGwithbatchsize1500becomesclearly\nbetter than BASE with batch size 4500 after 18 hours of training.\nTips on Model Size\n• Prefer the BIG over the BASE modelif you plan to train longer than one day and\nhave 11 GB (or more) memory available on GPU.\n• With less memory you should benchmark BIG and BASE with the maximum\npossible batch size.\n12"}
{"chunk_id": "d06p0013c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 13, "text": "maximum batch size longer sentences\nmax_length BIG+Adam BIG+Adafactor BASE+Adam train test\nnone 2040 2550 4950 0.0% 0.0%\n150 2230 2970 5430 0.2% 0.0%\n100 2390 3280 5990 0.7% 0.3%\n70 2630 3590 6290 2.1% 2.2%\n50 2750 3770 6430 5.0% 9.1%\nTable5: Maximumbatchsizewhichﬁtsinto11GBmemoryforvariouscombinations\nof max_length (maximum sentence length in subwords), model size (base or big)\nand optimizer (Adam or Adafactor). The last two columns show the percentage of\nsentences in the train (CzEng 1.7) and test (wmt13) data that are longer than a given\nthreshold.\n• For fast debugging (of model-size-unrelated aspects) use a model calledtrans-\nformer_tiny.\n4.4. Maximum Training Sentence Length\nTheparameter max_length speciﬁesthemaximumlengthofasentenceinsubwords."}
{"chunk_id": "d06p0013c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 13, "text": "sentences in the train (CzEng 1.7) and test (wmt13) data that are longer than a given\nthreshold.\n• For fast debugging (of model-size-unrelated aspects) use a model calledtrans-\nformer_tiny.\n4.4. Maximum Training Sentence Length\nTheparameter max_length speciﬁesthemaximumlengthofasentenceinsubwords.\nLonger sentences (either in source or target language) are excluded from the training\ncompletely. If nomax_length is speciﬁed (which is the default),batch_size is used\ninstead. Loweringthemax_length allowstouseahigherbatchsizeorabiggermodel.\nSince the Transformer implementation in T2T can suddenly run out of memory even\nafter several hours of training, it is good to know how large batch size ﬁts in your\nGPU. Table 5 presents what we empirically measured for the BASE and BIG models"}
{"chunk_id": "d06p0013c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 13, "text": "Since the Transformer implementation in T2T can suddenly run out of memory even\nafter several hours of training, it is good to know how large batch size ﬁts in your\nGPU. Table 5 presents what we empirically measured for the BASE and BIG models\nwith Adam and Adafactor/two.superior/one.superioroptimizers and variousmax_length values.\nSettingmax_length too low would result in excluding too many training sentences\nand biasing the translation towards shorter sentences, which would hurt the trans-\nlation quality. The last two columns in Table 5 show that settingmax_length to 70\n(resp. 100) results in excluding only 2.1% (resp. 0.7%) of sentences in the training\ndata, and only 2.2% (resp. 0.3%) sentences in the development test data are longer,"}
{"chunk_id": "d06p0013c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 13, "text": "lation quality. The last two columns in Table 5 show that settingmax_length to 70\n(resp. 100) results in excluding only 2.1% (resp. 0.7%) of sentences in the training\ndata, and only 2.2% (resp. 0.3%) sentences in the development test data are longer,\nso the detrimental eﬀect of smaller training data and length bias should be minimal\nin this setting. However, our experiments withbatch_size=1500 in Figure 4 show a\nstrange drop in BLEU after one hour of training for all experiments withmax_length\n70 or lower. Even withmax_length 150 or 200 the BLEU learning curve is worse than\nwithmax_length=400,whichﬁnallygivesthesameresultasnotusingany max_length\n/two.superior/one.superiorTheAdafactoroptimizer(ShazeerandStern,2018)isavailableonlyinT2T1.4.2ornewerandhasthree"}
{"chunk_id": "d06p0013c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 13, "text": "70 or lower. Even withmax_length 150 or 200 the BLEU learning curve is worse than\nwithmax_length=400,whichﬁnallygivesthesameresultasnotusingany max_length\n/two.superior/one.superiorTheAdafactoroptimizer(ShazeerandStern,2018)isavailableonlyinT2T1.4.2ornewerandhasthree\ntimes smaller models than Adam because it does not store ﬁrst and second moments for all weights. We\nleave further experiments with Adafactor for future work.\n13"}
{"chunk_id": "d06p0014c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 14, "text": "0\n5\n10\n15\n0 1 2 3 4 5 6 7 8 9\nBLEU\nTraining time (hours)\nmax length 400\nmax length 200\nmax length 150\nmax length 70\nmax length 50\nmax length 25\nFigure 4: Eﬀect of restricting the training data to variousmax_length values. All\ntrained on a single GPU with the BIG model andbatch_size=1500. An experiment\nwithout anymax_length is not shown, but it has the same curve asmax_length=400.\nrestriction. The training loss ofmax_length=25 (and 50 and 70) has high variance and\nstops improving after the ﬁrst hour of training but shows no sudden increase (as in\nthe case of diverged training discussed in Section 4.6 when the learning rate is too\nhigh). We have no explanation for this phenomenon./two.superior/two.superior\nWe did another set of experiments with varyingmax_length, but this time with"}
{"chunk_id": "d06p0014c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 14, "text": "the case of diverged training discussed in Section 4.6 when the learning rate is too\nhigh). We have no explanation for this phenomenon./two.superior/two.superior\nWe did another set of experiments with varyingmax_length, but this time with\nbatch_size=2000 instead of 1500. In this case,max_length 25 and 50 still results in\nslowergrowingBLEUcurves,but70andhigherhasthesamecurveasno max_length\nrestriction. So in our case,if the batch size is high enough, themax_length has almost no\neﬀect on BLEU, but this should be checked for each new dataset.\nWe trained several models with variousmax_length for three days and observed\nthattheyarenotabletoproducelongertranslationsthanwhatwasthemaximumlengthused\nin training, even if we change the decoding parameter alpha.\nTips onmax_length"}
{"chunk_id": "d06p0014c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 14, "text": "eﬀect on BLEU, but this should be checked for each new dataset.\nWe trained several models with variousmax_length for three days and observed\nthattheyarenotabletoproducelongertranslationsthanwhatwasthemaximumlengthused\nin training, even if we change the decoding parameter alpha.\nTips onmax_length\n• Set (a reasonably low)max_length. This allows to use a higher batch size and\nprevents out-of-memory errors after several hours of training. Also, with a\nhigher percentage of training sentences that are almostmax_length long, there\nisahigherchancethatthetrainingwillfaileitherimmediately(ifthebatchsize\nis too high) or never (otherwise).,\n• Set a reasonably highmax_length. Consider the percentage of sentences excluded\nfromtrainingandfromthetargeteddevelopmenttestsetandalsowatchforun-"}
{"chunk_id": "d06p0014c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 14, "text": "isahigherchancethatthetrainingwillfaileitherimmediately(ifthebatchsize\nis too high) or never (otherwise).,\n• Set a reasonably highmax_length. Consider the percentage of sentences excluded\nfromtrainingandfromthetargeteddevelopmenttestsetandalsowatchforun-\nexpecteddrops(orstagnations)oftheBLEUcurveintheﬁrsthoursoftraining.\n/two.superior/two.superiorhttps://github.com/tensorflow/tensor2tensor/issues/582\n14"}
{"chunk_id": "d06p0015c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 15, "text": "10\n12\n14\n16\n18\n20\n22\n0 10 20 30 40 50 60 70\nBLEU\nTraining time (hours)\nBASE, batch size 6000\nBASE, batch size 4500\nBASE, batch size 3000\nBASE, batch size 1500\nBASE, batch size 1000\nFigure 5: Eﬀect of the batch size with the BASE model. All trained on a single GPU.\n4.5. Batch Size\nThedefault batch_size valueinrecentT2Tversionsis4096subwordsforallmodels\nexcept for transformer_base_single_gpu, where the default is 2048. However, we\nrecommend to always set the batch size explicitly/two.superior/three.superioror at least make a note what was\nthe default in a given T2T version when reporting experimental results.\nFigure5showslearningcurvesforﬁvediﬀerentbatchsizes(1000,1500,3000,4500\nand 6000) for experiments with a single GPU and the BASE model./two.superior⁴A higher batch"}
{"chunk_id": "d06p0015c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 15, "text": "the default in a given T2T version when reporting experimental results.\nFigure5showslearningcurvesforﬁvediﬀerentbatchsizes(1000,1500,3000,4500\nand 6000) for experiments with a single GPU and the BASE model./two.superior⁴A higher batch\nsize up to 4500is clearly better in terms of BLEU as measured by Time Till Score and\nExamplesTillScoremetricsdeﬁnedinSection4.1. Forexample,togetoverBLEUof18\nwithbatch_size=3000,weneed7hours(260Mexamples),andwith batch_size=1500,\nwe need about 3 days (2260M examples) i.e. 10 times longer (9 time more examples).\nFromTable2aweknowthatbiggerbatcheshaveslowercomputationspeed,sowhen\nre-plotting Figure 5 with steps instead of time on the x-axis, the diﬀerence between\nthe curves would be even bigger. From Table 2b we know that bigger batches have"}
{"chunk_id": "d06p0015c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 15, "text": "FromTable2aweknowthatbiggerbatcheshaveslowercomputationspeed,sowhen\nre-plotting Figure 5 with steps instead of time on the x-axis, the diﬀerence between\nthe curves would be even bigger. From Table 2b we know that bigger batches have\nslightly higher training throughput, so when re-plotting with number of examples\nprocessed on the x-axis, the diﬀerence will be smaller, but still visible. The only\nexceptionisthediﬀerencebetweenbatchsize4500and6000, whichisverysmalland\n/two.superior/three.superiore.g. --hparams=\"batch_size=1500,learning_rate=0.20,learning_rate_warmup_steps=16000\"\nAs the batch size is speciﬁed in subwords, we see no advantage in using power-of-two values.\n/two.superior⁴All the experiments in Figure 5 usemax_length=70, but we have got the same curves when re-running"}
{"chunk_id": "d06p0015c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 15, "text": "As the batch size is speciﬁed in subwords, we see no advantage in using power-of-two values.\n/two.superior⁴All the experiments in Figure 5 usemax_length=70, but we have got the same curves when re-running\nwithout anymax_length restrictions, except forbatch_size=6000 which failed with OOM.\n15"}
{"chunk_id": "d06p0016c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 16, "text": "0\n5\n10\n15\n20\n0 5 10 15 20 25 30 35\nBLEU\nTraining time (hours)\nBIG, batch size 2000\nBIG, batch size 1500\nBIG, batch size 1450\nBIG, batch size 1400\nBIG, batch size 1300\nBIG, batch size 1000\nFigure 6: Eﬀect of the batch size with the BIG model. All trained on a single GPU.\ncanbefullyexplainedbythefactthatbatchsize6000has7%higherthroughputthan\nbatch size 4500.\nSo for the BASE model, a higher batch size gives better results, although with dimin-\nishing returns. This observation goes against the common knowledge in other NMT\nframeworks and deep learning in general (Keskar et al., 2017) that smaller batches\nproceedslower(trainingexamplesperhour)butresultinbettergeneralization(higher\ntest-set BLEU) in the end. In our experiments with the BASE model in T2T, bigger"}
{"chunk_id": "d06p0016c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 16, "text": "frameworks and deep learning in general (Keskar et al., 2017) that smaller batches\nproceedslower(trainingexamplesperhour)butresultinbettergeneralization(higher\ntest-set BLEU) in the end. In our experiments with the BASE model in T2T, bigger\nbatches are not only faster in training throughput (as could be expected), but also\nfaster in convergence speed, Time Till Score and Examples Till Score.\nInterestingly, when replicating these experimentswith the BIG model, we see quite\ndiﬀerentresults,asshowninFigure6. TheBIGmodelneedsacertainminimalbatchsize\ntostartconvergingatall,butforhigherbatchsizesthereisalmostnodiﬀerenceinthe\nBLEUcurves(butstill,biggerbatchnevermakestheBLEUworseinourexperiments).\nIn our case, the sharp diﬀerence is between batch size 1450, which trains well, and"}
{"chunk_id": "d06p0016c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 16, "text": "tostartconvergingatall,butforhigherbatchsizesthereisalmostnodiﬀerenceinthe\nBLEUcurves(butstill,biggerbatchnevermakestheBLEUworseinourexperiments).\nIn our case, the sharp diﬀerence is between batch size 1450, which trains well, and\n1400, which drops oﬀ after two hours of training, recovering only slowly.\nAccording to Smith and Le (2017) and Smith et al. (2017), thegradient noise scale,\ni.e. scaleofrandomﬂuctuationsintheSGD(orAdametc.) dynamics,isproportional\nto learning rate divided by the batch size (cf. Section 4.8). Thus when lowering the\nbatchsize,weincreasethenoisescaleandthetrainingmay diverge. Thismaybeeither\npermanent, as in the case of batch size 1000 in Figure 6, or temporary, as in the case\nof batch size 1300 and 1400, where the BLEU continues to grow after the temporary"}
{"chunk_id": "d06p0016c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 16, "text": "batchsize,weincreasethenoisescaleandthetrainingmay diverge. Thismaybeeither\npermanent, as in the case of batch size 1000 in Figure 6, or temporary, as in the case\nof batch size 1300 and 1400, where the BLEU continues to grow after the temporary\ndrop, but much more slowly than the non-diverged curves.\nWe are not sure what causes the diﬀerence between the BASE and BIG models\nwith regards to the sensitivity to batch size. One hypothesis is that the BIG model is\n16"}
{"chunk_id": "d06p0017c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 17, "text": "0\n5\n10\n15\n20\n0 5 10 15 20 25 30 35 40 45\nBLEU\nTraining time (hours)\nlearning rate 0.25\nlearning rate 0.20\nlearning rate 0.10\nlearning rate 0.05\nlearning rate 0.01\nFigure 7: Eﬀect of the learning rate on a single GPU. All trained on CzEng 1.0 with\nthe default batch size (1500) and warmup steps (16k).\nmore diﬃcult to initialize and thus more sensitive to divergence in the early training\nphase. Also while for BASE, increasing the batch size was highly helpful until 4500,\nfor BIG this limit may be below 1450, i.e. below the minimal batch size needed for\npreventing diverged training.\nTip on Batch Size\n• Batch size should be set as high as possiblewhile keeping a reserve for not hitting\nthe out-of-memory errors. It is advisable to establish the largest possible batch"}
{"chunk_id": "d06p0017c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 17, "text": "for BIG this limit may be below 1450, i.e. below the minimal batch size needed for\npreventing diverged training.\nTip on Batch Size\n• Batch size should be set as high as possiblewhile keeping a reserve for not hitting\nthe out-of-memory errors. It is advisable to establish the largest possible batch\nsize before starting the main and long training.\n4.6. Learning Rate and Warmup Steps on a Single GPU\nThe default learning rate in T2T translation models is 0.20. Figure 7 shows that\nvarying the value within range 0.05–0.25 makes almost no diﬀerence. Setting the\nlearningratetoolow(0.01)resultsinnotablyslowerconvergence. Settingthelearning\nrate too high (0.30, not shown in the ﬁgure) results indivergedtraining, which means"}
{"chunk_id": "d06p0017c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 17, "text": "varying the value within range 0.05–0.25 makes almost no diﬀerence. Setting the\nlearningratetoolow(0.01)resultsinnotablyslowerconvergence. Settingthelearning\nrate too high (0.30, not shown in the ﬁgure) results indivergedtraining, which means\nin this case that the learning curve starts growing as usual, but at one moment drops\ndown almost to zero and stays there forever.\nA common solution to prevent diverged training is to decrease thelearning_rate\nparameter or increaselearning_rate_warmup_steps or introduce gradient clipping.\nThe learning_rate_warmup_steps parameter conﬁgures alinear_warmup_rsqrt_decay\nschedule/two.superior⁵and it is set to 16 000 by default (for the BIG model), meaning that within\n/two.superior⁵The schedule was callednoam in T2T versions older than 1.4.4.\n17"}
{"chunk_id": "d06p0018c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 18, "text": "0\n5\n10\n15\n20\n0 5 10 15 20 25 30 35 40 45\nBLEU\nTraining time (hours)\nwarmup steps 12k\nwarmup steps 14k\nwarmup steps 16k\nwarmup steps 32k\nwarmup steps 48k\nFigure 8: Eﬀect of the warmup steps on a single GPU. All trained on CzEng 1.0 with\nthe default batch size (1500) and learning rate (0.20).\nthe ﬁrst 16k steps the learning rate grows linearly and then follows an inverse square\nroot decay (t−0.5, cf. Section 4.8.3). At 16k steps, the actual learning rate is thus the\nhighest.\nIf a divergence is to happen, it usually happens within the ﬁrst few hours of\ntraining, when the actual learning rate becomes the highest. Once we increased the\nwarmup steps from 16k to 32k, we were able to train with the learning rate of 0.30"}
{"chunk_id": "d06p0018c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 18, "text": "highest.\nIf a divergence is to happen, it usually happens within the ﬁrst few hours of\ntraining, when the actual learning rate becomes the highest. Once we increased the\nwarmup steps from 16k to 32k, we were able to train with the learning rate of 0.30\nand even 0.50 without any divergence. The learning curves looked similarly to the\nbaselineone(withdefaultvaluesof16kwarmupstepsandlearningrate0.20). When\ntryinglearningrate1.0,wehadtoincreasewarmupstepsto60k(with40kthetraining\ndivergedafteronehour)–thisresultedinaslowerconvergenceatﬁrst(about3BLEU\nlowerthanthebaselineafter8hoursoftraining),butafter3–4daysoftraininghaving\nthe same curve as the baseline.\nFigure 8 shows the eﬀect of diﬀerent warmup steps with a ﬁxed learning rate (the"}
{"chunk_id": "d06p0018c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 18, "text": "divergedafteronehour)–thisresultedinaslowerconvergenceatﬁrst(about3BLEU\nlowerthanthebaselineafter8hoursoftraining),butafter3–4daysoftraininghaving\nthe same curve as the baseline.\nFigure 8 shows the eﬀect of diﬀerent warmup steps with a ﬁxed learning rate (the\ndefault0.20). Settingwarmupstepstoolow(12k)resultsindivergedtraining. Setting\nthem too high (48k, green curve) results in a slightly slower convergence at ﬁrst, but\nmatching the baseline after a few hours of training.\nWe can conclude that for a single GPU and the BIG model, there is a relatively\nlargerangeoflearningrateandwarmupstepsvaluesthatachievetheoptimalresults.\nThe default valueslearning_rate=0.20 and learning_rate_warmup_steps=16000 are\nwithin this range.\nTips on Learning Rate and Warmup Steps"}
{"chunk_id": "d06p0018c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 18, "text": "We can conclude that for a single GPU and the BIG model, there is a relatively\nlargerangeoflearningrateandwarmupstepsvaluesthatachievetheoptimalresults.\nThe default valueslearning_rate=0.20 and learning_rate_warmup_steps=16000 are\nwithin this range.\nTips on Learning Rate and Warmup Steps\n• In case of diverged training, try gradient clipping and/or more warmup steps.\n18"}
{"chunk_id": "d06p0019c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 19, "text": "• Ifthatdoesnothelp(orifthewarmupstepsaretoohighrelativetotheexpected\ntotal training steps), try decreasing the learning rate.\n• Note that when you decrease warmup steps (and keep learning rate), you also\nincrease the maximum actual learning rate because of the way how thelin-\near_warmup_rsqrt_decay (akanoam) schedule is implemented./two.superior⁶\n4.7. Number of GPUs\nT2T allows to train with multiple GPUs on the same machine simply using the\nparameter --worker_gpus./two.superior⁷As explained in Section 2.3, the parameterbatch_size is\ninterpreted per GPU, so with 8 GPUs, theeﬀective batch sizeis 8 times bigger.\nAsingle-GPUexperimentwithbatchsize4000,shouldgiveexactlythesameresults\nas two GPUs and batch size 2000 and as four GPUs and batch size 1000 because the"}
{"chunk_id": "d06p0019c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 19, "text": "interpreted per GPU, so with 8 GPUs, theeﬀective batch sizeis 8 times bigger.\nAsingle-GPUexperimentwithbatchsize4000,shouldgiveexactlythesameresults\nas two GPUs and batch size 2000 and as four GPUs and batch size 1000 because the\neﬀective batch size is 4000 in all three cases. We have conﬁrmed this empirically. By\nthe “same results” we mean BLEU (or train loss) versus training steps on the x-axis.\nWhenconsideringtime,thefour-GPUexperimentwillbethefastestone,asexplained\nin Section 4.1.\nFigure 9 shows BLEU curves for diﬀerent numbers of GPUs and the BIG model\nwith batch size, learning rate and warmup steps ﬁxed on their default values (1500,\n0.20and16k,respectively). Ascouldbeexpected,trainingwithmoreGPUsconverges"}
{"chunk_id": "d06p0019c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 19, "text": "in Section 4.1.\nFigure 9 shows BLEU curves for diﬀerent numbers of GPUs and the BIG model\nwith batch size, learning rate and warmup steps ﬁxed on their default values (1500,\n0.20and16k,respectively). Ascouldbeexpected,trainingwithmoreGPUsconverges\nfaster. WhatisinterestingistheTimeTillScore. Table6liststheapproximatetraining\ntimeandnumberoftrainingexamples(inmillionsofsubwords)neededto“surpass”\n(i.e. achieve and never again fall below) BLEU of 25.6.\n# GPUs hours subwords (M)\n1 >600 >9000\n2 203 2322 ·2 = 4644\n6 56 451 ·6 = 2706\n8 40 341 ·8 = 2728\nTable 6: Time and training data consumed to reach BLEU of 25.6, i.e. Time Till Score\nandExamplesTillScore. Notethattheexperimenton1GPUwasendedafter25days\nof training without clearly surpassing the threshold (already outside of Figure 9)."}
{"chunk_id": "d06p0019c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 19, "text": "2 203 2322 ·2 = 4644\n6 56 451 ·6 = 2706\n8 40 341 ·8 = 2728\nTable 6: Time and training data consumed to reach BLEU of 25.6, i.e. Time Till Score\nandExamplesTillScore. Notethattheexperimenton1GPUwasendedafter25days\nof training without clearly surpassing the threshold (already outside of Figure 9).\n/two.superior⁶This holds at least in T2T versions 1.2.9–1.5.2, but as it is somewhat unexpected/unintuitive for some\nusers, it may be ﬁxed in future, seehttps://github.com/tensorflow/tensor2tensor/issues/517.\n/two.superior⁷and making sure environment variableCUDA_VISIBLE_DEVICES is set so enough cards are visible. T2T\nallows also distributed training (on multiple machines), but we have not experimented with it. Both"}
{"chunk_id": "d06p0019c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 19, "text": "/two.superior⁷and making sure environment variableCUDA_VISIBLE_DEVICES is set so enough cards are visible. T2T\nallows also distributed training (on multiple machines), but we have not experimented with it. Both\nsingle-machine multi-gpu and distributed training use synchronous Adam updates by default.\n19"}
{"chunk_id": "d06p0020c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 20, "text": "23.5\n24\n24.5\n25\n25.5\n26\n26.5\n27\n0 50 100 150 200 250 300 350\n1 2 3 4 5 6 7 8 9 10 11 12 13 14\nBLEU\nTraining time (hours)\nTraining time (days)\n8GPU\n6GPU\n2GPU\n1GPU\n25.6\nFigure 9: Eﬀect of the number of GPUs. BLEU=25.6 is marked with a black line.\nWe can see thattwo GPUs are more than three times faster than a single GPUwhen\nmeasuring the Time Till Score and need much less training examples (i.e. they have\nlowerExamplesTillScore). Similarly, eightGPUsaremorethanﬁvetimesfasterthantwo\nGPUsand 1.7 times less training data is needed.\nRecall that in Figure 6 we have shown that increasing the batch size from 1450 to\n2000hasalmost noeﬀectontheBLEUcurve. However, whenincreasingtheeﬀective\nbatch size by using more GPUs, the improvement is higher than could be expected"}
{"chunk_id": "d06p0020c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 20, "text": "GPUsand 1.7 times less training data is needed.\nRecall that in Figure 6 we have shown that increasing the batch size from 1450 to\n2000hasalmost noeﬀectontheBLEUcurve. However, whenincreasingtheeﬀective\nbatch size by using more GPUs, the improvement is higher than could be expected\nfrom the higher throughput./two.superior⁸We ﬁnd this quite surprising, especially considering\nthe fact that we have not tuned the learning rate and warmup steps (see the next\nsection).\nTips on the Number of GPUs\n• For the fastest BLEU convergenceuse as many GPUs as available(in our experi-\nments up to 8).\n• This holdseven when there are more experimentsto be done. For example, it is\nbetter to run one 8-GPUs experiment after another, rather than running two"}
{"chunk_id": "d06p0020c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 20, "text": "section).\nTips on the Number of GPUs\n• For the fastest BLEU convergenceuse as many GPUs as available(in our experi-\nments up to 8).\n• This holdseven when there are more experimentsto be done. For example, it is\nbetter to run one 8-GPUs experiment after another, rather than running two\n4-GPUs experiments in parallel or eight single-GPU experiments in parallel.\n/two.superior⁸It would be interesting to try simulating multi-GPU training on a single GPU, simply by doing the\nupdateonceafterNbatches(andsummingthegradients). Thisissimilartothe ghostbatches ofHoﬀeretal.\n(2017), but using ghost batch size higher than the actual batch size. We leave this for future work.\n20"}
{"chunk_id": "d06p0021c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "4.8. Learning Rate and Warmup Steps on Multiple GPUs\n4.8.1. Related Work\nThereisagrowingnumberofpapersonscalingdeeplearningtomultiplemachines\nwithsynchronousSGD(oritsvariants)byincreasingtheeﬀectivebatchsize. Wewill\nfocus mostly on the question how to adapt the learning rate schedule, when scaling\nfrom one GPU (or any device, in general) tok GPUs.\nKrizhevsky (2014) says “Theory suggests that when multiplying the batch size byk,\none should multiply the learning rate by\n√\nk to keep the variance in the gradient expectation\nconstant”, without actually explaining which theory suggests so. However, in the\nexperimentalparthereportsthatwhatworkedthebest, wasa linearscalingheuristics ,\ni.e. multiplying the learning rate byk, again without any explanation nor details on\nthe diﬀerence between\n√"}
{"chunk_id": "d06p0021c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "constant”, without actually explaining which theory suggests so. However, in the\nexperimentalparthereportsthatwhatworkedthebest, wasa linearscalingheuristics ,\ni.e. multiplying the learning rate byk, again without any explanation nor details on\nthe diﬀerence between\n√\nk scaling andk scaling.\nThe linear scaling heuristics become popular, leading to good scaling results in\npractice(Goyaletal.,2017;Smithetal.,2017)andalsotheoreticalexplanations(Bottou\netal.,2016;SmithandLe,2017;Jastrzebskietal.,2017). SmithandLe(2017)interpret\nSGD (and its variants) as a stochastic diﬀerential equation and show that thegradient\nnoise scaleg /equalxϵ\n( N\nB − 1\n)\n, whereϵis the learning rate,N is the training set size, and\nB is the eﬀective batch size. This noise “drives SGD away from sharp minima, and"}
{"chunk_id": "d06p0021c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "SGD (and its variants) as a stochastic diﬀerential equation and show that thegradient\nnoise scaleg /equalxϵ\n( N\nB − 1\n)\n, whereϵis the learning rate,N is the training set size, and\nB is the eﬀective batch size. This noise “drives SGD away from sharp minima, and\ntherefore there is an optimal batch size which maximizes the test set accuracy”. In other\nwords for keeping the optimal level of gradient noise (which leads to “ﬂat minima”\nthat generalize well), we need to scale the learning rate linearly when increasing the\neﬀective batch size.\nHowever,Hoﬀeretal.(2017)suggesttouse\n√\nk scalinginsteadofthelinearscaling\nand provide both theoretical and empirical support for this claim. They show that\ncov(∆w,∆w)∝ ϵ2\nNB ,thusifwewanttokeepthethecovariancematrixoftheparameters"}
{"chunk_id": "d06p0021c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "eﬀective batch size.\nHowever,Hoﬀeretal.(2017)suggesttouse\n√\nk scalinginsteadofthelinearscaling\nand provide both theoretical and empirical support for this claim. They show that\ncov(∆w,∆w)∝ ϵ2\nNB ,thusifwewanttokeepthethecovariancematrixoftheparameters\nupdate step∆w in the same range for any eﬀective batch sizeB, we need to scale the\nlearningrateproportionallytothesquarerootof B. Theyfoundthat\n√\nk scalingworks\nbetter than linear scaling on CIFAR10./two.superior⁹You et al. (2017) conﬁrm linear scaling does\nnot perform well on ImageNet and suggest to use Layer-wise Adaptive Rate Scaling.\nWe can see that large-batch training is still an open research question. Most of the\npapers cited above have experimental support only from the image recognition tasks"}
{"chunk_id": "d06p0021c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "not perform well on ImageNet and suggest to use Layer-wise Adaptive Rate Scaling.\nWe can see that large-batch training is still an open research question. Most of the\npapers cited above have experimental support only from the image recognition tasks\n(usuallyImageNet)andconvolutionalnetworks(e.g. ResNet),soitisnotclearwhether\ntheir suggestions can be applied also on sequence-to-sequence tasks (NMT) with\nself-attentional networks (Transformer). There are several other diﬀerences as well:\nModernconvolutionalnetworksareusuallytrainedwith batchnormalization (Ioﬀeand\nSzegedy, 2015), which seems to be important for the scaling, while Transformer uses\n/two.superior⁹Toclosethegapbetweensmall-batchtrainingandlarge-batchtraining,Hoﬀeretal.(2017)introduce(in\nadditionto\n√"}
{"chunk_id": "d06p0021c06", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "Modernconvolutionalnetworksareusuallytrainedwith batchnormalization (Ioﬀeand\nSzegedy, 2015), which seems to be important for the scaling, while Transformer uses\n/two.superior⁹Toclosethegapbetweensmall-batchtrainingandlarge-batchtraining,Hoﬀeretal.(2017)introduce(in\nadditionto\n√\nk scaling)so-called ghostbatchnormalization andadaptedtrainingregime ,whichmeansdecaying\nthe learning rate after a given number of steps instead of epochs.\n21"}
{"chunk_id": "d06p0022c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "layer normalization(Lei Ba et al., 2016)./three.superior⁰Also, Transformer uses Adam together with\nan inverse-square-root learning-rate decay, while most ImageNet papers use SGD\nwith momentum and piecewise-constant learning-rate decay.\n4.8.2. Our Experiments\nWe decided to ﬁnd out empirically the optimal learning rate for training on 8\nGPUs. Increasing the learning rate from 0.20 to 0.30 resulted in diverged training\n(BLEU dropped to almost 0 after two hours of training). Similarly to our single-GPU\nexperiments (Section 4.6), we were able prevent the divergence by increasing the\nwarmupstepsorbyintroducinggradientclipping(e.g. with clip_grad_norm=1.0,we\nwere able to use learning rate 0.40, but increasing it further to 0.60 led to divergence"}
{"chunk_id": "d06p0022c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "experiments (Section 4.6), we were able prevent the divergence by increasing the\nwarmupstepsorbyintroducinggradientclipping(e.g. with clip_grad_norm=1.0,we\nwere able to use learning rate 0.40, but increasing it further to 0.60 led to divergence\nanyway). However, none of these experiments led to any improvements over the default\nlearning rate– all had about the same BLEU curve after few hours of training.\nJastrzebski et al. (2017) shows that “the invariance under simultaneous rescaling of\nlearning rate and batch size breaks down if the learning rate gets too large or the batch size\ngets too small”. A similar observation was reported e.g. by Bottou et al. (2016). Thus\nour initial hypothesis was that 0.20 (or 0.25) is the maximal learning rate suitable for"}
{"chunk_id": "d06p0022c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "learning rate and batch size breaks down if the learning rate gets too large or the batch size\ngets too small”. A similar observation was reported e.g. by Bottou et al. (2016). Thus\nour initial hypothesis was that 0.20 (or 0.25) is the maximal learning rate suitable for\nstable training in our experiments even when we scale from a single GPU to 8 GPUs.\nConsideringthisinitialhypothesis,weweresurprisedthatwewereabletoachieveso\ngoodTimeTillScorewith8GPUs(morethan8timessmallerrelativetoasingleGPU,\nas reported in Table 6). To answer this riddle we need to understand how learning\nrate schedules are implemented in T2T.\n4.8.3. Parametrization of Learning Rate Schedules in T2T\nIn most works on learning rate schedules/three.superior/one.superiorthe “time” parameter is actually inter-"}
{"chunk_id": "d06p0022c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "as reported in Table 6). To answer this riddle we need to understand how learning\nrate schedules are implemented in T2T.\n4.8.3. Parametrization of Learning Rate Schedules in T2T\nIn most works on learning rate schedules/three.superior/one.superiorthe “time” parameter is actually inter-\npreted as the number of epochs or training examples. For example a popular setup\nforpiecewise-constantdecayinImageNettraining(e.g.Goyaletal.,2017)istodivide\nthe learning rate by a factor of 10 at the 30-th, 60-th, and 80-th epoch.\nHowever,inT2T,itisthe global_step variablethatisusedasthe“time”parameter.\nSo when increasing the eﬀective batch size 8 times, e.g. by using 8 GPUs instead of a\nsingleGPU,theactuallearningrate /three.superior/two.superiorachievesagivenvalueafterthesamenumberof"}
{"chunk_id": "d06p0022c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "However,inT2T,itisthe global_step variablethatisusedasthe“time”parameter.\nSo when increasing the eﬀective batch size 8 times, e.g. by using 8 GPUs instead of a\nsingleGPU,theactuallearningrate /three.superior/two.superiorachievesagivenvalueafterthesamenumberof\nsteps,butthismeansafter8timeslesstrainingexamples. Fortheinverse-square-root\n/three.superior⁰Applying batch normalization on RNN is diﬃcult. Transformer does not use RNN, but still we were\nnot successful in switching to batch normalization (and possibly ghost batch normalization) due to NaN\nloss errors.\n/three.superior/one.superiorExamplesoflearningrateschedulesareinverse-square-rootdecay,inverse-timedecay,exponentialde-\ncay, piecewise-constant decay, seehttps://www.tensorflow.org/api_guides/python/train#Decaying_the_"}
{"chunk_id": "d06p0022c06", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "loss errors.\n/three.superior/one.superiorExamplesoflearningrateschedulesareinverse-square-rootdecay,inverse-timedecay,exponentialde-\ncay, piecewise-constant decay, seehttps://www.tensorflow.org/api_guides/python/train#Decaying_the_\nlearning_rate for TF implementations.\n/three.superior/two.superiorByactuallearningratewemeanthelearningrateafterapplyingthedecayschedule. The learning_rate\nparameter stays the same in this case.\n22"}
{"chunk_id": "d06p0023c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 23, "text": "decay, we haveactual_lr(steps)/equalxc ·steps−0.5 /equalx1√\n8 ·actual_lr(steps ·8), where c is a\nconstant containing also thelearning_rate parameter. So with 8 GPUs, if we divide\nthe learning_rate parameter by\n√\n8, we achieve the same actual learning rate after a\ngiven number of training examples as in the original single-GPU setting.\nThis explains the riddle from the previous section.By keeping thelearning_rate\nparameter the same when scaling tok times bigger eﬀective batch, we actually increase the\nactuallearningrate\n√\nk times,inaccordancewiththesuggestionofHoﬀeretal.(2017). /three.superior/three.superior\nThis holds only for thelinear_warmup_rsqrt_decay (aka noam) schedule and ignoring\nthe warmup steps.\nIf we want to keep the same learning rate also in the warmup phase, we would"}
{"chunk_id": "d06p0023c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 23, "text": "actuallearningrate\n√\nk times,inaccordancewiththesuggestionofHoﬀeretal.(2017). /three.superior/three.superior\nThis holds only for thelinear_warmup_rsqrt_decay (aka noam) schedule and ignoring\nthe warmup steps.\nIf we want to keep the same learning rate also in the warmup phase, we would\nneed to divide the warmup steps byk. However, this means that the maximum\nactual learning rate will be\n√\nk times higher, relative to the single-GPU maximal\nactual learning rate and this leads to divergence in our experiments. In deed, many\nresearchers (e.g. Goyal et al., 2017) suggest to use a warmup when scaling to more\nGPUs in order to prevent divergence. Transformer uses learning rate warmup by\ndefault even for single-GPU training (cf. Section 4.6), but it makes sense to use more"}
{"chunk_id": "d06p0023c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 23, "text": "researchers (e.g. Goyal et al., 2017) suggest to use a warmup when scaling to more\nGPUs in order to prevent divergence. Transformer uses learning rate warmup by\ndefault even for single-GPU training (cf. Section 4.6), but it makes sense to use more\nwarmup training examples in multi-GPU setting.\nIn our experiments with 8 GPUs and the default learning rate 0.20, using 8k\nwarmup steps instead of the default 16k had no eﬀect on the BLEU curve (it was a\nbit higher in the ﬁrst few hours, but the same afterwards). Further decreasing the\nwarmup steps resulted in a retarded BLEU curve (for 6k) or a complete divergence\n(for 2k).\nTips on Learning Rate and Warmup Steps on Multiple GPUs\n• Keep the learning_rate parameter at its optimal value found in single-GPU\nexperiments."}
{"chunk_id": "d06p0023c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 23, "text": "warmup steps resulted in a retarded BLEU curve (for 6k) or a complete divergence\n(for 2k).\nTips on Learning Rate and Warmup Steps on Multiple GPUs\n• Keep the learning_rate parameter at its optimal value found in single-GPU\nexperiments.\n• Youcantrydecreasingthewarmupsteps,butlessthanlinearlyandyoushould\nnot expect to improve the ﬁnal BLEU this way.\n4.9. Resumed Training\nT2Tallowstoresumetrainingfromacheckpoint,simplybypointingthe output_dir\nparametertoadirectorywithanexistingcheckpoint(speciﬁedinthe checkpoint ﬁle).\nThismaybeusefulwhenthetrainingfails(e.g. becauseofhardwareerror), whenwe\nneed to continue training on a diﬀerent machine or during hyper-parameter search,\nwhen we want to continue with the most promising setups. T2T saves also Adam"}
{"chunk_id": "d06p0023c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 23, "text": "Thismaybeusefulwhenthetrainingfails(e.g. becauseofhardwareerror), whenwe\nneed to continue training on a diﬀerent machine or during hyper-parameter search,\nwhen we want to continue with the most promising setups. T2T saves also Adam\n/three.superior/three.superiorIn addition to suggesting the\n√\nk learning-rate scaling, Hoﬀer et al. (2017) show that to fully close the\n“generalization gap”, we need to train longer because the absolute number of steps (updates) matters. So\nfrom this point of view, using steps instead of epochs as the time parameter for learning rate schedules\nmay not be a completely wrong idea.\n23"}
{"chunk_id": "d06p0024c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 24, "text": "26\n26.2\n26.4\n26.6\n26.8\n110 120 130 140 150 160 170 180\nBLEU\nTraining time (hours)\naveraging 16 checkpoints\naveraging 8 checkpoints\nno averaging\nFigure 10: Eﬀect of checkpoint averaging. All trained on 6 GPUs.\nmomentumintothecheckpoint, sothetrainingcontinuesalmostasifithadnotbeen\nstopped. However, it does not store the position in the training data – it starts from a\nrandom position. Also the relative time (and wall-clock time) in TensorBoard graphs\nwill be inﬂuenced by the stopping.\nResumed training can also be exploited for changing some hyper-parameters,\nwhich cannot be meta-parametrized by the number of steps. For example, Smith\net al. (2017) suggest to increase the eﬀective batch size (and number of GPUs) during\ntraining, instead of decaying the learning rate."}
{"chunk_id": "d06p0024c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 24, "text": "Resumed training can also be exploited for changing some hyper-parameters,\nwhich cannot be meta-parametrized by the number of steps. For example, Smith\net al. (2017) suggest to increase the eﬀective batch size (and number of GPUs) during\ntraining, instead of decaying the learning rate.\nYet another usage is to do domain adaptation by switching from (large) general-\ndomain training data to (small) target-domain training data for the few last epochs.\nInthiscase,considereditingalsothelearningrateorlearningrateschedule(orfaking\nthe global_step stored in the checkpoint) to make sure the learning rate is not too\nsmall.\n4.10. Checkpoint Averaging\nVaswanietal.(2017)suggesttoaveragethelast20checkpointssavedin10-minute\nintervals (usingutils/avg_checkpoints.py). According to our experiments slightly"}
{"chunk_id": "d06p0024c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 24, "text": "the global_step stored in the checkpoint) to make sure the learning rate is not too\nsmall.\n4.10. Checkpoint Averaging\nVaswanietal.(2017)suggesttoaveragethelast20checkpointssavedin10-minute\nintervals (usingutils/avg_checkpoints.py). According to our experiments slightly\nbetterresultsareachievedwithaveragingcheckpointssavedin1-hourintervals. This\nhas also the advantage that less time is spent with checkpoint saving, so the training\nis faster.\nFigure 10 shows the eﬀect of averaging is twofold: the averaged curve has lower\nvariance (ﬂickering) from checkpoint to checkpoint and it is almost always better\nthan the baseline without averaging (usually by about 0.2 BLEU). In some setups,\nwe have seen improvements due to averaging over 1 BLEU. In the early phases of"}
{"chunk_id": "d06p0024c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 24, "text": "variance (ﬂickering) from checkpoint to checkpoint and it is almost always better\nthan the baseline without averaging (usually by about 0.2 BLEU). In some setups,\nwe have seen improvements due to averaging over 1 BLEU. In the early phases of\ntraining, while the (baseline) learning curve grows fast, it is better to use fewer\n24"}
{"chunk_id": "d06p0025c01", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 25, "text": "Manual Automatic Scores\n# Ave % Ave z BLEU TER CharacTER BEER System\n– – – 23.8 0.662 0.582 0.543 T2T 8 GPUs 8 days\n1 62.0 0.308 22.8 0.667 0.588 0.540 uedin-nmt\n2 59.7 0.240 20.1 0.703 0.612 0.519 online-B\n3 55.9 0.111 20.2 0.696 0.607 0.524 limsi-factored\n55.2 0.102 20.0 0.699 - - LIUM-FNMT\n55.2 0.090 20.2 0.701 0.605 0.522 LIUM-NMT\n54.1 0.050 20.5 0.696 0.624 0.523 CU-Chimera\n53.3 0.029 16.6 0.743 0.637 0.503 online-A\n8 41.9 -0.327 16.2 0.757 0.697 0.485 PJATK\nTable7: WMT17systemsforEnglish-to-CzechandourbestT2Ttrainingrun. Manual\nscores are from the oﬃcial WMT17 ranking. Automatic metrics were provided by\nhttp://matrix.statmt.org/. For *TER metrics, lower is better. Best results in bold,\nsecond-best in italics."}
{"chunk_id": "d06p0025c02", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 25, "text": "8 41.9 -0.327 16.2 0.757 0.697 0.485 PJATK\nTable7: WMT17systemsforEnglish-to-CzechandourbestT2Ttrainingrun. Manual\nscores are from the oﬃcial WMT17 ranking. Automatic metrics were provided by\nhttp://matrix.statmt.org/. For *TER metrics, lower is better. Best results in bold,\nsecond-best in italics.\ncheckpoints for averaging. In later phases (as shown in Figure 10, after 4.5–7.5 days\nof training), it seems that 16 checkpoints (covering last 16 hours) give slightly better\nresults on average than 8 checkpoints, but we have not done any proper evaluation\nfor signiﬁcance (using paired bootstrap testing for each hour and then summarizing\nthe results).\nThe fact that resumed training starts from a random position in the training data"}
{"chunk_id": "d06p0025c03", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 25, "text": "results on average than 8 checkpoints, but we have not done any proper evaluation\nfor signiﬁcance (using paired bootstrap testing for each hour and then summarizing\nthe results).\nThe fact that resumed training starts from a random position in the training data\n(cf. Section 4.9) can be actually exploited for “forking” a training to get two (or\nmore) copies of the model, which are trained for the same number of steps, but\nindependentlyinthelaterstagesandthusendingwithdiﬀerentweightssavedinthe\nﬁnal checkpoint. These semi-independent models can be averaged in the same way\nascheckpointsfromthesamerun, asdescribedabove. Ourpreliminaryresultsshow\nthis helps a bit (on top of checkpoint averaging).\nTips on Checkpoint Averaging"}
{"chunk_id": "d06p0025c04", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 25, "text": "ﬁnal checkpoint. These semi-independent models can be averaged in the same way\nascheckpointsfromthesamerun, asdescribedabove. Ourpreliminaryresultsshow\nthis helps a bit (on top of checkpoint averaging).\nTips on Checkpoint Averaging\n• Use it. Averaging 8 checkpoints takes about 5 minutes, so it is a “BLEU boost\nfor free” (compared with the time needed for the whole training).\n• See the tools for automatic checkpoint averaging and evaluation described in\nSection 2.4.\n5. Comparison with WMT17 Systems\nTable 7 provides the results of WMT17 English-to-Czech news translation task,\nwith our best Transformer model (BIG trained on 8 GPUs for 8 days, averaging 8\ncheckpoints) evaluated using the exact same implementation of automatic metrics."}
{"chunk_id": "d06p0025c05", "doc_id": 7, "doc": "TrainingTipsForTransformers.pdf", "page": 25, "text": "Section 2.4.\n5. Comparison with WMT17 Systems\nTable 7 provides the results of WMT17 English-to-Czech news translation task,\nwith our best Transformer model (BIG trained on 8 GPUs for 8 days, averaging 8\ncheckpoints) evaluated using the exact same implementation of automatic metrics.\nWhile the automatic evaluation is not fully reliable (see e.g. the high BLEU score\n25"}
{"chunk_id": "d07p0001c01", "doc_id": 8, "doc": "Transformers.pdf", "page": 1, "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗ †\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best"}
{"chunk_id": "d07p0001c02", "doc_id": 8, "doc": "Transformers.pdf", "page": 1, "text": "aidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-"}
{"chunk_id": "d07p0001c03", "doc_id": 8, "doc": "Transformers.pdf", "page": 1, "text": "entirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data."}
{"chunk_id": "d07p0001c04", "doc_id": 8, "doc": "Transformers.pdf", "page": 1, "text": "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every"}
{"chunk_id": "d07p0001c05", "doc_id": 8, "doc": "Transformers.pdf", "page": 1, "text": "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research."}
{"chunk_id": "d07p0001c06", "doc_id": 8, "doc": "Transformers.pdf", "page": 1, "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023"}
{"chunk_id": "d07p0002c01", "doc_id": 8, "doc": "Transformers.pdf", "page": 2, "text": "1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently"}
{"chunk_id": "d07p0002c02", "doc_id": 8, "doc": "Transformers.pdf", "page": 2, "text": "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-"}
{"chunk_id": "d07p0002c03", "doc_id": 8, "doc": "Transformers.pdf", "page": 2, "text": "computation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in"}
{"chunk_id": "d07p0002c04", "doc_id": 8, "doc": "Transformers.pdf", "page": 2, "text": "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,"}
{"chunk_id": "d07p0002c05", "doc_id": 8, "doc": "Transformers.pdf", "page": 2, "text": "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2."}
{"chunk_id": "d07p0002c06", "doc_id": 8, "doc": "Transformers.pdf", "page": 2, "text": "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-"}
{"chunk_id": "d07p0002c07", "doc_id": 8, "doc": "Transformers.pdf", "page": 2, "text": "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9]."}
{"chunk_id": "d07p0002c08", "doc_id": 8, "doc": "Transformers.pdf", "page": 2, "text": "entirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2"}
{"chunk_id": "d07p0003c01", "doc_id": 8, "doc": "Transformers.pdf", "page": 3, "text": "Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer"}
{"chunk_id": "d07p0003c02", "doc_id": 8, "doc": "Transformers.pdf", "page": 3, "text": "wise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections"}
{"chunk_id": "d07p0003c03", "doc_id": 8, "doc": "Transformers.pdf", "page": 3, "text": "Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,"}
{"chunk_id": "d07p0003c04", "doc_id": 8, "doc": "Transformers.pdf", "page": 3, "text": "predictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3"}
{"chunk_id": "d07p0004c01", "doc_id": 8, "doc": "Transformers.pdf", "page": 4, "text": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together"}
{"chunk_id": "d07p0004c02", "doc_id": 8, "doc": "Transformers.pdf", "page": 4, "text": "query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V) = softmax(QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1√dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is"}
{"chunk_id": "d07p0004c03", "doc_id": 8, "doc": "Transformers.pdf", "page": 4, "text": "of 1√dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n.\n3.2.2 Multi-Head Attention"}
{"chunk_id": "d07p0004c04", "doc_id": 8, "doc": "Transformers.pdf", "page": 4, "text": "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random"}
{"chunk_id": "d07p0004c05", "doc_id": 8, "doc": "Transformers.pdf", "page": 4, "text": "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4"}
{"chunk_id": "d07p0005c01", "doc_id": 8, "doc": "Transformers.pdf", "page": 5, "text": "output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nWhere the projections are parameter matricesWQ\ni ∈ Rdmodel×dk , WK\ni ∈ Rdmodel×dk , WV\ni ∈ Rdmodel×dv\nand WO ∈ Rhdv×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality."}
{"chunk_id": "d07p0005c02", "doc_id": 8, "doc": "Transformers.pdf", "page": 5, "text": "i ∈ Rdmodel×dv\nand WO ∈ Rhdv×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9]."}
{"chunk_id": "d07p0005c03", "doc_id": 8, "doc": "Transformers.pdf", "page": 5, "text": "and the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward"}
{"chunk_id": "d07p0005c04", "doc_id": 8, "doc": "Transformers.pdf", "page": 5, "text": "encoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This"}
{"chunk_id": "d07p0005c05", "doc_id": 8, "doc": "Transformers.pdf", "page": 5, "text": "3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input"}
{"chunk_id": "d07p0005c06", "doc_id": 8, "doc": "Transformers.pdf", "page": 5, "text": "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\n5"}
{"chunk_id": "d07p0006c01", "doc_id": 8, "doc": "Transformers.pdf", "page": 6, "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 · d) O(1) O(1)\nRecurrent O(n · d2) O(n) O(n)\nConvolutional O(k · n · d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the"}
{"chunk_id": "d07p0006c02", "doc_id": 8, "doc": "Transformers.pdf", "page": 6, "text": "Self-Attention (restricted) O(r · n · d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )"}
{"chunk_id": "d07p0006c03", "doc_id": 8, "doc": "Transformers.pdf", "page": 6, "text": "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two"}
{"chunk_id": "d07p0006c04", "doc_id": 8, "doc": "Transformers.pdf", "page": 6, "text": "relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden"}
{"chunk_id": "d07p0006c05", "doc_id": 8, "doc": "Transformers.pdf", "page": 6, "text": "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range"}
{"chunk_id": "d07p0006c06", "doc_id": 8, "doc": "Transformers.pdf", "page": 6, "text": "One is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types."}
{"chunk_id": "d07p0006c07", "doc_id": 8, "doc": "Transformers.pdf", "page": 6, "text": "and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6"}
{"chunk_id": "d07p0007c01", "doc_id": 8, "doc": "Transformers.pdf", "page": 7, "text": "length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,"}
{"chunk_id": "d07p0007c02", "doc_id": 8, "doc": "Transformers.pdf", "page": 7, "text": "path length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,"}
{"chunk_id": "d07p0007c03", "doc_id": 8, "doc": "Transformers.pdf", "page": 7, "text": "recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching"}
{"chunk_id": "d07p0007c04", "doc_id": 8, "doc": "Transformers.pdf", "page": 7, "text": "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training"}
{"chunk_id": "d07p0007c05", "doc_id": 8, "doc": "Transformers.pdf", "page": 7, "text": "target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the"}
{"chunk_id": "d07p0007c06", "doc_id": 8, "doc": "Transformers.pdf", "page": 7, "text": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,"}
{"chunk_id": "d07p0007c07", "doc_id": 8, "doc": "Transformers.pdf", "page": 7, "text": "rate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7"}
{"chunk_id": "d07p0008c01", "doc_id": 8, "doc": "Transformers.pdf", "page": 8, "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the"}
{"chunk_id": "d07p0008c02", "doc_id": 8, "doc": "Transformers.pdf", "page": 8, "text": "GNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation"}
{"chunk_id": "d07p0008c03", "doc_id": 8, "doc": "Transformers.pdf", "page": 8, "text": "Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models."}
{"chunk_id": "d07p0008c04", "doc_id": 8, "doc": "Transformers.pdf", "page": 8, "text": "listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We"}
{"chunk_id": "d07p0008c05", "doc_id": 8, "doc": "Transformers.pdf", "page": 8, "text": "dropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a"}
{"chunk_id": "d07p0008c06", "doc_id": 8, "doc": "Transformers.pdf", "page": 8, "text": "inference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8"}
{"chunk_id": "d07p0009c01", "doc_id": 8, "doc": "Transformers.pdf", "page": 9, "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop ϵls\ntrain PPL BLEU params\nsteps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7"}
{"chunk_id": "d07p0009c02", "doc_id": 8, "doc": "Transformers.pdf", "page": 9, "text": "4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head"}
{"chunk_id": "d07p0009c03", "doc_id": 8, "doc": "Transformers.pdf", "page": 9, "text": "checkpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our"}
{"chunk_id": "d07p0009c04", "doc_id": 8, "doc": "Transformers.pdf", "page": 9, "text": "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37]."}
{"chunk_id": "d07p0009c05", "doc_id": 8, "doc": "Transformers.pdf", "page": 9, "text": "constituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting."}
{"chunk_id": "d07p0009c06", "doc_id": 8, "doc": "Transformers.pdf", "page": 9, "text": "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9"}
