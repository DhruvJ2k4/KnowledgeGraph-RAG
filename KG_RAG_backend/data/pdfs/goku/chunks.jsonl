{"chunk_id": "d00p0001c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 1, "text": "Training Tips for the Transformer Model\nMartin Popel, Ondřej Bojar\nCharles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics,\nPrague, Czechia\nAbstract\nThis article describes our experiments in neural machine translation using the recent Ten-\nsor2TensorframeworkandtheTransformersequence-to-sequencemodel(Vaswanietal.,2017).\nWe examine some of the critical parameters that aﬀect the ﬁnal translation quality, memory\nusage, training stability and training time, concluding each experiment with a set of recom-\nmendations for fellow researchers. In addition to conﬁrming the general mantra “more data\nand larger models”, we address scaling to multiple GPUs and provide practical tips for im-"}
{"chunk_id": "d00p0001c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 1, "text": "usage, training stability and training time, concluding each experiment with a set of recom-\nmendations for fellow researchers. In addition to conﬁrming the general mantra “more data\nand larger models”, we address scaling to multiple GPUs and provide practical tips for im-\nproved training regarding batch size, learning rate, warmup steps, maximum sentence length\nandcheckpointaveraging. Wehopethatourobservationswillallowotherstogetbetterresults\ngiven their particular hardware and data constraints.\n1. Introduction\nIt has been already clearly established that neural machine translation (NMT) is\nthe new state of the art in machine translation, see e.g. the most recent evaluation\ncampaigns(Bojaretal.,2017a;Cettoloetal.,2017). Manyfundamentalchangesofthe"}
{"chunk_id": "d00p0001c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 1, "text": "1. Introduction\nIt has been already clearly established that neural machine translation (NMT) is\nthe new state of the art in machine translation, see e.g. the most recent evaluation\ncampaigns(Bojaretal.,2017a;Cettoloetal.,2017). Manyfundamentalchangesofthe\nunderlying neural network architecture are nevertheless still frequent and it is very\ndiﬃcult to predict which of the architectures has the best combination of properties\ntowininthelongterm,consideringallrelevantcriterialiketranslationquality,model\nsize, stability and speed of training, interpretability but also practical availability of\ngoodimplementations. Aconsiderablepartofamodel’ssuccessintranslationquality\nconsists in the training data, the model’s sensitivity to noise in the data but also on a"}
{"chunk_id": "d00p0001c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 1, "text": "size, stability and speed of training, interpretability but also practical availability of\ngoodimplementations. Aconsiderablepartofamodel’ssuccessintranslationquality\nconsists in the training data, the model’s sensitivity to noise in the data but also on a\nwide range of hyper-parameters that aﬀect the training. Having the right setting of\nthem turns out to be often a critical component for the success.\n1\narXiv:1804.00247v2  [cs.CL]  2 May 2018"}
{"chunk_id": "d00p0002c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "Inthisarticle,weexperimentwitharelativelynewNMTmodel,calledTransformer\n(Vaswanietal.,2017)asimplementedintheTensor2Tensor /one.superior(abbreviatedT2T)toolkit,\nversion1.2.9. Themodelandthetoolkithavebeenreleasedshortlyaftertheevaluation\ncampaignatWMT2017 /two.superioranditsbehavioronlarge-datanewstranslationisnotyetfully\nexplored. We want to empirically explore some of the important hyper-parameters.\nHopefully, our observations will be useful also for other researchers considering this\nmodel and framework.\nWhileinvestigationsintotheeﬀectofhyper-parameterslikelearningrateandbatch\nsize are available in the deep-learning community (e.g. Bottou et al., 2016; Smith and\nLe, 2017; Jastrzebski et al., 2017), these are either mostly theoretic or experimentally"}
{"chunk_id": "d00p0002c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "model and framework.\nWhileinvestigationsintotheeﬀectofhyper-parameterslikelearningrateandbatch\nsize are available in the deep-learning community (e.g. Bottou et al., 2016; Smith and\nLe, 2017; Jastrzebski et al., 2017), these are either mostly theoretic or experimentally\nsupported from domains like image recognition rather than machine translation. In\nthis article, we ﬁll the gap by focusing exclusively on MT and on the Transformer\nmodel only, providing hopefully the best practices for this particular setting.\nSome of our observations conﬁrm the general wisdom (e.g. larger training data\naregenerallybetter)andquantifythebehavioronEnglish-to-Czechtranslationexper-\niments. Some of our observations are somewhat surprising, e.g. that two GPUs are"}
{"chunk_id": "d00p0002c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "Some of our observations conﬁrm the general wisdom (e.g. larger training data\naregenerallybetter)andquantifythebehavioronEnglish-to-Czechtranslationexper-\niments. Some of our observations are somewhat surprising, e.g. that two GPUs are\nmore than three times faster than a single GPU, or our ﬁndings about the interaction\nbetween maximum sentence length, learning rate and batch size.\nThearticleisstructuredasfollows. InSection2,wediscussourevaluationmethod-\nologyandmaincriteria: translationqualityandspeedoftraining. Section3describes\nour dataset and its preparations. Section 4 is the main contribution of the article: a\nset of commented experiments, each with a set of recommendations. Finally, Sec-\ntion 5 compares our best Transformer run with systems participating in WMT17. We"}
{"chunk_id": "d00p0002c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "our dataset and its preparations. Section 4 is the main contribution of the article: a\nset of commented experiments, each with a set of recommendations. Finally, Sec-\ntion 5 compares our best Transformer run with systems participating in WMT17. We\nconclude in Section 6.\n2. Evaluation Methodology\nMachine translation can be evaluated in many ways and some forms of human\njudgment should be always used for the ultimate resolution in any ﬁnal application.\nThecommonpracticeinMTresearchistoevaluatethemodelperformanceonatestset\nagainst one or more human reference translations. The most widespread automatic\nmetricisundoubtedlytheBLEUscore(Papinenietal.,2002),despiteitsacknowledged\nproblems and better-performing alternatives (Bojar et al., 2017b). For simplicity, we"}
{"chunk_id": "d00p0002c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "against one or more human reference translations. The most widespread automatic\nmetricisundoubtedlytheBLEUscore(Papinenietal.,2002),despiteitsacknowledged\nproblems and better-performing alternatives (Bojar et al., 2017b). For simplicity, we\nstick to BLEU, too (we evaluated all our results also with/c.sc/h.sc/r.scF(Popović, 2015), but\nfoundnosubstantialdiﬀerencesfromBLEU).Inparticular,weusethecase-insensitive\nsacréBLEU/three.superiorwhich uses a ﬁxed tokenization (identical tomteval-v14.pl --interna-\n/one.superiorhttps://github.com/tensorflow/tensor2tensor\n/two.superiorhttp://www.statmt.org/wmt17\n/three.superiorhttps://github.com/awslabs/sockeye/tree/master/contrib/sacrebleu\nThe signature of the BLEU scores reported in this paper isBLEU+case.lc+lang.en-cs+numrefs.1+smooth."}
{"chunk_id": "d00p0002c06", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 2, "text": "/one.superiorhttps://github.com/tensorflow/tensor2tensor\n/two.superiorhttp://www.statmt.org/wmt17\n/three.superiorhttps://github.com/awslabs/sockeye/tree/master/contrib/sacrebleu\nThe signature of the BLEU scores reported in this paper isBLEU+case.lc+lang.en-cs+numrefs.1+smooth.\nexp+test.wmt13+tok.intl+version.1.2.3.\n2"}
{"chunk_id": "d00p0003c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 3, "text": "tional-tokenization) and automatically downloads the reference translation for a\ngiven WMT testset.\n2.1. Considerations on Stopping Criterion\nThe situation in NMT is further complicated by the fact that the training of NMT\nsystemsisusuallynon-deterministic, ⁴ and(esp. withthemostrecentmodels)hardly\neverconvergesorstartsoverﬁtting ⁵ onreasonablybigdatasets. Thisleadstolearning\ncurvesthatneverfullyﬂattenletalonestartdecreasing(seeSection4.2). Thecommon\npractice of machine learning to evaluate the model on a ﬁnal test set when it started\noverﬁtting (or a bit sooner) is thus not applicable in practice.\nMany papers in neural machine translation do not specify any stopping criteria\nwhatsoever. Sometimes,theymentiononlyanapproximatenumberofdaysthemodel"}
{"chunk_id": "d00p0003c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 3, "text": "overﬁtting (or a bit sooner) is thus not applicable in practice.\nMany papers in neural machine translation do not specify any stopping criteria\nwhatsoever. Sometimes,theymentiononlyanapproximatenumberofdaysthemodel\nwas trained for, e.g. Bahdanau et al. (2015), sometimes the exact number of training\nsteps is given but no indication on “how much converged” the model was at that\npoint, e.g. Vaswani et al. (2017). Most probably, the training was run until no further\nimprovements were clearly apparent on the development test set, and the model was\nevaluated at that point. Such an approximate stopping criterion is rather risky: it is\nconceivablethatdiﬀerentsetupswerestoppedatdiﬀerentstagesoftrainingandtheir\ncomparison is not fair."}
{"chunk_id": "d00p0003c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 3, "text": "improvements were clearly apparent on the development test set, and the model was\nevaluated at that point. Such an approximate stopping criterion is rather risky: it is\nconceivablethatdiﬀerentsetupswerestoppedatdiﬀerentstagesoftrainingandtheir\ncomparison is not fair.\nA somewhat more reliable method is to keep training for a speciﬁed number of\niterations or a certain number of epochs. This is however not a perfect solution\neither, if the models are not quite converged at that time and the diﬀerence in their\nperformance is not suﬃciently large. It is quite possible that e.g. a more complex\nmodel would need a few more epochs and eventually arrived at a higher score than\nits competitor. Also, the duration of one training step (or one epoch) diﬀers between"}
{"chunk_id": "d00p0003c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 3, "text": "performance is not suﬃciently large. It is quite possible that e.g. a more complex\nmodel would need a few more epochs and eventually arrived at a higher score than\nits competitor. Also, the duration of one training step (or one epoch) diﬀers between\nmodels(seeSection4.1)andfromthepracticalpointofview,wearemostlyinterested\nin the wall-clock time.\nWhen we tried the standard technique of early stopping, whenN subsequent\nevaluationsonthedevelopmenttestsetdonotgiveimprovementslargerthanagiven\ndelta,wesawabigvarianceinthetrainingtimeandﬁnalBLEU,evenforexperiments\nwith the same hyper-parameters and just a diﬀerent random seed. Moreover to get\nthe best results, we would have had to use a very largeN and a very small delta."}
{"chunk_id": "d00p0003c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 3, "text": "delta,wesawabigvarianceinthetrainingtimeandﬁnalBLEU,evenforexperiments\nwith the same hyper-parameters and just a diﬀerent random seed. Moreover to get\nthe best results, we would have had to use a very largeN and a very small delta.\n⁴ Even if we ﬁx the random seed (which was not done properly in T2T v1.2.9), a change of some hyper-\nparameters may aﬀect the results not because of the change itself, but because it inﬂuenced the random\ninitialization.\n⁵ By overﬁtting we mean here that the translation quality (test-set BLEU) begins to worsen, while the\ntraining loss keeps improving.\n3"}
{"chunk_id": "d00p0004c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 4, "text": "2.2. Our Final Choice: Full Learning Curves\nBasedonthediscussionabove,wedecidedtoreportalwaysthefulllearningcurves\nand not just single scores. This solution does not fully prevent the risk of premature\njudgments, butthereaderscanatleastjudgeforthemselvesiftheywouldexpectany\nsudden twist in the results or not.\nIn all cases, we plot the case-insensitive BLEU score against the wall-clock time\nin hours. This solution obviously depends on the hardware chosen, so we always\nused the same equipment: one up to eight GeForce GTX 1080 Ti GPUs with NVIDIA\ndriver 375.66. Some variation in the measurements is unfortunately unavoidable\nbecause we could not fully isolate the computation from diﬀerent processes on the\nsame machine and from general network traﬃc, but based on our experiments with"}
{"chunk_id": "d00p0004c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 4, "text": "driver 375.66. Some variation in the measurements is unfortunately unavoidable\nbecause we could not fully isolate the computation from diﬀerent processes on the\nsame machine and from general network traﬃc, but based on our experiments with\nreplicated experiments such variation is negligible.\n2.3. Terminology\nFor clarity, we deﬁne the following terms and adhere to them for the rest of the\npaper:\nTranslation qualityis an automatic estimate of how well the translation carried out\nby a particular ﬁxed model expresses the meaning of the source. We estimate\ntranslation quality solely by BLEU score against one reference translation.\nTraining Stepsdenote the number of iterations, i.e. the number of times the opti-\nmizer update was run. This number also equals the number of (mini)batches"}
{"chunk_id": "d00p0004c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 4, "text": "translation quality solely by BLEU score against one reference translation.\nTraining Stepsdenote the number of iterations, i.e. the number of times the opti-\nmizer update was run. This number also equals the number of (mini)batches\nthat were processed.\nBatch Size isthenumberoftrainingexamplesusedbyoneGPUinonetrainingstep.\nIn sequence-to-sequence models, batch size is usually speciﬁed as the number\nofsentencepairs. However,theparameter batch_size inT2Ttranslationspeciﬁes\nthe approximate number oftokens(subwords) in one batch.⁶ This allows to use\na higher number of short sentences in one batch or a smaller number of long\nsentences.\nEﬀective Batch Sizeis the number of training examples consumed in one training\nstep. WhentrainingonmultipleGPUs,theparameter batch_size isinterpreted"}
{"chunk_id": "d00p0004c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 4, "text": "a higher number of short sentences in one batch or a smaller number of long\nsentences.\nEﬀective Batch Sizeis the number of training examples consumed in one training\nstep. WhentrainingonmultipleGPUs,theparameter batch_size isinterpreted\nperGPU.Thatis,with batch_size=1500 and8GPUs,thesystemactuallydigests\n12k subwords of each language in one step.\nTraining Epochcorresponds to one complete pass over the training data. Unfortu-\nnately, it is not easy to measure the number of training epochs in T2T.⁷ T2T\n⁶ For this purpose, the number of tokens in a sentence is deﬁned as the maximum of source and target\nsubwords. T2T also does reordering and bucketing of the sentences by their length to minimize the use of"}
{"chunk_id": "d00p0004c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 4, "text": "nately, it is not easy to measure the number of training epochs in T2T.⁷ T2T\n⁶ For this purpose, the number of tokens in a sentence is deﬁned as the maximum of source and target\nsubwords. T2T also does reordering and bucketing of the sentences by their length to minimize the use of\npadding symbols. However, some padding is still needed, thusbatch_size only approximates the actual\nnumber of (non-padding) subwords in a batch.\n⁷https://github.com/tensorflow/tensor2tensor/issues/415\n4"}
{"chunk_id": "d00p0005c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 5, "text": "reports only the number of training steps. In order to convert training steps to\nepochs, we need to multiply the steps by the eﬀective batch size and divide by\nthenumberofsubwordsinthetrainingdata(seeSection3.1). Thesegmentation\nofthetrainingdataintosubwordsisusuallyhiddentotheuserandthenumber\nof subwords must be thus computed by a special script.\nComputation Speedissimplytheobservednumberoftrainingstepsperhour. Com-\nputation speed obviously depends on the hardware (GPU speed, GPU-CPU\ncommunication) and software (driver version, CUDA library version, imple-\nmentation). The main parameters aﬀecting computation speed are the model\nsize,optimizerandothersettingsthatdirectlymodifytheformulaoftheneural\nnetwork.\nTraining Throughputis the amount of training data digested by the training. We"}
{"chunk_id": "d00p0005c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 5, "text": "mentation). The main parameters aﬀecting computation speed are the model\nsize,optimizerandothersettingsthatdirectlymodifytheformulaoftheneural\nnetwork.\nTraining Throughputis the amount of training data digested by the training. We\nreport training throughput in subwords per hour. Training Throughput equals\nto the Computation Speed multiplied by the eﬀective batch size.\nConvergence SpeedorBLEUConvergence istheincreaseinBLEUdividedbytime.\nConvergence speed changes heavily during training, starting very high and\ndecreasing as the training progresses. A converged model should have conver-\ngence speed of zero.\nTime Till Scoreis the training time needed to achieve a certain level of translation\nquality, in our case BLEU. We use this as an informal measure because it is not"}
{"chunk_id": "d00p0005c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 5, "text": "decreasing as the training progresses. A converged model should have conver-\ngence speed of zero.\nTime Till Scoreis the training time needed to achieve a certain level of translation\nquality, in our case BLEU. We use this as an informal measure because it is not\nclear how to deﬁne the moment of “achieving” a given BLEU score. We deﬁne\nit as time after which the BLEU never falls below the given level.⁸\nExamples Till Scoreis the number of training examples (in subwords) needed to\nachieve a certain level of BLEU. It equals to the Time Till Score multiplied by\nTraining Throughput.\n2.4. Tools for Evaluation within Tensor2Tensor\nT2T, being implemented in TensorFlow, provides nice TensorBoard visualizations\nof the training progress. The original implementation was optimized towards speed"}
{"chunk_id": "d00p0005c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 5, "text": "Training Throughput.\n2.4. Tools for Evaluation within Tensor2Tensor\nT2T, being implemented in TensorFlow, provides nice TensorBoard visualizations\nof the training progress. The original implementation was optimized towards speed\nof evaluation rather than towards following the standards of the ﬁeld. T2T thus\nreports“approx-bleu”bydefault,whichiscomputedontheinternalsubwords(never\nexposed to the user, actually) instead of words (according to BLEU tokenization).\nAs a result, “approx-bleu” is usually about 1.2–1.8 times higher than the real BLEU.\nDue to its dependence on the training data (for the subword vocabulary), it is not\neasily reproducible in varying experiments and thus not suitable for reporting in\npublications."}
{"chunk_id": "d00p0005c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 5, "text": "As a result, “approx-bleu” is usually about 1.2–1.8 times higher than the real BLEU.\nDue to its dependence on the training data (for the subword vocabulary), it is not\neasily reproducible in varying experiments and thus not suitable for reporting in\npublications.\n⁸ Such deﬁnition of Time Till Score leads to a high variance of its values because of the relatively high\nBLEU variance between subsequent checkpoints (visible as a “ﬂickering” of the learning curves in the\nﬁgures). To decrease the variation one can use a bigger development test set.\n5"}
{"chunk_id": "d00p0006c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 6, "text": "sentences EN words CS words\nCzEng 1.7 57 M 618 M 543 M\neuroparl-v7 647 k 15 M 13 M\nnews-commentary-v11 190 k 4.1 M 3.7 M\ncommoncrawl 161 k 3.3 M 2.9 M\nTotal 58 M 640 M 563 M\nTable 1: Training data resources\nWeimplementedahelperscript t2t-bleu whichcomputesthe“real”BLEU(giving\nthe same result as sacréBLEU with--tokenization intl). Our script can be used in\ntwo ways:\n• To evaluate one translated ﬁle:\nt2t-bleu --translation=my-wmt13.de --reference=wmt13_deen.de\n• To evaluate all translations in a given directory (created e.g. byt2t-translate-\nall) and store the results in a TensorBoard events ﬁle. All the ﬁgures in this\narticle were created this way.\nWe also implementedt2t-translate-all andt2t-avg-all scripts, which translate"}
{"chunk_id": "d00p0006c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 6, "text": "• To evaluate all translations in a given directory (created e.g. byt2t-translate-\nall) and store the results in a TensorBoard events ﬁle. All the ﬁgures in this\narticle were created this way.\nWe also implementedt2t-translate-all andt2t-avg-all scripts, which translate\nall checkpoints in a given directory and average a window of N subsequent check-\npoints, respectively.⁹ For details on averaging see Section 4.10.\n3. Data Selection and Preprocessing\nWe focused on the English-to-Czech translation direction. Most of our training\ndata comes from the CzEng parallel treebank, version 1.7 (57M sentence pairs),/one.superior⁰\nand the rest (1M sentence pairs) comes from three smaller sources (Europarl, News\nCommentary, Common Crawl) as detailed in Table 1."}
{"chunk_id": "d00p0006c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 6, "text": "data comes from the CzEng parallel treebank, version 1.7 (57M sentence pairs),/one.superior⁰\nand the rest (1M sentence pairs) comes from three smaller sources (Europarl, News\nCommentary, Common Crawl) as detailed in Table 1.\nWe use this dataset of 58M sentence pairs for most our experiments. In some\nexperiments (in Sections 4.2 and 4.6), we substitute CzEng 1.7 with an older and\nconsiderably smaller CzEng 1.0 (Bojar et al., 2012) containing 15M sentence pairs\n(233M/206M of en/cs words).\nTo plot the performance throughout the training, we use WMT newstest2013 as\na development set (not overlapping with the training data). In Section 5, we apply\nour best model (judged from the performance on the development set) to the WMT\nnewstest2017, for comparison with the state-of-the-art systems."}
{"chunk_id": "d00p0006c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 6, "text": "a development set (not overlapping with the training data). In Section 5, we apply\nour best model (judged from the performance on the development set) to the WMT\nnewstest2017, for comparison with the state-of-the-art systems.\n⁹ All three scripts are now merged in the T2T master. All three scripts can be used while the training is\nstill in progress, i.e. they wait a given number of minutes for new checkpoints to appear.\n/one.superior⁰http://ufal.mff.cuni.cz/czeng/czeng17, which is a subset of CzEng 1.6 (Bojar et al., 2016).\n6"}
{"chunk_id": "d00p0007c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "3.1. Training Data Preprocessing\nData preprocessing such as tokenization and truecasing has always been a very\nimportant part of the setup of statistical machine translation systems. A huge leap in\nscaling NMT to realistic data size has been achieved by the introduction of subword\nunits(Sennrichetal.,2016),butthelong-termvisionofthedeep-learningcommunity\nis to leave all these “technicalities” up to the trained neural network and feed it with\nas original input as possible (see e.g. Lee et al., 2016).\nT2T adopts this vision and while it supports the use of external subword units, it\ncomes with its own built-in method similar to the word-piece algorithm by Wu et al.\n(2016)anddoesnotexpecttheinputtobeeventokenized. Basedonasmallsampleof"}
{"chunk_id": "d00p0007c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "as original input as possible (see e.g. Lee et al., 2016).\nT2T adopts this vision and while it supports the use of external subword units, it\ncomes with its own built-in method similar to the word-piece algorithm by Wu et al.\n(2016)anddoesnotexpecttheinputtobeeventokenized. Basedonasmallsampleof\nthe training data, T2T will train a subword vocabulary and apply it to all the training\nand later evaluation data.\nWe follow the T2T default and provide raw plain text training sentences. We\nuse the default parameters: shared source and target (English and Czech) subword\nvocabulary of size 32k./one.superior/one.superiorAfter this preprocessing, the total number of subwords in\nour main training data is 992 millions (taking the maximum of English and Czech"}
{"chunk_id": "d00p0007c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "use the default parameters: shared source and target (English and Czech) subword\nvocabulary of size 32k./one.superior/one.superiorAfter this preprocessing, the total number of subwords in\nour main training data is 992 millions (taking the maximum of English and Czech\nlengths for each sentence pair, as needed for computing the number of epochs, see\nSection 2.3). The smaller dataset CzEng 1.0 has 327 million subwords. In both cases\nthe average number of subwords per (space-delimited) word is about 1.5.\nEvenwhenfollowingthedefaults,therearesomeimportantdetailsthatshouldbe\nconsidered. We thus provide our ﬁrst set of technical tips here:\nTips on Training Data Preprocessing\n• Makesurethatthesubwordvocabularyistrainedonasuﬃcientlylargesample\nof the training data./one.superior/two.superior"}
{"chunk_id": "d00p0007c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "Evenwhenfollowingthedefaults,therearesomeimportantdetailsthatshouldbe\nconsidered. We thus provide our ﬁrst set of technical tips here:\nTips on Training Data Preprocessing\n• Makesurethatthesubwordvocabularyistrainedonasuﬃcientlylargesample\nof the training data./one.superior/two.superior\n• AsdiscussedinSection4.5,ahigherbatchsizemaybebeneﬁcialforthetraining\nand the batch size can be higher when excluding training sentences longer\nthan a given threshold. This can be controlled with parametermax_length (see\nSection4.4),butitmaybeagoodideatoexcludetoolongsentencesevenbefore\npreparingthetrainingdatausing t2t-datagen. ThiswaytheTFRecordstraining\nﬁles will be smaller and their processing a bit faster./one.superior/three.superior"}
{"chunk_id": "d00p0007c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "than a given threshold. This can be controlled with parametermax_length (see\nSection4.4),butitmaybeagoodideatoexcludetoolongsentencesevenbefore\npreparingthetrainingdatausing t2t-datagen. ThiswaytheTFRecordstraining\nﬁles will be smaller and their processing a bit faster./one.superior/three.superior\n/one.superior/one.superiorMoredetailsonT2TwithBPEsubwordunitsbySennrichetal.(2016)vs. theinternalimplementation\ncanbefoundinthetechnicalreport“MorphologicalandLanguage-AgnosticWordSegmentationforNMT”\nattached to the Deliverable 2.3 of the project QT21:http://www.qt21.eu/resources/.\n/one.superior/two.superiorThis is controlled by afile_byte_budget constant, which must be changed directly in the source code"}
{"chunk_id": "d00p0007c06", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 7, "text": "canbefoundinthetechnicalreport“MorphologicalandLanguage-AgnosticWordSegmentationforNMT”\nattached to the Deliverable 2.3 of the project QT21:http://www.qt21.eu/resources/.\n/one.superior/two.superiorThis is controlled by afile_byte_budget constant, which must be changed directly in the source code\ninT2Tv1.2.9. Asignoftoosmalltrainingdataforthesubwordvocabularyisthatthe min_count asreported\nin the logs is too low, so the vocabulary is estimated from words seen only once or twice.\n/one.superior/three.superiorWe did no such pre-ﬁltering in our experiments.\n7"}
{"chunk_id": "d00p0008c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "4. Experiments\nIn this section, we present several experiments, always summarizing the obser-\nvations and giving some generally applicable tips that we learned. All experiments\nwere done with T2T v1.2.9 unless stated otherwise.\nWe experiment with two sets of hyper-parameters pre-deﬁned in T2T:trans-\nformer_big_single_gpu (BIG) andtransformer_base_single_gpu (BASE), which diﬀer\nmainly in the size of the model. Note thattransformer_big_single_gpu and trans-\nformer_base_single_gpu are just names of a set of hyper-parameters, which can be\napplied even when training on multiple GPUs, as we do in our experiments, see\nSection 4.7./one.superior⁴\nOur baseline setting uses the BIG model with its default hyper-parameters except\nfor:"}
{"chunk_id": "d00p0008c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "former_base_single_gpu are just names of a set of hyper-parameters, which can be\napplied even when training on multiple GPUs, as we do in our experiments, see\nSection 4.7./one.superior⁴\nOur baseline setting uses the BIG model with its default hyper-parameters except\nfor:\n• batch_size=1500 (see the discussion of diﬀerent sizes in Section 4.5),\n• --train_steps=6000000, i.e. high enough, so we can stop each experiment man-\nually as needed,\n• --save_checkpoints_secs=3600 which forces checkpoint saving each hour (see\nSection 4.10),\n• --schedule=train which disables the internal evaluation withapprox_bleu and\nthus makes training a bit faster (see Section 2)./one.superior⁵\n4.1. Computation Speed and Training Throughput"}
{"chunk_id": "d00p0008c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "ually as needed,\n• --save_checkpoints_secs=3600 which forces checkpoint saving each hour (see\nSection 4.10),\n• --schedule=train which disables the internal evaluation withapprox_bleu and\nthus makes training a bit faster (see Section 2)./one.superior⁵\n4.1. Computation Speed and Training Throughput\nWe are primarily interested in the translation quality (BLEU learning curves and\nTimeTillScore)andwediscussitinthefollowingsections4.2–4.10. Inthissection,we\nfocushoweveronlyonthe computationspeed andtrainingthroughput. Bothareaﬀected\nby three important factors: batch size, number of used GPUs and model size. The\nspeed is usually almost constant for a given experiment./one.superior⁶\nTable 2 shows the computation speed and training throughput for a single GPU"}
{"chunk_id": "d00p0008c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "by three important factors: batch size, number of used GPUs and model size. The\nspeed is usually almost constant for a given experiment./one.superior⁶\nTable 2 shows the computation speed and training throughput for a single GPU\nand various batch sizes and model sizes (BASE and BIG). The BASE model allows\nfor using a higher batch size than the BIG model. The cells where the BIG model\nresulted in out-of-memory errors are marked with “OOM”./one.superior⁷We can see that the\n/one.superior⁴According to our experiments (not reported here),transformer_big_single_gpu is better thantrans-\nformer_big even when training on 8 GPUs, although the naming suggests that the T2T authors had an\nopposite experience."}
{"chunk_id": "d00p0008c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "/one.superior⁴According to our experiments (not reported here),transformer_big_single_gpu is better thantrans-\nformer_big even when training on 8 GPUs, although the naming suggests that the T2T authors had an\nopposite experience.\n/one.superior⁵Alsotherearesomeproblemswiththealternativeschedules train_and_evaluate (itneedsmoremem-\nory) andcontinuous_train_and_eval (see https://github.com/tensorflow/tensor2tensor/issues/556).\n/one.superior⁶TensorBoard showsglobal_step/sec statistics, i.e. the computation speed curve. These curves in our\nexperimentsarealmostconstantforthewholetrainingwithvariationwithin2%,exceptformomentswhen\na checkpoint is being saved (and the computation speed is thus much slower)."}
{"chunk_id": "d00p0008c06", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 8, "text": "/one.superior⁶TensorBoard showsglobal_step/sec statistics, i.e. the computation speed curve. These curves in our\nexperimentsarealmostconstantforthewholetrainingwithvariationwithin2%,exceptformomentswhen\na checkpoint is being saved (and the computation speed is thus much slower).\n/one.superior⁷For these experiments, we usedmax_length=50 in order to be able to test bigger batch sizes. However,\nin additional experiments we checked thatmax_length does not aﬀect the training throughput itself.\n8"}
{"chunk_id": "d00p0009c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 9, "text": "model\nbatch_size BASE BIG\n500 43.4k 23.6k\n1000 30.2k 13.5k\n1500 22.3k 9.8k\n2000 16.8k 7.5k\n2500 14.4k 6.5k\n3000 12.3k OOM\n4500 8.2k OOM\n6000 6.6k OOM\n(a) Computation speed (steps/hour)\nmodel\nbatch_size BASE BIG\n500 21.7M 11.9M\n1000 30.2M 13.5M\n1500 33.4M 14.7M\n2000 33.7M 15.0M\n2500 36.0M 16.2M\n3000 37.0M OOM\n4500 36.7M OOM\n6000 39.4M OOM\n(b) Training throughput (subwords/hour)\nTable 2: Computation speed and training throughput for a single GPU.\ncomputation speed decreases with increasing batch size because not all operations\nin GPU are fully batch-parallelizable. The training throughput grows sub-linearly\nwith increasing batch size, so based on these experiments only, there is just a small\nadvantage when setting the batch size to the maximum value. We will return to this"}
{"chunk_id": "d00p0009c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 9, "text": "in GPU are fully batch-parallelizable. The training throughput grows sub-linearly\nwith increasing batch size, so based on these experiments only, there is just a small\nadvantage when setting the batch size to the maximum value. We will return to this\nquestion in Section 4.5, while taking into account the translation quality.\nWe can also see the BASE model has approximately two times bigger throughput\nas well as computation speed relative to the BIG model.\nGPUs steps/hour subwords/hour\n1 9.8k 14.7M\n2 7.4k 22.2M\n6 5.4k 48.6M\n8 5.6k 67.2M\nTable 3: Computation speed and training throughput for various numbers of GPUs,\nwith the BIG model andbatch_size=1500.\nTable 3 uses the BIG model andbatch_size=1500, while varying the number of"}
{"chunk_id": "d00p0009c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 9, "text": "GPUs steps/hour subwords/hour\n1 9.8k 14.7M\n2 7.4k 22.2M\n6 5.4k 48.6M\n8 5.6k 67.2M\nTable 3: Computation speed and training throughput for various numbers of GPUs,\nwith the BIG model andbatch_size=1500.\nTable 3 uses the BIG model andbatch_size=1500, while varying the number of\nGPUs. The overhead in GPU synchronization is apparent from the decreasing com-\nputation speed. Nevertheless, the training throughput still grows with more GPUs,\nso e.g. with 6 GPUs we process 3.2 times more training data per hour relative to\na single GPU (while without any overhead we would hypothetically expect 6 times\nmore data).\n9"}
{"chunk_id": "d00p0010c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 10, "text": "24\n25\n26\n27\n0 50 100 150 200 250\n1 2 3 4 5 6 7 8 9 10 11\nBLEU\nTraining time (hours)\nTraining time (days)\n58M training sentences\n16M training sentences\nFigure 1: Training data size eﬀect. BLEU learning curves for our main training\ndataset with 58 million sentence pairs and an alternative training dataset with 16\nmillion sentence pairs. Both trained with 8 GPUs, BIG model andbatch_size=1500.\nThe overhead when scaling to multiple GPUs is smaller than the overhead when\nscaling to a higher batch size. Scaling from a single GPU to 6 GPUs increases the\nthroughput 3.2 times, but scaling from batch size 1000 to 6000 on a single GPU\nincreases the throughput 1.3 times.\n4.2. Training Data Size\nForthisexperiment,wesubstitutedCzEng1.7withCzEng1.0inthetrainingdata,"}
{"chunk_id": "d00p0010c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 10, "text": "scaling to a higher batch size. Scaling from a single GPU to 6 GPUs increases the\nthroughput 3.2 times, but scaling from batch size 1000 to 6000 on a single GPU\nincreases the throughput 1.3 times.\n4.2. Training Data Size\nForthisexperiment,wesubstitutedCzEng1.7withCzEng1.0inthetrainingdata,\nso the total training size is 16 million sentence pairs (255M / 226M of English/Czech\nwords). Figure1comparestheBLEUlearningcurvesoftwoexperimentswhichdiﬀer\nonly in the training data: the baseline CzEng 1.7 versus the smaller CzEng 1.0. Both\nare trained on the same hardware with the same hyper-parameters (8 GPUs, BIG,\nbatch_size=1500). Training on the smaller dataset (2.5 times smaller in the number\nof words) converges to BLEU of about 25.5 after two days of training and does not"}
{"chunk_id": "d00p0010c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 10, "text": "are trained on the same hardware with the same hyper-parameters (8 GPUs, BIG,\nbatch_size=1500). Training on the smaller dataset (2.5 times smaller in the number\nof words) converges to BLEU of about 25.5 after two days of training and does not\nimprove over the next week of training. Training on the bigger dataset gives slightly\nworse results in the ﬁrst eight hours of training (not shown in the graph) but clearly\nbetter results after two days of training, reaching over 26.5 BLEU after eight days./one.superior⁸\nWithbatch_size=1500 and8GPUs,trainingoneepochofthesmallerdataset(with\nCzEng 1.0) takes 27k steps (5 hours of training), compared to 83k steps (15 hours) for\nthe bigger dataset (with CzEng 1.7). This meansabout 10 epochs in the smaller dataset"}
{"chunk_id": "d00p0010c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 10, "text": "Withbatch_size=1500 and8GPUs,trainingoneepochofthesmallerdataset(with\nCzEng 1.0) takes 27k steps (5 hours of training), compared to 83k steps (15 hours) for\nthe bigger dataset (with CzEng 1.7). This meansabout 10 epochs in the smaller dataset\nwere needed for reaching the convergenceand this is also the moment when the bigger\n/one.superior⁸We compared the two datasets also in another experiment with two GPUs, where CzEng 1.7 gave\nslightly worse results than CzEng 1.0 during the ﬁrst two days of training but clearly better results after\neight days. We hypothesize CzEng 1.0 is somewhat cleaner than CzEng 1.7.\n10"}
{"chunk_id": "d00p0011c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 11, "text": "dataset startsbeing clearlybetter. However,even 18epochs inthe bigger datasetwere not\nenough to reach the convergence. enough to reach the convergence\nTips on Training Data Size\n• For comparing diﬀerent datasets (e.g. smaller and cleaner vs. bigger and\nnoisier), we need to train long enough becauseresults after ﬁrst hours (or days if\ntraining on a single GPU) may be misleading.\n• For large training data (as CzEng 1.7 which has over half a gigaword),BLEU\nimproves even after one week of training on eight GPUs(or after 20 days of training\non two GPUs in another experiment).\n• We cannot easily interpolate one dataset results to another dataset.While the smaller\ntraining data (with CzEng 1.0) converged after 2 days, the main training data"}
{"chunk_id": "d00p0011c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 11, "text": "improves even after one week of training on eight GPUs(or after 20 days of training\non two GPUs in another experiment).\n• We cannot easily interpolate one dataset results to another dataset.While the smaller\ntraining data (with CzEng 1.0) converged after 2 days, the main training data\n(with CzEng 1.7), which is 2.5 times bigger, continues improving even after\n2.5×2 days./one.superior⁹\n4.3. Model Size\nChoosing the right model size is important for practical reasons: larger models\nmaynotﬁtanymoreonyourGPUortheymayrequiretouseaverysmallbatchsize.\nWe experiment with two models,/two.superior⁰as pre-deﬁned in Tensor2Tensor –transfor-\nmer_big_single_gpu (BIG) andtransformer_base_single_gpu (BASE), which diﬀer in\nfour hyper-parameters summarized in Table 4."}
{"chunk_id": "d00p0011c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 11, "text": "maynotﬁtanymoreonyourGPUortheymayrequiretouseaverysmallbatchsize.\nWe experiment with two models,/two.superior⁰as pre-deﬁned in Tensor2Tensor –transfor-\nmer_big_single_gpu (BIG) andtransformer_base_single_gpu (BASE), which diﬀer in\nfour hyper-parameters summarized in Table 4.\nmodel hidden_size ﬁlter_size num_heads adam_beta2\nBASE 512 2048 8 0.980\nBIG 1024 4096 16 0.998\nTable4: transformer_big_single_gpu (BIG)and transformer_base_single_gpu (BASE)\nhyper-parameter diﬀerences.\nFigure2showsthatonasingleGPU,theBIGmodelbecomesclearlybetterthanthe\nBASEmodelafter4hoursoftrainingifwekeepthebatchsizethesame–2000(andwe\nhave conﬁrmed it with 1500 in other experiments). However, the BASE model takes\nlessmemory,sowecanaﬀordahigherbatchsize,inourcase4500(withno max_length"}
{"chunk_id": "d00p0011c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 11, "text": "Figure2showsthatonasingleGPU,theBIGmodelbecomesclearlybetterthanthe\nBASEmodelafter4hoursoftrainingifwekeepthebatchsizethesame–2000(andwe\nhave conﬁrmed it with 1500 in other experiments). However, the BASE model takes\nlessmemory,sowecanaﬀordahigherbatchsize,inourcase4500(withno max_length\nrestriction,seethenextsection),whichimprovestheBLEU(seeSection4.5). Buteven\n/one.superior⁹Althoughsuchanexpectationmayseemnaïve,wecanﬁnditinliterature. Forexample,Bottou(2012)\ninSection4.2writes: “ Expectthevalidationperformancetoplateauafteranumberofepochsroughlycomparableto\nthe number of epochs needed to reach this point on the small training set.”\n/two.superior⁰We tried also a model three times as large as BASE (1.5 times as large as BIG), but it did not reach"}
{"chunk_id": "d00p0011c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 11, "text": "inSection4.2writes: “ Expectthevalidationperformancetoplateauafteranumberofepochsroughlycomparableto\nthe number of epochs needed to reach this point on the small training set.”\n/two.superior⁰We tried also a model three times as large as BASE (1.5 times as large as BIG), but it did not reach\nbetter results than BIG, so we don’t report it here.\n11"}
{"chunk_id": "d00p0012c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 12, "text": "16\n18\n20\n22\n24\n0 10 20 30 40 50 60 70\nBLEU\nTraining time (hours)\nBIG model, batch size 2000, 1 GPU\nBASE model, batch size 4500, 1 GPU\nBASE model, batch size 2000, 1 GPU\nFigure 2: Eﬀect of model size and batch size on a single GPU.\n22\n23\n24\n25\n26\n0 10 20 30 40 50\nBLEU\nTraining time (hours)\nBIG model, batch size 1500, 8 GPUs\nBASE model, batch size 4500, 8 GPUs\nFigure 3: Eﬀect of model size and batch size on 8 GPUs.\nso, after less than one day of training, BIG with batch size 2000 becomes better than\nBASE with batch size 4500 (or even 6000 withmax_length=70 in another experiment)\nand the diﬀerence grows up to 1.8 BLEU after three days of training.\nFigure3conﬁrmsthiswith8GPUs–hereBIGwithbatchsize1500becomesclearly\nbetter than BASE with batch size 4500 after 18 hours of training."}
{"chunk_id": "d00p0012c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 12, "text": "BASE with batch size 4500 (or even 6000 withmax_length=70 in another experiment)\nand the diﬀerence grows up to 1.8 BLEU after three days of training.\nFigure3conﬁrmsthiswith8GPUs–hereBIGwithbatchsize1500becomesclearly\nbetter than BASE with batch size 4500 after 18 hours of training.\nTips on Model Size\n• Prefer the BIG over the BASE modelif you plan to train longer than one day and\nhave 11 GB (or more) memory available on GPU.\n• With less memory you should benchmark BIG and BASE with the maximum\npossible batch size.\n12"}
{"chunk_id": "d00p0013c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 13, "text": "maximum batch size longer sentences\nmax_length BIG+Adam BIG+Adafactor BASE+Adam train test\nnone 2040 2550 4950 0.0% 0.0%\n150 2230 2970 5430 0.2% 0.0%\n100 2390 3280 5990 0.7% 0.3%\n70 2630 3590 6290 2.1% 2.2%\n50 2750 3770 6430 5.0% 9.1%\nTable5: Maximumbatchsizewhichﬁtsinto11GBmemoryforvariouscombinations\nof max_length (maximum sentence length in subwords), model size (base or big)\nand optimizer (Adam or Adafactor). The last two columns show the percentage of\nsentences in the train (CzEng 1.7) and test (wmt13) data that are longer than a given\nthreshold.\n• For fast debugging (of model-size-unrelated aspects) use a model calledtrans-\nformer_tiny.\n4.4. Maximum Training Sentence Length\nTheparameter max_length speciﬁesthemaximumlengthofasentenceinsubwords."}
{"chunk_id": "d00p0013c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 13, "text": "sentences in the train (CzEng 1.7) and test (wmt13) data that are longer than a given\nthreshold.\n• For fast debugging (of model-size-unrelated aspects) use a model calledtrans-\nformer_tiny.\n4.4. Maximum Training Sentence Length\nTheparameter max_length speciﬁesthemaximumlengthofasentenceinsubwords.\nLonger sentences (either in source or target language) are excluded from the training\ncompletely. If nomax_length is speciﬁed (which is the default),batch_size is used\ninstead. Loweringthemax_length allowstouseahigherbatchsizeorabiggermodel.\nSince the Transformer implementation in T2T can suddenly run out of memory even\nafter several hours of training, it is good to know how large batch size ﬁts in your\nGPU. Table 5 presents what we empirically measured for the BASE and BIG models"}
{"chunk_id": "d00p0013c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 13, "text": "Since the Transformer implementation in T2T can suddenly run out of memory even\nafter several hours of training, it is good to know how large batch size ﬁts in your\nGPU. Table 5 presents what we empirically measured for the BASE and BIG models\nwith Adam and Adafactor/two.superior/one.superioroptimizers and variousmax_length values.\nSettingmax_length too low would result in excluding too many training sentences\nand biasing the translation towards shorter sentences, which would hurt the trans-\nlation quality. The last two columns in Table 5 show that settingmax_length to 70\n(resp. 100) results in excluding only 2.1% (resp. 0.7%) of sentences in the training\ndata, and only 2.2% (resp. 0.3%) sentences in the development test data are longer,"}
{"chunk_id": "d00p0013c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 13, "text": "lation quality. The last two columns in Table 5 show that settingmax_length to 70\n(resp. 100) results in excluding only 2.1% (resp. 0.7%) of sentences in the training\ndata, and only 2.2% (resp. 0.3%) sentences in the development test data are longer,\nso the detrimental eﬀect of smaller training data and length bias should be minimal\nin this setting. However, our experiments withbatch_size=1500 in Figure 4 show a\nstrange drop in BLEU after one hour of training for all experiments withmax_length\n70 or lower. Even withmax_length 150 or 200 the BLEU learning curve is worse than\nwithmax_length=400,whichﬁnallygivesthesameresultasnotusingany max_length\n/two.superior/one.superiorTheAdafactoroptimizer(ShazeerandStern,2018)isavailableonlyinT2T1.4.2ornewerandhasthree"}
{"chunk_id": "d00p0013c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 13, "text": "70 or lower. Even withmax_length 150 or 200 the BLEU learning curve is worse than\nwithmax_length=400,whichﬁnallygivesthesameresultasnotusingany max_length\n/two.superior/one.superiorTheAdafactoroptimizer(ShazeerandStern,2018)isavailableonlyinT2T1.4.2ornewerandhasthree\ntimes smaller models than Adam because it does not store ﬁrst and second moments for all weights. We\nleave further experiments with Adafactor for future work.\n13"}
{"chunk_id": "d00p0014c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 14, "text": "0\n5\n10\n15\n0 1 2 3 4 5 6 7 8 9\nBLEU\nTraining time (hours)\nmax length 400\nmax length 200\nmax length 150\nmax length 70\nmax length 50\nmax length 25\nFigure 4: Eﬀect of restricting the training data to variousmax_length values. All\ntrained on a single GPU with the BIG model andbatch_size=1500. An experiment\nwithout anymax_length is not shown, but it has the same curve asmax_length=400.\nrestriction. The training loss ofmax_length=25 (and 50 and 70) has high variance and\nstops improving after the ﬁrst hour of training but shows no sudden increase (as in\nthe case of diverged training discussed in Section 4.6 when the learning rate is too\nhigh). We have no explanation for this phenomenon./two.superior/two.superior\nWe did another set of experiments with varyingmax_length, but this time with"}
{"chunk_id": "d00p0014c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 14, "text": "the case of diverged training discussed in Section 4.6 when the learning rate is too\nhigh). We have no explanation for this phenomenon./two.superior/two.superior\nWe did another set of experiments with varyingmax_length, but this time with\nbatch_size=2000 instead of 1500. In this case,max_length 25 and 50 still results in\nslowergrowingBLEUcurves,but70andhigherhasthesamecurveasno max_length\nrestriction. So in our case,if the batch size is high enough, themax_length has almost no\neﬀect on BLEU, but this should be checked for each new dataset.\nWe trained several models with variousmax_length for three days and observed\nthattheyarenotabletoproducelongertranslationsthanwhatwasthemaximumlengthused\nin training, even if we change the decoding parameter alpha.\nTips onmax_length"}
{"chunk_id": "d00p0014c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 14, "text": "eﬀect on BLEU, but this should be checked for each new dataset.\nWe trained several models with variousmax_length for three days and observed\nthattheyarenotabletoproducelongertranslationsthanwhatwasthemaximumlengthused\nin training, even if we change the decoding parameter alpha.\nTips onmax_length\n• Set (a reasonably low)max_length. This allows to use a higher batch size and\nprevents out-of-memory errors after several hours of training. Also, with a\nhigher percentage of training sentences that are almostmax_length long, there\nisahigherchancethatthetrainingwillfaileitherimmediately(ifthebatchsize\nis too high) or never (otherwise).,\n• Set a reasonably highmax_length. Consider the percentage of sentences excluded\nfromtrainingandfromthetargeteddevelopmenttestsetandalsowatchforun-"}
{"chunk_id": "d00p0014c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 14, "text": "isahigherchancethatthetrainingwillfaileitherimmediately(ifthebatchsize\nis too high) or never (otherwise).,\n• Set a reasonably highmax_length. Consider the percentage of sentences excluded\nfromtrainingandfromthetargeteddevelopmenttestsetandalsowatchforun-\nexpecteddrops(orstagnations)oftheBLEUcurveintheﬁrsthoursoftraining.\n/two.superior/two.superiorhttps://github.com/tensorflow/tensor2tensor/issues/582\n14"}
{"chunk_id": "d00p0015c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 15, "text": "10\n12\n14\n16\n18\n20\n22\n0 10 20 30 40 50 60 70\nBLEU\nTraining time (hours)\nBASE, batch size 6000\nBASE, batch size 4500\nBASE, batch size 3000\nBASE, batch size 1500\nBASE, batch size 1000\nFigure 5: Eﬀect of the batch size with the BASE model. All trained on a single GPU.\n4.5. Batch Size\nThedefault batch_size valueinrecentT2Tversionsis4096subwordsforallmodels\nexcept for transformer_base_single_gpu, where the default is 2048. However, we\nrecommend to always set the batch size explicitly/two.superior/three.superioror at least make a note what was\nthe default in a given T2T version when reporting experimental results.\nFigure5showslearningcurvesforﬁvediﬀerentbatchsizes(1000,1500,3000,4500\nand 6000) for experiments with a single GPU and the BASE model./two.superior⁴A higher batch"}
{"chunk_id": "d00p0015c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 15, "text": "the default in a given T2T version when reporting experimental results.\nFigure5showslearningcurvesforﬁvediﬀerentbatchsizes(1000,1500,3000,4500\nand 6000) for experiments with a single GPU and the BASE model./two.superior⁴A higher batch\nsize up to 4500is clearly better in terms of BLEU as measured by Time Till Score and\nExamplesTillScoremetricsdeﬁnedinSection4.1. Forexample,togetoverBLEUof18\nwithbatch_size=3000,weneed7hours(260Mexamples),andwith batch_size=1500,\nwe need about 3 days (2260M examples) i.e. 10 times longer (9 time more examples).\nFromTable2aweknowthatbiggerbatcheshaveslowercomputationspeed,sowhen\nre-plotting Figure 5 with steps instead of time on the x-axis, the diﬀerence between\nthe curves would be even bigger. From Table 2b we know that bigger batches have"}
{"chunk_id": "d00p0015c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 15, "text": "FromTable2aweknowthatbiggerbatcheshaveslowercomputationspeed,sowhen\nre-plotting Figure 5 with steps instead of time on the x-axis, the diﬀerence between\nthe curves would be even bigger. From Table 2b we know that bigger batches have\nslightly higher training throughput, so when re-plotting with number of examples\nprocessed on the x-axis, the diﬀerence will be smaller, but still visible. The only\nexceptionisthediﬀerencebetweenbatchsize4500and6000, whichisverysmalland\n/two.superior/three.superiore.g. --hparams=\"batch_size=1500,learning_rate=0.20,learning_rate_warmup_steps=16000\"\nAs the batch size is speciﬁed in subwords, we see no advantage in using power-of-two values.\n/two.superior⁴All the experiments in Figure 5 usemax_length=70, but we have got the same curves when re-running"}
{"chunk_id": "d00p0015c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 15, "text": "As the batch size is speciﬁed in subwords, we see no advantage in using power-of-two values.\n/two.superior⁴All the experiments in Figure 5 usemax_length=70, but we have got the same curves when re-running\nwithout anymax_length restrictions, except forbatch_size=6000 which failed with OOM.\n15"}
{"chunk_id": "d00p0016c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 16, "text": "0\n5\n10\n15\n20\n0 5 10 15 20 25 30 35\nBLEU\nTraining time (hours)\nBIG, batch size 2000\nBIG, batch size 1500\nBIG, batch size 1450\nBIG, batch size 1400\nBIG, batch size 1300\nBIG, batch size 1000\nFigure 6: Eﬀect of the batch size with the BIG model. All trained on a single GPU.\ncanbefullyexplainedbythefactthatbatchsize6000has7%higherthroughputthan\nbatch size 4500.\nSo for the BASE model, a higher batch size gives better results, although with dimin-\nishing returns. This observation goes against the common knowledge in other NMT\nframeworks and deep learning in general (Keskar et al., 2017) that smaller batches\nproceedslower(trainingexamplesperhour)butresultinbettergeneralization(higher\ntest-set BLEU) in the end. In our experiments with the BASE model in T2T, bigger"}
{"chunk_id": "d00p0016c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 16, "text": "frameworks and deep learning in general (Keskar et al., 2017) that smaller batches\nproceedslower(trainingexamplesperhour)butresultinbettergeneralization(higher\ntest-set BLEU) in the end. In our experiments with the BASE model in T2T, bigger\nbatches are not only faster in training throughput (as could be expected), but also\nfaster in convergence speed, Time Till Score and Examples Till Score.\nInterestingly, when replicating these experimentswith the BIG model, we see quite\ndiﬀerentresults,asshowninFigure6. TheBIGmodelneedsacertainminimalbatchsize\ntostartconvergingatall,butforhigherbatchsizesthereisalmostnodiﬀerenceinthe\nBLEUcurves(butstill,biggerbatchnevermakestheBLEUworseinourexperiments).\nIn our case, the sharp diﬀerence is between batch size 1450, which trains well, and"}
{"chunk_id": "d00p0016c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 16, "text": "tostartconvergingatall,butforhigherbatchsizesthereisalmostnodiﬀerenceinthe\nBLEUcurves(butstill,biggerbatchnevermakestheBLEUworseinourexperiments).\nIn our case, the sharp diﬀerence is between batch size 1450, which trains well, and\n1400, which drops oﬀ after two hours of training, recovering only slowly.\nAccording to Smith and Le (2017) and Smith et al. (2017), thegradient noise scale,\ni.e. scaleofrandomﬂuctuationsintheSGD(orAdametc.) dynamics,isproportional\nto learning rate divided by the batch size (cf. Section 4.8). Thus when lowering the\nbatchsize,weincreasethenoisescaleandthetrainingmay diverge. Thismaybeeither\npermanent, as in the case of batch size 1000 in Figure 6, or temporary, as in the case\nof batch size 1300 and 1400, where the BLEU continues to grow after the temporary"}
{"chunk_id": "d00p0016c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 16, "text": "batchsize,weincreasethenoisescaleandthetrainingmay diverge. Thismaybeeither\npermanent, as in the case of batch size 1000 in Figure 6, or temporary, as in the case\nof batch size 1300 and 1400, where the BLEU continues to grow after the temporary\ndrop, but much more slowly than the non-diverged curves.\nWe are not sure what causes the diﬀerence between the BASE and BIG models\nwith regards to the sensitivity to batch size. One hypothesis is that the BIG model is\n16"}
{"chunk_id": "d00p0017c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 17, "text": "0\n5\n10\n15\n20\n0 5 10 15 20 25 30 35 40 45\nBLEU\nTraining time (hours)\nlearning rate 0.25\nlearning rate 0.20\nlearning rate 0.10\nlearning rate 0.05\nlearning rate 0.01\nFigure 7: Eﬀect of the learning rate on a single GPU. All trained on CzEng 1.0 with\nthe default batch size (1500) and warmup steps (16k).\nmore diﬃcult to initialize and thus more sensitive to divergence in the early training\nphase. Also while for BASE, increasing the batch size was highly helpful until 4500,\nfor BIG this limit may be below 1450, i.e. below the minimal batch size needed for\npreventing diverged training.\nTip on Batch Size\n• Batch size should be set as high as possiblewhile keeping a reserve for not hitting\nthe out-of-memory errors. It is advisable to establish the largest possible batch"}
{"chunk_id": "d00p0017c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 17, "text": "for BIG this limit may be below 1450, i.e. below the minimal batch size needed for\npreventing diverged training.\nTip on Batch Size\n• Batch size should be set as high as possiblewhile keeping a reserve for not hitting\nthe out-of-memory errors. It is advisable to establish the largest possible batch\nsize before starting the main and long training.\n4.6. Learning Rate and Warmup Steps on a Single GPU\nThe default learning rate in T2T translation models is 0.20. Figure 7 shows that\nvarying the value within range 0.05–0.25 makes almost no diﬀerence. Setting the\nlearningratetoolow(0.01)resultsinnotablyslowerconvergence. Settingthelearning\nrate too high (0.30, not shown in the ﬁgure) results indivergedtraining, which means"}
{"chunk_id": "d00p0017c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 17, "text": "varying the value within range 0.05–0.25 makes almost no diﬀerence. Setting the\nlearningratetoolow(0.01)resultsinnotablyslowerconvergence. Settingthelearning\nrate too high (0.30, not shown in the ﬁgure) results indivergedtraining, which means\nin this case that the learning curve starts growing as usual, but at one moment drops\ndown almost to zero and stays there forever.\nA common solution to prevent diverged training is to decrease thelearning_rate\nparameter or increaselearning_rate_warmup_steps or introduce gradient clipping.\nThe learning_rate_warmup_steps parameter conﬁgures alinear_warmup_rsqrt_decay\nschedule/two.superior⁵and it is set to 16 000 by default (for the BIG model), meaning that within\n/two.superior⁵The schedule was callednoam in T2T versions older than 1.4.4.\n17"}
{"chunk_id": "d00p0018c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 18, "text": "0\n5\n10\n15\n20\n0 5 10 15 20 25 30 35 40 45\nBLEU\nTraining time (hours)\nwarmup steps 12k\nwarmup steps 14k\nwarmup steps 16k\nwarmup steps 32k\nwarmup steps 48k\nFigure 8: Eﬀect of the warmup steps on a single GPU. All trained on CzEng 1.0 with\nthe default batch size (1500) and learning rate (0.20).\nthe ﬁrst 16k steps the learning rate grows linearly and then follows an inverse square\nroot decay (t−0.5, cf. Section 4.8.3). At 16k steps, the actual learning rate is thus the\nhighest.\nIf a divergence is to happen, it usually happens within the ﬁrst few hours of\ntraining, when the actual learning rate becomes the highest. Once we increased the\nwarmup steps from 16k to 32k, we were able to train with the learning rate of 0.30"}
{"chunk_id": "d00p0018c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 18, "text": "highest.\nIf a divergence is to happen, it usually happens within the ﬁrst few hours of\ntraining, when the actual learning rate becomes the highest. Once we increased the\nwarmup steps from 16k to 32k, we were able to train with the learning rate of 0.30\nand even 0.50 without any divergence. The learning curves looked similarly to the\nbaselineone(withdefaultvaluesof16kwarmupstepsandlearningrate0.20). When\ntryinglearningrate1.0,wehadtoincreasewarmupstepsto60k(with40kthetraining\ndivergedafteronehour)–thisresultedinaslowerconvergenceatﬁrst(about3BLEU\nlowerthanthebaselineafter8hoursoftraining),butafter3–4daysoftraininghaving\nthe same curve as the baseline.\nFigure 8 shows the eﬀect of diﬀerent warmup steps with a ﬁxed learning rate (the"}
{"chunk_id": "d00p0018c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 18, "text": "divergedafteronehour)–thisresultedinaslowerconvergenceatﬁrst(about3BLEU\nlowerthanthebaselineafter8hoursoftraining),butafter3–4daysoftraininghaving\nthe same curve as the baseline.\nFigure 8 shows the eﬀect of diﬀerent warmup steps with a ﬁxed learning rate (the\ndefault0.20). Settingwarmupstepstoolow(12k)resultsindivergedtraining. Setting\nthem too high (48k, green curve) results in a slightly slower convergence at ﬁrst, but\nmatching the baseline after a few hours of training.\nWe can conclude that for a single GPU and the BIG model, there is a relatively\nlargerangeoflearningrateandwarmupstepsvaluesthatachievetheoptimalresults.\nThe default valueslearning_rate=0.20 and learning_rate_warmup_steps=16000 are\nwithin this range.\nTips on Learning Rate and Warmup Steps"}
{"chunk_id": "d00p0018c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 18, "text": "We can conclude that for a single GPU and the BIG model, there is a relatively\nlargerangeoflearningrateandwarmupstepsvaluesthatachievetheoptimalresults.\nThe default valueslearning_rate=0.20 and learning_rate_warmup_steps=16000 are\nwithin this range.\nTips on Learning Rate and Warmup Steps\n• In case of diverged training, try gradient clipping and/or more warmup steps.\n18"}
{"chunk_id": "d00p0019c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 19, "text": "• Ifthatdoesnothelp(orifthewarmupstepsaretoohighrelativetotheexpected\ntotal training steps), try decreasing the learning rate.\n• Note that when you decrease warmup steps (and keep learning rate), you also\nincrease the maximum actual learning rate because of the way how thelin-\near_warmup_rsqrt_decay (akanoam) schedule is implemented./two.superior⁶\n4.7. Number of GPUs\nT2T allows to train with multiple GPUs on the same machine simply using the\nparameter --worker_gpus./two.superior⁷As explained in Section 2.3, the parameterbatch_size is\ninterpreted per GPU, so with 8 GPUs, theeﬀective batch sizeis 8 times bigger.\nAsingle-GPUexperimentwithbatchsize4000,shouldgiveexactlythesameresults\nas two GPUs and batch size 2000 and as four GPUs and batch size 1000 because the"}
{"chunk_id": "d00p0019c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 19, "text": "interpreted per GPU, so with 8 GPUs, theeﬀective batch sizeis 8 times bigger.\nAsingle-GPUexperimentwithbatchsize4000,shouldgiveexactlythesameresults\nas two GPUs and batch size 2000 and as four GPUs and batch size 1000 because the\neﬀective batch size is 4000 in all three cases. We have conﬁrmed this empirically. By\nthe “same results” we mean BLEU (or train loss) versus training steps on the x-axis.\nWhenconsideringtime,thefour-GPUexperimentwillbethefastestone,asexplained\nin Section 4.1.\nFigure 9 shows BLEU curves for diﬀerent numbers of GPUs and the BIG model\nwith batch size, learning rate and warmup steps ﬁxed on their default values (1500,\n0.20and16k,respectively). Ascouldbeexpected,trainingwithmoreGPUsconverges"}
{"chunk_id": "d00p0019c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 19, "text": "in Section 4.1.\nFigure 9 shows BLEU curves for diﬀerent numbers of GPUs and the BIG model\nwith batch size, learning rate and warmup steps ﬁxed on their default values (1500,\n0.20and16k,respectively). Ascouldbeexpected,trainingwithmoreGPUsconverges\nfaster. WhatisinterestingistheTimeTillScore. Table6liststheapproximatetraining\ntimeandnumberoftrainingexamples(inmillionsofsubwords)neededto“surpass”\n(i.e. achieve and never again fall below) BLEU of 25.6.\n# GPUs hours subwords (M)\n1 >600 >9000\n2 203 2322 ·2 = 4644\n6 56 451 ·6 = 2706\n8 40 341 ·8 = 2728\nTable 6: Time and training data consumed to reach BLEU of 25.6, i.e. Time Till Score\nandExamplesTillScore. Notethattheexperimenton1GPUwasendedafter25days\nof training without clearly surpassing the threshold (already outside of Figure 9)."}
{"chunk_id": "d00p0019c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 19, "text": "2 203 2322 ·2 = 4644\n6 56 451 ·6 = 2706\n8 40 341 ·8 = 2728\nTable 6: Time and training data consumed to reach BLEU of 25.6, i.e. Time Till Score\nandExamplesTillScore. Notethattheexperimenton1GPUwasendedafter25days\nof training without clearly surpassing the threshold (already outside of Figure 9).\n/two.superior⁶This holds at least in T2T versions 1.2.9–1.5.2, but as it is somewhat unexpected/unintuitive for some\nusers, it may be ﬁxed in future, seehttps://github.com/tensorflow/tensor2tensor/issues/517.\n/two.superior⁷and making sure environment variableCUDA_VISIBLE_DEVICES is set so enough cards are visible. T2T\nallows also distributed training (on multiple machines), but we have not experimented with it. Both"}
{"chunk_id": "d00p0019c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 19, "text": "/two.superior⁷and making sure environment variableCUDA_VISIBLE_DEVICES is set so enough cards are visible. T2T\nallows also distributed training (on multiple machines), but we have not experimented with it. Both\nsingle-machine multi-gpu and distributed training use synchronous Adam updates by default.\n19"}
{"chunk_id": "d00p0020c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 20, "text": "23.5\n24\n24.5\n25\n25.5\n26\n26.5\n27\n0 50 100 150 200 250 300 350\n1 2 3 4 5 6 7 8 9 10 11 12 13 14\nBLEU\nTraining time (hours)\nTraining time (days)\n8GPU\n6GPU\n2GPU\n1GPU\n25.6\nFigure 9: Eﬀect of the number of GPUs. BLEU=25.6 is marked with a black line.\nWe can see thattwo GPUs are more than three times faster than a single GPUwhen\nmeasuring the Time Till Score and need much less training examples (i.e. they have\nlowerExamplesTillScore). Similarly, eightGPUsaremorethanﬁvetimesfasterthantwo\nGPUsand 1.7 times less training data is needed.\nRecall that in Figure 6 we have shown that increasing the batch size from 1450 to\n2000hasalmost noeﬀectontheBLEUcurve. However, whenincreasingtheeﬀective\nbatch size by using more GPUs, the improvement is higher than could be expected"}
{"chunk_id": "d00p0020c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 20, "text": "GPUsand 1.7 times less training data is needed.\nRecall that in Figure 6 we have shown that increasing the batch size from 1450 to\n2000hasalmost noeﬀectontheBLEUcurve. However, whenincreasingtheeﬀective\nbatch size by using more GPUs, the improvement is higher than could be expected\nfrom the higher throughput./two.superior⁸We ﬁnd this quite surprising, especially considering\nthe fact that we have not tuned the learning rate and warmup steps (see the next\nsection).\nTips on the Number of GPUs\n• For the fastest BLEU convergenceuse as many GPUs as available(in our experi-\nments up to 8).\n• This holdseven when there are more experimentsto be done. For example, it is\nbetter to run one 8-GPUs experiment after another, rather than running two"}
{"chunk_id": "d00p0020c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 20, "text": "section).\nTips on the Number of GPUs\n• For the fastest BLEU convergenceuse as many GPUs as available(in our experi-\nments up to 8).\n• This holdseven when there are more experimentsto be done. For example, it is\nbetter to run one 8-GPUs experiment after another, rather than running two\n4-GPUs experiments in parallel or eight single-GPU experiments in parallel.\n/two.superior⁸It would be interesting to try simulating multi-GPU training on a single GPU, simply by doing the\nupdateonceafterNbatches(andsummingthegradients). Thisissimilartothe ghostbatches ofHoﬀeretal.\n(2017), but using ghost batch size higher than the actual batch size. We leave this for future work.\n20"}
{"chunk_id": "d00p0021c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "4.8. Learning Rate and Warmup Steps on Multiple GPUs\n4.8.1. Related Work\nThereisagrowingnumberofpapersonscalingdeeplearningtomultiplemachines\nwithsynchronousSGD(oritsvariants)byincreasingtheeﬀectivebatchsize. Wewill\nfocus mostly on the question how to adapt the learning rate schedule, when scaling\nfrom one GPU (or any device, in general) tok GPUs.\nKrizhevsky (2014) says “Theory suggests that when multiplying the batch size byk,\none should multiply the learning rate by\n√\nk to keep the variance in the gradient expectation\nconstant”, without actually explaining which theory suggests so. However, in the\nexperimentalparthereportsthatwhatworkedthebest, wasa linearscalingheuristics ,\ni.e. multiplying the learning rate byk, again without any explanation nor details on\nthe diﬀerence between\n√"}
{"chunk_id": "d00p0021c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "constant”, without actually explaining which theory suggests so. However, in the\nexperimentalparthereportsthatwhatworkedthebest, wasa linearscalingheuristics ,\ni.e. multiplying the learning rate byk, again without any explanation nor details on\nthe diﬀerence between\n√\nk scaling andk scaling.\nThe linear scaling heuristics become popular, leading to good scaling results in\npractice(Goyaletal.,2017;Smithetal.,2017)andalsotheoreticalexplanations(Bottou\netal.,2016;SmithandLe,2017;Jastrzebskietal.,2017). SmithandLe(2017)interpret\nSGD (and its variants) as a stochastic diﬀerential equation and show that thegradient\nnoise scaleg /equalxϵ\n( N\nB − 1\n)\n, whereϵis the learning rate,N is the training set size, and\nB is the eﬀective batch size. This noise “drives SGD away from sharp minima, and"}
{"chunk_id": "d00p0021c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "SGD (and its variants) as a stochastic diﬀerential equation and show that thegradient\nnoise scaleg /equalxϵ\n( N\nB − 1\n)\n, whereϵis the learning rate,N is the training set size, and\nB is the eﬀective batch size. This noise “drives SGD away from sharp minima, and\ntherefore there is an optimal batch size which maximizes the test set accuracy”. In other\nwords for keeping the optimal level of gradient noise (which leads to “ﬂat minima”\nthat generalize well), we need to scale the learning rate linearly when increasing the\neﬀective batch size.\nHowever,Hoﬀeretal.(2017)suggesttouse\n√\nk scalinginsteadofthelinearscaling\nand provide both theoretical and empirical support for this claim. They show that\ncov(∆w,∆w)∝ ϵ2\nNB ,thusifwewanttokeepthethecovariancematrixoftheparameters"}
{"chunk_id": "d00p0021c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "eﬀective batch size.\nHowever,Hoﬀeretal.(2017)suggesttouse\n√\nk scalinginsteadofthelinearscaling\nand provide both theoretical and empirical support for this claim. They show that\ncov(∆w,∆w)∝ ϵ2\nNB ,thusifwewanttokeepthethecovariancematrixoftheparameters\nupdate step∆w in the same range for any eﬀective batch sizeB, we need to scale the\nlearningrateproportionallytothesquarerootof B. Theyfoundthat\n√\nk scalingworks\nbetter than linear scaling on CIFAR10./two.superior⁹You et al. (2017) conﬁrm linear scaling does\nnot perform well on ImageNet and suggest to use Layer-wise Adaptive Rate Scaling.\nWe can see that large-batch training is still an open research question. Most of the\npapers cited above have experimental support only from the image recognition tasks"}
{"chunk_id": "d00p0021c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "not perform well on ImageNet and suggest to use Layer-wise Adaptive Rate Scaling.\nWe can see that large-batch training is still an open research question. Most of the\npapers cited above have experimental support only from the image recognition tasks\n(usuallyImageNet)andconvolutionalnetworks(e.g. ResNet),soitisnotclearwhether\ntheir suggestions can be applied also on sequence-to-sequence tasks (NMT) with\nself-attentional networks (Transformer). There are several other diﬀerences as well:\nModernconvolutionalnetworksareusuallytrainedwith batchnormalization (Ioﬀeand\nSzegedy, 2015), which seems to be important for the scaling, while Transformer uses\n/two.superior⁹Toclosethegapbetweensmall-batchtrainingandlarge-batchtraining,Hoﬀeretal.(2017)introduce(in\nadditionto\n√"}
{"chunk_id": "d00p0021c06", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 21, "text": "Modernconvolutionalnetworksareusuallytrainedwith batchnormalization (Ioﬀeand\nSzegedy, 2015), which seems to be important for the scaling, while Transformer uses\n/two.superior⁹Toclosethegapbetweensmall-batchtrainingandlarge-batchtraining,Hoﬀeretal.(2017)introduce(in\nadditionto\n√\nk scaling)so-called ghostbatchnormalization andadaptedtrainingregime ,whichmeansdecaying\nthe learning rate after a given number of steps instead of epochs.\n21"}
{"chunk_id": "d00p0022c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "layer normalization(Lei Ba et al., 2016)./three.superior⁰Also, Transformer uses Adam together with\nan inverse-square-root learning-rate decay, while most ImageNet papers use SGD\nwith momentum and piecewise-constant learning-rate decay.\n4.8.2. Our Experiments\nWe decided to ﬁnd out empirically the optimal learning rate for training on 8\nGPUs. Increasing the learning rate from 0.20 to 0.30 resulted in diverged training\n(BLEU dropped to almost 0 after two hours of training). Similarly to our single-GPU\nexperiments (Section 4.6), we were able prevent the divergence by increasing the\nwarmupstepsorbyintroducinggradientclipping(e.g. with clip_grad_norm=1.0,we\nwere able to use learning rate 0.40, but increasing it further to 0.60 led to divergence"}
{"chunk_id": "d00p0022c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "experiments (Section 4.6), we were able prevent the divergence by increasing the\nwarmupstepsorbyintroducinggradientclipping(e.g. with clip_grad_norm=1.0,we\nwere able to use learning rate 0.40, but increasing it further to 0.60 led to divergence\nanyway). However, none of these experiments led to any improvements over the default\nlearning rate– all had about the same BLEU curve after few hours of training.\nJastrzebski et al. (2017) shows that “the invariance under simultaneous rescaling of\nlearning rate and batch size breaks down if the learning rate gets too large or the batch size\ngets too small”. A similar observation was reported e.g. by Bottou et al. (2016). Thus\nour initial hypothesis was that 0.20 (or 0.25) is the maximal learning rate suitable for"}
{"chunk_id": "d00p0022c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "learning rate and batch size breaks down if the learning rate gets too large or the batch size\ngets too small”. A similar observation was reported e.g. by Bottou et al. (2016). Thus\nour initial hypothesis was that 0.20 (or 0.25) is the maximal learning rate suitable for\nstable training in our experiments even when we scale from a single GPU to 8 GPUs.\nConsideringthisinitialhypothesis,weweresurprisedthatwewereabletoachieveso\ngoodTimeTillScorewith8GPUs(morethan8timessmallerrelativetoasingleGPU,\nas reported in Table 6). To answer this riddle we need to understand how learning\nrate schedules are implemented in T2T.\n4.8.3. Parametrization of Learning Rate Schedules in T2T\nIn most works on learning rate schedules/three.superior/one.superiorthe “time” parameter is actually inter-"}
{"chunk_id": "d00p0022c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "as reported in Table 6). To answer this riddle we need to understand how learning\nrate schedules are implemented in T2T.\n4.8.3. Parametrization of Learning Rate Schedules in T2T\nIn most works on learning rate schedules/three.superior/one.superiorthe “time” parameter is actually inter-\npreted as the number of epochs or training examples. For example a popular setup\nforpiecewise-constantdecayinImageNettraining(e.g.Goyaletal.,2017)istodivide\nthe learning rate by a factor of 10 at the 30-th, 60-th, and 80-th epoch.\nHowever,inT2T,itisthe global_step variablethatisusedasthe“time”parameter.\nSo when increasing the eﬀective batch size 8 times, e.g. by using 8 GPUs instead of a\nsingleGPU,theactuallearningrate /three.superior/two.superiorachievesagivenvalueafterthesamenumberof"}
{"chunk_id": "d00p0022c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "However,inT2T,itisthe global_step variablethatisusedasthe“time”parameter.\nSo when increasing the eﬀective batch size 8 times, e.g. by using 8 GPUs instead of a\nsingleGPU,theactuallearningrate /three.superior/two.superiorachievesagivenvalueafterthesamenumberof\nsteps,butthismeansafter8timeslesstrainingexamples. Fortheinverse-square-root\n/three.superior⁰Applying batch normalization on RNN is diﬃcult. Transformer does not use RNN, but still we were\nnot successful in switching to batch normalization (and possibly ghost batch normalization) due to NaN\nloss errors.\n/three.superior/one.superiorExamplesoflearningrateschedulesareinverse-square-rootdecay,inverse-timedecay,exponentialde-\ncay, piecewise-constant decay, seehttps://www.tensorflow.org/api_guides/python/train#Decaying_the_"}
{"chunk_id": "d00p0022c06", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 22, "text": "loss errors.\n/three.superior/one.superiorExamplesoflearningrateschedulesareinverse-square-rootdecay,inverse-timedecay,exponentialde-\ncay, piecewise-constant decay, seehttps://www.tensorflow.org/api_guides/python/train#Decaying_the_\nlearning_rate for TF implementations.\n/three.superior/two.superiorByactuallearningratewemeanthelearningrateafterapplyingthedecayschedule. The learning_rate\nparameter stays the same in this case.\n22"}
{"chunk_id": "d00p0023c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 23, "text": "decay, we haveactual_lr(steps)/equalxc ·steps−0.5 /equalx1√\n8 ·actual_lr(steps ·8), where c is a\nconstant containing also thelearning_rate parameter. So with 8 GPUs, if we divide\nthe learning_rate parameter by\n√\n8, we achieve the same actual learning rate after a\ngiven number of training examples as in the original single-GPU setting.\nThis explains the riddle from the previous section.By keeping thelearning_rate\nparameter the same when scaling tok times bigger eﬀective batch, we actually increase the\nactuallearningrate\n√\nk times,inaccordancewiththesuggestionofHoﬀeretal.(2017). /three.superior/three.superior\nThis holds only for thelinear_warmup_rsqrt_decay (aka noam) schedule and ignoring\nthe warmup steps.\nIf we want to keep the same learning rate also in the warmup phase, we would"}
{"chunk_id": "d00p0023c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 23, "text": "actuallearningrate\n√\nk times,inaccordancewiththesuggestionofHoﬀeretal.(2017). /three.superior/three.superior\nThis holds only for thelinear_warmup_rsqrt_decay (aka noam) schedule and ignoring\nthe warmup steps.\nIf we want to keep the same learning rate also in the warmup phase, we would\nneed to divide the warmup steps byk. However, this means that the maximum\nactual learning rate will be\n√\nk times higher, relative to the single-GPU maximal\nactual learning rate and this leads to divergence in our experiments. In deed, many\nresearchers (e.g. Goyal et al., 2017) suggest to use a warmup when scaling to more\nGPUs in order to prevent divergence. Transformer uses learning rate warmup by\ndefault even for single-GPU training (cf. Section 4.6), but it makes sense to use more"}
{"chunk_id": "d00p0023c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 23, "text": "researchers (e.g. Goyal et al., 2017) suggest to use a warmup when scaling to more\nGPUs in order to prevent divergence. Transformer uses learning rate warmup by\ndefault even for single-GPU training (cf. Section 4.6), but it makes sense to use more\nwarmup training examples in multi-GPU setting.\nIn our experiments with 8 GPUs and the default learning rate 0.20, using 8k\nwarmup steps instead of the default 16k had no eﬀect on the BLEU curve (it was a\nbit higher in the ﬁrst few hours, but the same afterwards). Further decreasing the\nwarmup steps resulted in a retarded BLEU curve (for 6k) or a complete divergence\n(for 2k).\nTips on Learning Rate and Warmup Steps on Multiple GPUs\n• Keep the learning_rate parameter at its optimal value found in single-GPU\nexperiments."}
{"chunk_id": "d00p0023c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 23, "text": "warmup steps resulted in a retarded BLEU curve (for 6k) or a complete divergence\n(for 2k).\nTips on Learning Rate and Warmup Steps on Multiple GPUs\n• Keep the learning_rate parameter at its optimal value found in single-GPU\nexperiments.\n• Youcantrydecreasingthewarmupsteps,butlessthanlinearlyandyoushould\nnot expect to improve the ﬁnal BLEU this way.\n4.9. Resumed Training\nT2Tallowstoresumetrainingfromacheckpoint,simplybypointingthe output_dir\nparametertoadirectorywithanexistingcheckpoint(speciﬁedinthe checkpoint ﬁle).\nThismaybeusefulwhenthetrainingfails(e.g. becauseofhardwareerror), whenwe\nneed to continue training on a diﬀerent machine or during hyper-parameter search,\nwhen we want to continue with the most promising setups. T2T saves also Adam"}
{"chunk_id": "d00p0023c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 23, "text": "Thismaybeusefulwhenthetrainingfails(e.g. becauseofhardwareerror), whenwe\nneed to continue training on a diﬀerent machine or during hyper-parameter search,\nwhen we want to continue with the most promising setups. T2T saves also Adam\n/three.superior/three.superiorIn addition to suggesting the\n√\nk learning-rate scaling, Hoﬀer et al. (2017) show that to fully close the\n“generalization gap”, we need to train longer because the absolute number of steps (updates) matters. So\nfrom this point of view, using steps instead of epochs as the time parameter for learning rate schedules\nmay not be a completely wrong idea.\n23"}
{"chunk_id": "d00p0024c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 24, "text": "26\n26.2\n26.4\n26.6\n26.8\n110 120 130 140 150 160 170 180\nBLEU\nTraining time (hours)\naveraging 16 checkpoints\naveraging 8 checkpoints\nno averaging\nFigure 10: Eﬀect of checkpoint averaging. All trained on 6 GPUs.\nmomentumintothecheckpoint, sothetrainingcontinuesalmostasifithadnotbeen\nstopped. However, it does not store the position in the training data – it starts from a\nrandom position. Also the relative time (and wall-clock time) in TensorBoard graphs\nwill be inﬂuenced by the stopping.\nResumed training can also be exploited for changing some hyper-parameters,\nwhich cannot be meta-parametrized by the number of steps. For example, Smith\net al. (2017) suggest to increase the eﬀective batch size (and number of GPUs) during\ntraining, instead of decaying the learning rate."}
{"chunk_id": "d00p0024c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 24, "text": "Resumed training can also be exploited for changing some hyper-parameters,\nwhich cannot be meta-parametrized by the number of steps. For example, Smith\net al. (2017) suggest to increase the eﬀective batch size (and number of GPUs) during\ntraining, instead of decaying the learning rate.\nYet another usage is to do domain adaptation by switching from (large) general-\ndomain training data to (small) target-domain training data for the few last epochs.\nInthiscase,considereditingalsothelearningrateorlearningrateschedule(orfaking\nthe global_step stored in the checkpoint) to make sure the learning rate is not too\nsmall.\n4.10. Checkpoint Averaging\nVaswanietal.(2017)suggesttoaveragethelast20checkpointssavedin10-minute\nintervals (usingutils/avg_checkpoints.py). According to our experiments slightly"}
{"chunk_id": "d00p0024c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 24, "text": "the global_step stored in the checkpoint) to make sure the learning rate is not too\nsmall.\n4.10. Checkpoint Averaging\nVaswanietal.(2017)suggesttoaveragethelast20checkpointssavedin10-minute\nintervals (usingutils/avg_checkpoints.py). According to our experiments slightly\nbetterresultsareachievedwithaveragingcheckpointssavedin1-hourintervals. This\nhas also the advantage that less time is spent with checkpoint saving, so the training\nis faster.\nFigure 10 shows the eﬀect of averaging is twofold: the averaged curve has lower\nvariance (ﬂickering) from checkpoint to checkpoint and it is almost always better\nthan the baseline without averaging (usually by about 0.2 BLEU). In some setups,\nwe have seen improvements due to averaging over 1 BLEU. In the early phases of"}
{"chunk_id": "d00p0024c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 24, "text": "variance (ﬂickering) from checkpoint to checkpoint and it is almost always better\nthan the baseline without averaging (usually by about 0.2 BLEU). In some setups,\nwe have seen improvements due to averaging over 1 BLEU. In the early phases of\ntraining, while the (baseline) learning curve grows fast, it is better to use fewer\n24"}
{"chunk_id": "d00p0025c01", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 25, "text": "Manual Automatic Scores\n# Ave % Ave z BLEU TER CharacTER BEER System\n– – – 23.8 0.662 0.582 0.543 T2T 8 GPUs 8 days\n1 62.0 0.308 22.8 0.667 0.588 0.540 uedin-nmt\n2 59.7 0.240 20.1 0.703 0.612 0.519 online-B\n3 55.9 0.111 20.2 0.696 0.607 0.524 limsi-factored\n55.2 0.102 20.0 0.699 - - LIUM-FNMT\n55.2 0.090 20.2 0.701 0.605 0.522 LIUM-NMT\n54.1 0.050 20.5 0.696 0.624 0.523 CU-Chimera\n53.3 0.029 16.6 0.743 0.637 0.503 online-A\n8 41.9 -0.327 16.2 0.757 0.697 0.485 PJATK\nTable7: WMT17systemsforEnglish-to-CzechandourbestT2Ttrainingrun. Manual\nscores are from the oﬃcial WMT17 ranking. Automatic metrics were provided by\nhttp://matrix.statmt.org/. For *TER metrics, lower is better. Best results in bold,\nsecond-best in italics."}
{"chunk_id": "d00p0025c02", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 25, "text": "8 41.9 -0.327 16.2 0.757 0.697 0.485 PJATK\nTable7: WMT17systemsforEnglish-to-CzechandourbestT2Ttrainingrun. Manual\nscores are from the oﬃcial WMT17 ranking. Automatic metrics were provided by\nhttp://matrix.statmt.org/. For *TER metrics, lower is better. Best results in bold,\nsecond-best in italics.\ncheckpoints for averaging. In later phases (as shown in Figure 10, after 4.5–7.5 days\nof training), it seems that 16 checkpoints (covering last 16 hours) give slightly better\nresults on average than 8 checkpoints, but we have not done any proper evaluation\nfor signiﬁcance (using paired bootstrap testing for each hour and then summarizing\nthe results).\nThe fact that resumed training starts from a random position in the training data"}
{"chunk_id": "d00p0025c03", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 25, "text": "results on average than 8 checkpoints, but we have not done any proper evaluation\nfor signiﬁcance (using paired bootstrap testing for each hour and then summarizing\nthe results).\nThe fact that resumed training starts from a random position in the training data\n(cf. Section 4.9) can be actually exploited for “forking” a training to get two (or\nmore) copies of the model, which are trained for the same number of steps, but\nindependentlyinthelaterstagesandthusendingwithdiﬀerentweightssavedinthe\nﬁnal checkpoint. These semi-independent models can be averaged in the same way\nascheckpointsfromthesamerun, asdescribedabove. Ourpreliminaryresultsshow\nthis helps a bit (on top of checkpoint averaging).\nTips on Checkpoint Averaging"}
{"chunk_id": "d00p0025c04", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 25, "text": "ﬁnal checkpoint. These semi-independent models can be averaged in the same way\nascheckpointsfromthesamerun, asdescribedabove. Ourpreliminaryresultsshow\nthis helps a bit (on top of checkpoint averaging).\nTips on Checkpoint Averaging\n• Use it. Averaging 8 checkpoints takes about 5 minutes, so it is a “BLEU boost\nfor free” (compared with the time needed for the whole training).\n• See the tools for automatic checkpoint averaging and evaluation described in\nSection 2.4.\n5. Comparison with WMT17 Systems\nTable 7 provides the results of WMT17 English-to-Czech news translation task,\nwith our best Transformer model (BIG trained on 8 GPUs for 8 days, averaging 8\ncheckpoints) evaluated using the exact same implementation of automatic metrics."}
{"chunk_id": "d00p0025c05", "doc_id": 1, "doc": "TrainingTipsForTransformers.pdf", "page": 25, "text": "Section 2.4.\n5. Comparison with WMT17 Systems\nTable 7 provides the results of WMT17 English-to-Czech news translation task,\nwith our best Transformer model (BIG trained on 8 GPUs for 8 days, averaging 8\ncheckpoints) evaluated using the exact same implementation of automatic metrics.\nWhile the automatic evaluation is not fully reliable (see e.g. the high BLEU score\n25"}
{"chunk_id": "d01p0001c01", "doc_id": 2, "doc": "Transformers.pdf", "page": 1, "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗ †\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best"}
{"chunk_id": "d01p0001c02", "doc_id": 2, "doc": "Transformers.pdf", "page": 1, "text": "aidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-"}
{"chunk_id": "d01p0001c03", "doc_id": 2, "doc": "Transformers.pdf", "page": 1, "text": "entirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data."}
{"chunk_id": "d01p0001c04", "doc_id": 2, "doc": "Transformers.pdf", "page": 1, "text": "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every"}
{"chunk_id": "d01p0001c05", "doc_id": 2, "doc": "Transformers.pdf", "page": 1, "text": "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research."}
{"chunk_id": "d01p0001c06", "doc_id": 2, "doc": "Transformers.pdf", "page": 1, "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023"}
{"chunk_id": "d01p0002c01", "doc_id": 2, "doc": "Transformers.pdf", "page": 2, "text": "1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently"}
{"chunk_id": "d01p0002c02", "doc_id": 2, "doc": "Transformers.pdf", "page": 2, "text": "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-"}
{"chunk_id": "d01p0002c03", "doc_id": 2, "doc": "Transformers.pdf", "page": 2, "text": "computation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in"}
{"chunk_id": "d01p0002c04", "doc_id": 2, "doc": "Transformers.pdf", "page": 2, "text": "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,"}
{"chunk_id": "d01p0002c05", "doc_id": 2, "doc": "Transformers.pdf", "page": 2, "text": "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2."}
{"chunk_id": "d01p0002c06", "doc_id": 2, "doc": "Transformers.pdf", "page": 2, "text": "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-"}
{"chunk_id": "d01p0002c07", "doc_id": 2, "doc": "Transformers.pdf", "page": 2, "text": "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9]."}
{"chunk_id": "d01p0002c08", "doc_id": 2, "doc": "Transformers.pdf", "page": 2, "text": "entirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2"}
{"chunk_id": "d01p0003c01", "doc_id": 2, "doc": "Transformers.pdf", "page": 3, "text": "Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer"}
{"chunk_id": "d01p0003c02", "doc_id": 2, "doc": "Transformers.pdf", "page": 3, "text": "wise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections"}
{"chunk_id": "d01p0003c03", "doc_id": 2, "doc": "Transformers.pdf", "page": 3, "text": "Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,"}
{"chunk_id": "d01p0003c04", "doc_id": 2, "doc": "Transformers.pdf", "page": 3, "text": "predictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3"}
{"chunk_id": "d01p0004c01", "doc_id": 2, "doc": "Transformers.pdf", "page": 4, "text": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together"}
{"chunk_id": "d01p0004c02", "doc_id": 2, "doc": "Transformers.pdf", "page": 4, "text": "query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V) = softmax(QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1√dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is"}
{"chunk_id": "d01p0004c03", "doc_id": 2, "doc": "Transformers.pdf", "page": 4, "text": "of 1√dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n.\n3.2.2 Multi-Head Attention"}
{"chunk_id": "d01p0004c04", "doc_id": 2, "doc": "Transformers.pdf", "page": 4, "text": "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random"}
{"chunk_id": "d01p0004c05", "doc_id": 2, "doc": "Transformers.pdf", "page": 4, "text": "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4"}
{"chunk_id": "d01p0005c01", "doc_id": 2, "doc": "Transformers.pdf", "page": 5, "text": "output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nWhere the projections are parameter matricesWQ\ni ∈ Rdmodel×dk , WK\ni ∈ Rdmodel×dk , WV\ni ∈ Rdmodel×dv\nand WO ∈ Rhdv×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality."}
{"chunk_id": "d01p0005c02", "doc_id": 2, "doc": "Transformers.pdf", "page": 5, "text": "i ∈ Rdmodel×dv\nand WO ∈ Rhdv×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9]."}
{"chunk_id": "d01p0005c03", "doc_id": 2, "doc": "Transformers.pdf", "page": 5, "text": "and the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward"}
{"chunk_id": "d01p0005c04", "doc_id": 2, "doc": "Transformers.pdf", "page": 5, "text": "encoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This"}
{"chunk_id": "d01p0005c05", "doc_id": 2, "doc": "Transformers.pdf", "page": 5, "text": "3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input"}
{"chunk_id": "d01p0005c06", "doc_id": 2, "doc": "Transformers.pdf", "page": 5, "text": "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\n5"}
{"chunk_id": "d01p0006c01", "doc_id": 2, "doc": "Transformers.pdf", "page": 6, "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 · d) O(1) O(1)\nRecurrent O(n · d2) O(n) O(n)\nConvolutional O(k · n · d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the"}
{"chunk_id": "d01p0006c02", "doc_id": 2, "doc": "Transformers.pdf", "page": 6, "text": "Self-Attention (restricted) O(r · n · d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )"}
{"chunk_id": "d01p0006c03", "doc_id": 2, "doc": "Transformers.pdf", "page": 6, "text": "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two"}
{"chunk_id": "d01p0006c04", "doc_id": 2, "doc": "Transformers.pdf", "page": 6, "text": "relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden"}
{"chunk_id": "d01p0006c05", "doc_id": 2, "doc": "Transformers.pdf", "page": 6, "text": "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range"}
{"chunk_id": "d01p0006c06", "doc_id": 2, "doc": "Transformers.pdf", "page": 6, "text": "One is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types."}
{"chunk_id": "d01p0006c07", "doc_id": 2, "doc": "Transformers.pdf", "page": 6, "text": "and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6"}
{"chunk_id": "d01p0007c01", "doc_id": 2, "doc": "Transformers.pdf", "page": 7, "text": "length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,"}
{"chunk_id": "d01p0007c02", "doc_id": 2, "doc": "Transformers.pdf", "page": 7, "text": "path length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,"}
{"chunk_id": "d01p0007c03", "doc_id": 2, "doc": "Transformers.pdf", "page": 7, "text": "recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching"}
{"chunk_id": "d01p0007c04", "doc_id": 2, "doc": "Transformers.pdf", "page": 7, "text": "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training"}
{"chunk_id": "d01p0007c05", "doc_id": 2, "doc": "Transformers.pdf", "page": 7, "text": "target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the"}
{"chunk_id": "d01p0007c06", "doc_id": 2, "doc": "Transformers.pdf", "page": 7, "text": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,"}
{"chunk_id": "d01p0007c07", "doc_id": 2, "doc": "Transformers.pdf", "page": 7, "text": "rate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7"}
{"chunk_id": "d01p0008c01", "doc_id": 2, "doc": "Transformers.pdf", "page": 8, "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the"}
{"chunk_id": "d01p0008c02", "doc_id": 2, "doc": "Transformers.pdf", "page": 8, "text": "GNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation"}
{"chunk_id": "d01p0008c03", "doc_id": 2, "doc": "Transformers.pdf", "page": 8, "text": "Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models."}
{"chunk_id": "d01p0008c04", "doc_id": 2, "doc": "Transformers.pdf", "page": 8, "text": "listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We"}
{"chunk_id": "d01p0008c05", "doc_id": 2, "doc": "Transformers.pdf", "page": 8, "text": "dropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a"}
{"chunk_id": "d01p0008c06", "doc_id": 2, "doc": "Transformers.pdf", "page": 8, "text": "inference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8"}
{"chunk_id": "d01p0009c01", "doc_id": 2, "doc": "Transformers.pdf", "page": 9, "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop ϵls\ntrain PPL BLEU params\nsteps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7"}
{"chunk_id": "d01p0009c02", "doc_id": 2, "doc": "Transformers.pdf", "page": 9, "text": "4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head"}
{"chunk_id": "d01p0009c03", "doc_id": 2, "doc": "Transformers.pdf", "page": 9, "text": "checkpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our"}
{"chunk_id": "d01p0009c04", "doc_id": 2, "doc": "Transformers.pdf", "page": 9, "text": "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37]."}
{"chunk_id": "d01p0009c05", "doc_id": 2, "doc": "Transformers.pdf", "page": 9, "text": "constituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting."}
{"chunk_id": "d01p0009c06", "doc_id": 2, "doc": "Transformers.pdf", "page": 9, "text": "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9"}
